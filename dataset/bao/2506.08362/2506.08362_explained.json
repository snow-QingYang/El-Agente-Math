{
  "formulas": [
    {
      "label": "<<FORMULA_0071>>",
      "formula": "\\langle \\mF(\\bar \\vz) + \\nabla \\mF(\\bar \\vz)(\\vz - \\bar \\vz) + \\frac{\\gamma}{2} \\Vert \\vz - \\bar \\vz \\Vert (\\vz - \\bar \\vz), \\vz' - \\vz  \\rangle \\ge 0, ~\\forall \\vz' \\in \\gZ; \\\\\n    \\vu =  -\\left(\\mF(\\bar\\vz) + \\nabla \\mF(\\bar \\vz) (\\vz - \\bar \\vz) + \\frac{\\gamma}{2} \\Vert \\vz - \\bar \\vz \\Vert \n    (\\vz- \\bar \\vz)  \\right) \\in \n    \\begin{bmatrix}\n        \\partial \\gI_{\\gX} (\\vx) \\\\\n        -\\partial \\gI_{\\gY}(\\vy)\n    \\end{bmatrix},",
      "raw_latex": "\\begin{align*}\n    \\langle \\mF(\\bar \\vz) + \\nabla \\mF(\\bar \\vz)(\\vz - \\bar \\vz) + \\frac{\\gamma}{2} \\Vert \\vz - \\bar \\vz \\Vert (\\vz - \\bar \\vz), \\vz' - \\vz  \\rangle \\ge 0, ~\\forall \\vz' \\in \\gZ; \\\\\n    \\vu =  -\\left(\\mF(\\bar\\vz) + \\nabla \\mF(\\bar \\vz) (\\vz - \\bar \\vz) + \\frac{\\gamma}{2} \\Vert \\vz - \\bar \\vz \\Vert \n    (\\vz- \\bar \\vz)  \\right) \\in \n    \\begin{bmatrix}\n        \\partial \\gI_{\\gX} (\\vx) \\\\\n        -\\partial \\gI_{\\gY}(\\vy)\n    \\end{bmatrix},\n\\end{align*}",
      "formula_type": "align*",
      "line_number": 853,
      "is_formula": true,
      "high_level_explanation": "This condition characterizes the solution z returned by a cubic-regularized Newton (CRN) oracle for a variational inequality: z satisfies a variational inequality with respect to a cubic-regularized first-order model of the operator at the anchor point bar z. The first line is the VI optimality condition, requiring the inner product with any feasible direction to be nonnegative. The second line introduces a vector u that equals minus the model’s residual and lies in the appropriate normal-cone set, serving as a KKT-type certificate of feasibility with respect to X and Y. Together, these are the optimality conditions for the CRN step over the feasible set.",
      "notations": {
        "\\mF": "NOT MENTIONED",
        "\\bar \\vz": "Anchor (query) point provided to the CRN oracle",
        "\\vz": "Point returned by the CRN step that satisfies the variational inequality",
        "\\vz'": "An arbitrary feasible point used to state the variational inequality",
        "\\gZ": "Feasible set for \\vz",
        "\\nabla \\mF(\\bar \\vz)": "Jacobian of \\mF evaluated at \\bar \\vz",
        "\\gamma": "Regularization parameter used in the cubic term of the CRN model",
        "\\vu": "Dual/normal-cone vector associated with \\vz returned by the CRN oracle",
        "\\partial \\gI_{\\gX} (\\vx)": "Subdifferential of the indicator of \\gX at \\vx (the normal cone to \\gX at \\vx)",
        "-\\partial \\gI_{\\gY}(\\vy)": "Negative subdifferential of the indicator of \\gY at \\vy (the negative normal cone to \\gY at \\vy)",
        "\\gX": "Feasible set for \\vx (assumed convex and compact)",
        "\\gY": "Feasible set for \\vy (assumed convex and compact)",
        "\\vx": "x-component of \\vz",
        "\\vy": "y-component of \\vz"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:42:19.945843"
    },
    {
      "label": "<<FORMULA_0074>>",
      "formula": "\\vz = \\arg & \\min_{\\vz' \\in \\gZ}  \\left\\{ \n    f(\\vz') + \\langle \\nabla f(\\bar \\vz), \\vz' - \\bar \\vz \\rangle + \\frac{1}{2} \\langle \\nabla^2 f(\\bar \\vz) (\\vz' - \\bar \\vz), \\vz' - \\bar \\vz \\rangle + \\frac{\\gamma}{6} \\Vert \\vz' - \\bar \\vz \\Vert^3\n    \\right\\}; \\\\\n    \\vu &= - ( \\nabla f(\\bar \\vz) + \\nabla^2 f(\\bar \\vz) (\\vz - \\bar \\vz) + \\frac{\\gamma}{2} \\Vert \\vz - \\bar \\vz \\Vert (\\vz-  \\bar \\vz) ) \\in \\partial \\gI_{\\gZ}(\\vz).",
      "raw_latex": "\\begin{align*}\n    \\vz = \\arg & \\min_{\\vz' \\in \\gZ}  \\left\\{ \n    f(\\vz') + \\langle \\nabla f(\\bar \\vz), \\vz' - \\bar \\vz \\rangle + \\frac{1}{2} \\langle \\nabla^2 f(\\bar \\vz) (\\vz' - \\bar \\vz), \\vz' - \\bar \\vz \\rangle + \\frac{\\gamma}{6} \\Vert \\vz' - \\bar \\vz \\Vert^3\n    \\right\\}; \\\\\n    \\vu &= - ( \\nabla f(\\bar \\vz) + \\nabla^2 f(\\bar \\vz) (\\vz - \\bar \\vz) + \\frac{\\gamma}{2} \\Vert \\vz - \\bar \\vz \\Vert (\\vz-  \\bar \\vz) ) \\in \\partial \\gI_{\\gZ}(\\vz).\n\\end{align*}",
      "formula_type": "align*",
      "line_number": 868,
      "is_formula": true,
      "high_level_explanation": "The point z is obtained by minimizing, over the feasible set, a cubic-regularized second-order Taylor model of f around a reference point z̄. The vector u equals the negative gradient of this cubic model at z and lies in the normal cone to the feasible set at z, encoding the first-order optimality (KKT) condition for the constrained subproblem. This is the standard cubic-regularized Newton step used in CRN oracles.",
      "notations": {
        "\\vz": "Minimizer of the cubic-regularized subproblem",
        "\\vz'": "Trial point in the feasible set used inside the argmin",
        "\\bar \\vz": "Reference/query point at which the local cubic model is built",
        "\\gZ": "NOT MENTIONED",
        "f": "NOT MENTIONED",
        "\\gamma": "Cubic regularization parameter",
        "\\nabla f(\\bar \\vz)": "Gradient of f evaluated at the reference point",
        "\\nabla^2 f(\\bar \\vz)": "Hessian of f evaluated at the reference point",
        "\\Vert \\vz' - \\bar \\vz \\Vert": "Norm of the displacement from the reference point to the trial point",
        "\\Vert \\vz - \\bar \\vz \\Vert": "Norm of the displacement from the reference point to the minimizer",
        "\\vu": "A normal-cone vector (dual certificate) at z satisfying the KKT condition for the subproblem",
        "\\partial \\gI_{\\gZ}(\\vz)": "Normal cone to \\gZ at \\vz (subdifferential of the indicator of \\gZ)"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:42:04.480588"
    },
    {
      "label": "<<FORMULA_0346>>",
      "formula": "\\label{eq:MS-1}\n\\begin{split}\n    R_{t+1} &= \\frac{1}{2} \\left \\Vert {\\rm Proj}_{\\gZ} (\\vv_t - a_{t+1}  (\\vg_{t+1} + \\vu_{t+1})) - \\vz^* \\right \\Vert^2 \\\\\n    &\\le \\frac{1}{2} \\left \\Vert (\\vv_t - a_{t+1} (\\vg_{t+1} + \\vu_{t+1})) - \\vz^* \\right \\Vert^2 \\\\\n    &= R_t + a_{t+1} \\langle \\vg_{t+1} + \\vu_{t+1}, \\vz^* - \\vv_t \\rangle + \\frac{a_{t+1}^2}{2} \\Vert \\vg_{t+1} + \\vu_{t+1} \\Vert^2 \\\\\n    &\\\\ge  R_t + a_{t+1} \\langle  \\nabla h(\\tilde \\vz_{t+1}) + u_{t+1}, \\vz^* - \\vv_t \\rangle + a_{t+1}^2 \\Vert \\nabla h(\\tilde \\vz_{t+1}) + \\vu_{t+1} \\Vert^2 \\\\\n    &\\quad + a_{t+1} \\delta \\Vert \\vv_t - \\vz^* \\Vert + a_{t+1}^2 \\delta^2.\n    % &\\le D_t + a_{t+1} \\langle \\nabla f(\\tilde \\vz_{t+1}), \\vz^* - \\vv_t \\rangle + a_{t+1}^2 \\Vert \\nabla f(\\tilde \\vz_{t+1}) \\Vert^2 + a_{t+1} D \\delta_{t+1} + a_{t+1}^2 \\delta_{t+1}^2.\n\\end{split}",
      "raw_latex": "\\begin{align} \\label{eq:MS-1}\n\\begin{split}\n    R_{t+1} &= \\frac{1}{2} \\left \\Vert {\\rm Proj}_{\\gZ} (\\vv_t - a_{t+1}  (\\vg_{t+1} + \\vu_{t+1})) - \\vz^* \\right \\Vert^2 \\\\\n    &\\le \\frac{1}{2} \\left \\Vert (\\vv_t - a_{t+1} (\\vg_{t+1} + \\vu_{t+1})) - \\vz^* \\right \\Vert^2 \\\\\n    &= R_t + a_{t+1} \\langle \\vg_{t+1} + \\vu_{t+1}, \\vz^* - \\vv_t \\rangle + \\frac{a_{t+1}^2}{2} \\Vert \\vg_{t+1} + \\vu_{t+1} \\Vert^2 \\\\\n    &\\\\ge  R_t + a_{t+1} \\langle  \\nabla h(\\tilde \\vz_{t+1}) + u_{t+1}, \\vz^* - \\vv_t \\rangle + a_{t+1}^2 \\Vert \\nabla h(\\tilde \\vz_{t+1}) + \\vu_{t+1} \\Vert^2 \\\\\n    &\\quad + a_{t+1} \\delta \\Vert \\vv_t - \\vz^* \\Vert + a_{t+1}^2 \\delta^2.\n    % &\\le D_t + a_{t+1} \\langle \\nabla f(\\tilde \\vz_{t+1}), \\vz^* - \\vv_t \\rangle + a_{t+1}^2 \\Vert \\nabla f(\\tilde \\vz_{t+1}) \\Vert^2 + a_{t+1} D \\delta_{t+1} + a_{t+1}^2 \\delta_{t+1}^2.\n\\end{split}\n\\end{align}",
      "formula_type": "align",
      "line_number": 1956,
      "is_formula": true,
      "high_level_explanation": "This chain of relations defines and bounds the potential R_{t+1}, the half squared distance between the projected update and a reference point z*, after taking a step from v_t. The first inequality uses the nonexpansiveness of the projection to upper-bound the distance by the unprojected step, and the next equality expands the squared norm to relate R_{t+1} to R_t, an inner-product progress term, and a quadratic term in the search direction. The final bound substitutes an inexact gradient model based on ∇h(tilde z_{t+1}) and introduces δ-dependent error terms, showing how step size and oracle inaccuracies influence the one-step progress under constraints.",
      "notations": {
        "R_{t+1}": "Defined as 1/2 times the squared norm of {\\rm Proj}_{\\gZ}(\\vv_t - a_{t+1}(\\vg_{t+1} + \\vu_{t+1})) - \\vz^*",
        "R_t": "Defined as 1/2 times the squared norm of \\vv_t - \\vz^* (as implied by the norm expansion)",
        "{\\rm Proj}_{\\gZ}": "Projection operator onto the set \\gZ",
        "\\gZ": "Constraint set (feasible set)",
        "\\vv_t": "NOT MENTIONED",
        "a_{t+1}": "NOT MENTIONED",
        "\\vg_{t+1}": "NOT MENTIONED",
        "\\vu_{t+1}": "NOT MENTIONED",
        "u_{t+1}": "NOT MENTIONED",
        "\\vz^*": "NOT MENTIONED",
        "h": "Convex function (objective), as indicated by the use of convexity",
        "\\tilde \\vz_{t+1}": "NOT MENTIONED",
        "\\delta": "NOT MENTIONED"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:40:58.842434"
    },
    {
      "label": "<<FORMULA_0352>>",
      "formula": "\\label{eq:to-plug}\n\\begin{split} \n    &\\quad a_{t+1}' \\langle \\nabla h(\\tilde \\vz_{t+1}) + \\vu_{t+1}, \\vz^* - \\vv_t \\rangle \\\\\n&= \\langle \\nabla h(\\tilde \\vz_{t+1}) + \\vu_{t+1}, a_{t+1}'  ( \\vz^*-  \\tilde \\vz_{t+1}) + A_t (\\vz_t - \\tilde \\vz_{t+1})  + A_{t+1}' (\\tilde \\vz_{t+1}- \\bar \\vz_t) \\rangle  \\\\\n&\\le a_{t+1}' ( h(\\vz^*) - h(\\tilde \\vz_{t+1})  )  + A_t (h(\\vz_t) - h(\\tilde \\vz_{t+1})) + A_{t+1}' \\langle \\nabla h(\\tilde \\vz_{t+1}) + \\vu_{t+1}, \\tilde \\vz_{t+1} - \\bar \\vz_t \\rangle,\n\\end{split}",
      "raw_latex": "\\begin{align}\\label{eq:to-plug}\n\\begin{split} \n    &\\quad a_{t+1}' \\langle \\nabla h(\\tilde \\vz_{t+1}) + \\vu_{t+1}, \\vz^* - \\vv_t \\rangle \\\\\n&= \\langle \\nabla h(\\tilde \\vz_{t+1}) + \\vu_{t+1}, a_{t+1}'  ( \\vz^*-  \\tilde \\vz_{t+1}) + A_t (\\vz_t - \\tilde \\vz_{t+1})  + A_{t+1}' (\\tilde \\vz_{t+1}- \\bar \\vz_t) \\rangle  \\\\\n&\\le a_{t+1}' ( h(\\vz^*) - h(\\tilde \\vz_{t+1})  )  + A_t (h(\\vz_t) - h(\\tilde \\vz_{t+1})) + A_{t+1}' \\langle \\nabla h(\\tilde \\vz_{t+1}) + \\vu_{t+1}, \\tilde \\vz_{t+1} - \\bar \\vz_t \\rangle,\n\\end{split}\n\\end{align}",
      "formula_type": "align",
      "line_number": 1971,
      "is_formula": true,
      "high_level_explanation": "The display rewrites the key inner product a'_{t+1}⟨∇h(tilde z_{t+1}) + u_{t+1}, z* − v_t⟩ by expressing z* − v_t as a weighted sum of three displacement vectors relative to tilde z_{t+1} and bar z_t. Using convexity of h, the inner products with (z* − tilde z_{t+1}) and (z_t − tilde z_{t+1}) are upper bounded by function-value gaps, leaving a residual inner product with (tilde z_{t+1} − bar z_t). This yields an inequality that controls the original inner product via differences h(z*) − h(tilde z_{t+1}) and h(z_t) − h(tilde z_{t+1}) plus a remaining term to be further bounded. The step sets up a plug-in bound used in the subsequent analysis.",
      "notations": {
        "a_{t+1}'": "NOT MENTIONED",
        "\\nabla h(\\tilde \\vz_{t+1})": "Gradient of h evaluated at \\tilde \\vz_{t+1}",
        "\\vu_{t+1}": "NOT MENTIONED",
        "\\vz^*": "NOT MENTIONED",
        "\\vv_t": "NOT MENTIONED",
        "A_t": "NOT MENTIONED",
        "\\vz_t": "NOT MENTIONED",
        "A_{t+1}'": "NOT MENTIONED",
        "\\tilde \\vz_{t+1}": "NOT MENTIONED",
        "\\bar \\vz_t": "NOT MENTIONED",
        "h": "Convex function (used via its convexity inequality)"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:40:53.325019"
    },
    {
      "label": "<<FORMULA_0354>>",
      "formula": "&\\quad \\langle \\nabla h(\\tilde \\vz_{t+1}) + \\vu_{t+1}, \\tilde \\vz_{t+1} - \\bar \\vz_t \\rangle \\\\\n     &=\\frac{1}{2 \\lambda_{t+1}} \\Vert \\nabla h(\\tilde \\vz_{t+1}) + \\vu_{t+1} + \\lambda_{t+1} (\\tilde \\vz_{t\\\\times1} - \\bar \\vz_t) \\Vert^2 \\\\\n     &\\quad - \\frac{1}{2 \\lambda_{t+1} } \\Vert \\nabla h(\\tilde \\vz_{t+1}) + \\vu_{t+1} \\Vert^2  - \\frac{\\lambda_{t+1}}{2} \\Vert \\tilde \\vz_{t+1} - \\bar \\vz_t \\Vert^2.",
      "raw_latex": "\\begin{align*}\n     &\\quad \\langle \\nabla h(\\tilde \\vz_{t+1}) + \\vu_{t+1}, \\tilde \\vz_{t+1} - \\bar \\vz_t \\rangle \\\\\n     &=\\frac{1}{2 \\lambda_{t+1}} \\Vert \\nabla h(\\tilde \\vz_{t+1}) + \\vu_{t+1} + \\lambda_{t+1} (\\tilde \\vz_{t\\\\times1} - \\bar \\vz_t) \\Vert^2 \\\\\n     &\\quad - \\frac{1}{2 \\lambda_{t+1} } \\Vert \\nabla h(\\tilde \\vz_{t+1}) + \\vu_{t+1} \\Vert^2  - \\frac{\\lambda_{t+1}}{2} \\Vert \\tilde \\vz_{t+1} - \\bar \\vz_t \\Vert^2.\n\\end{align*}",
      "formula_type": "align*",
      "line_number": 1980,
      "is_formula": true,
      "high_level_explanation": "This identity rewrites the inner product between the gradient term plus an error vector and the displacement (tilde z_{t+1} minus bar z_t) as a difference of squared norms, obtained by completing the square with a free positive parameter lambda_{t+1}. Such a rewrite is standard in first- and second-order optimization analyses to facilitate upper bounds via norm inequalities or oracle conditions. It allows the inner product to be controlled by introducing and subtracting quadratic terms in the two vectors involved. The form is valid in any Hilbert space and is used here to plug into subsequent inequalities in the proof.",
      "notations": {
        "h": "convex function (by context)",
        "\\tilde \\vz_{t+1}": "NOT MENTIONED",
        "\\bar \\vz_t": "NOT MENTIONED",
        "\\vu_{t+1}": "NOT MENTIONED",
        "\\lambda_{t+1}": "NOT MENTIONED",
        "\\tilde \\vz_{t\\\\times1}": "NOT MENTIONED"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:41:43.578697"
    },
    {
      "label": "<<FORMULA_0355>>",
      "formula": "&\\quad a_{t+1}' \\langle \\nabla h(\\tilde \\vz_{t+1}) + \\vu_{t+1}, \\vz^* - \\vv_t \\rangle \\\\\n    &\\le a_{t+1}' ( h(\\vz^*) - h(\\tilde \\vz_{t+1})  )  + A_t (h(\\vz_t) - h(\\tilde \\vz_{t+1})) - \\frac{A_{t+1}'}{2 \\lambda_{t+1} } \\Vert \\nabla h(\\tilde \\vz_{t+1}) + \\vu_{t+1} \\Vert^2 \\\\\n    &\\quad + \\frac{A_{t+1}'}{2 \\lambda_{t+1}} \\Vert \\nabla h(\\tilde \\vz_{t+1}) + \\vu_{t+1} + \\lambda_{t+1} (\\tilde \\vz_{t+1} - \\bar \\vz_t) \\Vert^2  - \\frac{A_{t+1}' \\lambda_{t+1}}{2} \\Vert \\tilde \\vz_{t+1} - \\bar \\vz_t \\Vert^2.",
      "raw_latex": "\\begin{align*}\n    &\\quad a_{t+1}' \\langle \\nabla h(\\tilde \\vz_{t+1}) + \\vu_{t+1}, \\vz^* - \\vv_t \\rangle \\\\\n    &\\le a_{t+1}' ( h(\\vz^*) - h(\\tilde \\vz_{t+1})  )  + A_t (h(\\vz_t) - h(\\tilde \\vz_{t+1})) - \\frac{A_{t+1}'}{2 \\lambda_{t+1} } \\Vert \\nabla h(\\tilde \\vz_{t+1}) + \\vu_{t+1} \\Vert^2 \\\\\n    &\\quad + \\frac{A_{t+1}'}{2 \\lambda_{t+1}} \\Vert \\nabla h(\\tilde \\vz_{t+1}) + \\vu_{t+1} + \\lambda_{t+1} (\\tilde \\vz_{t+1} - \\bar \\vz_t) \\Vert^2  - \\frac{A_{t+1}' \\lambda_{t+1}}{2} \\Vert \\tilde \\vz_{t+1} - \\bar \\vz_t \\Vert^2.\n\\end{align*}",
      "formula_type": "align*",
      "line_number": 1986,
      "is_formula": true,
      "high_level_explanation": "This inequality upper-bounds the weighted inner product between the gradient-plus-error at the trial point and the displacement vector by convexity-based function value gaps and quadratic terms. It uses the standard quadratic identity to rewrite an inner product as a difference of squared norms, introducing a proximal-like term scaled by \\lambda_{t+1}. The coefficients a'_{t+1}, A_t, and A'_{t+1} act as weights that will allow telescoping or aggregation in the subsequent analysis with an inexact oracle.",
      "notations": {
        "a_{t+1}'": "NOT MENTIONED",
        "A_t": "NOT MENTIONED",
        "A_{t+1}'": "NOT MENTIONED",
        "\\lambda_{t+1}": "NOT MENTIONED",
        "h": "A convex function (by the stated use of convexity of h)",
        "\\tilde \\vz_{t+1}": "NOT MENTIONED",
        "\\bar \\vz_t": "NOT MENTIONED",
        "\\vz_t": "NOT MENTIONED",
        "\\vv_t": "Algorithm iterate at step t (mentioned via its update rule)",
        "\\vz^*": "NOT MENTIONED",
        "\\vu_{t+1}": "NOT MENTIONED",
        "\\nabla h(\\tilde \\vz_{t+1})": "Gradient of h evaluated at \\tilde \\vz_{t+1}"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:40:51.057476"
    },
    {
      "label": "<<FORMULA_0356>>",
      "formula": "\\label{eq:MS-2}\n\\begin{split}\n     &\\quad a_{t+1}' \\langle \\nabla h(\\tilde \\vz_{t\\\\times1}) + \\vu_{t+1}, \\vz^* - \\vv_t \\rangle \\\\\n    &\\le a_{t+1}' ( h(\\vz^*) - f(\\tilde \\vz_{t+1})  )  + A_t (h(\\vz_t) - h(\\tilde \\vz_{t+1})) \\\\\n    &\\quad - \\frac{A_{t+1}'}{2 \\lambda_{t+1} } \\Vert \\nabla h(\\tilde \\vz_{t+1}) + \\vu_{t+1} \\Vert^2 - \\frac{3}{4} \\lambda_{t+1} A_{t+1}'  N_{t+1} + \\frac{A_{t+1}'}{2 \\lambda_{t+1}} \\delta^2.\n\\end{split}",
      "raw_latex": "\\begin{align} \\label{eq:MS-2}\n\\begin{split}\n     &\\quad a_{t+1}' \\langle \\nabla h(\\tilde \\vz_{t\\\\times1}) + \\vu_{t+1}, \\vz^* - \\vv_t \\rangle \\\\\n    &\\le a_{t+1}' ( h(\\vz^*) - f(\\tilde \\vz_{t+1})  )  + A_t (h(\\vz_t) - h(\\tilde \\vz_{t+1})) \\\\\n    &\\quad - \\frac{A_{t+1}'}{2 \\lambda_{t+1} } \\Vert \\nabla h(\\tilde \\vz_{t+1}) + \\vu_{t+1} \\Vert^2 - \\frac{3}{4} \\lambda_{t+1} A_{t+1}'  N_{t+1} + \\frac{A_{t+1}'}{2 \\lambda_{t+1}} \\delta^2.\n\\end{split}\n\\end{align}",
      "formula_type": "align",
      "line_number": 1993,
      "is_formula": true,
      "high_level_explanation": "This inequality upper-bounds a weighted inner product between the gradient of a convex function h at an approximate point (plus an error term) and the displacement to a reference point. The bound decomposes into decreases of h (and f at the trial point), minus penalization terms proportional to the squared norm of the gradient-plus-error and another oracle-related quantity N_{t+1}, plus a small residual proportional to delta^2. It is a key progress inequality used in the convergence analysis under convexity of h and properties of an inexact oracle.",
      "notations": {
        "a_{t+1}'": "NOT MENTIONED",
        "A_t": "NOT MENTIONED",
        "A_{t+1}'": "NOT MENTIONED",
        "\\lambda_{t+1}": "NOT MENTIONED",
        "\\vu_{t+1}": "NOT MENTIONED",
        "\\vz^*": "NOT MENTIONED",
        "\\vv_t": "NOT MENTIONED",
        "\\tilde \\vz_{t+1}": "NOT MENTIONED",
        "\\tilde \\vz_{t\\\\times1}": "NOT MENTIONED",
        "h": "Convex function (used via its convexity in the derivation)",
        "f": "NOT MENTIONED",
        "N_{t+1}": "NOT MENTIONED",
        "delta": "NOT MENTIONED"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:42:14.859152"
    },
    {
      "label": "<<FORMULA_0434>>",
      "formula": "f(\\vx_2,\\vy_1) - f(\\vx_1,\\vy_1) &\\ge \\langle \\nabla_x f(\\vx_1,\\vy_1) + \\vu_1, \\vx_2 - \\vx_1 \\rangle + \\frac{\\mu}{p} \\Vert \\vx_2 - \\vx_1 \\Vert^p, \\quad \\vu_1 \\in \\partial \\gI_{\\gX} (\\vx_1); \\\\\n    f(\\vx_1,\\vy_2) - f(\\vx_2,\\vy_2) &\\ge \\langle \\nabla_x f(\\vx_2, \\vy_2) + \\vu_2, \\vx_1 - \\vx_2 \\rangle + \\frac{\\mu}{p} \\Vert \\vx_2 - \\vx_1 \\Vert^p, \\quad \\vu_2 \\in \\partial \\gI_{\\gX} (\\vx_2).",
      "raw_latex": "\\begin{align*}\n    f(\\vx_2,\\vy_1) - f(\\vx_1,\\vy_1) &\\ge \\langle \\nabla_x f(\\vx_1,\\vy_1) + \\vu_1, \\vx_2 - \\vx_1 \\rangle + \\frac{\\mu}{p} \\Vert \\vx_2 - \\vx_1 \\Vert^p, \\quad \\vu_1 \\in \\partial \\gI_{\\gX} (\\vx_1); \\\\\n    f(\\vx_1,\\vy_2) - f(\\vx_2,\\vy_2) &\\ge \\langle \\nabla_x f(\\vx_2, \\vy_2) + \\vu_2, \\vx_1 - \\vx_2 \\rangle + \\frac{\\mu}{p} \\Vert \\vx_2 - \\vx_1 \\Vert^p, \\quad \\vu_2 \\in \\partial \\gI_{\\gX} (\\vx_2).\n\\end{align*}",
      "formula_type": "align*",
      "line_number": 2170,
      "is_formula": true,
      "high_level_explanation": "These two inequalities are first-order lower bounds on the change of the function when varying the x-variable, implied by uniform convexity in x and incorporating constraints via normal-cone terms. Each bound compares f at two x-points (with y fixed) to the linearization at one point plus a growth term proportional to the p-th power of the step size, with an additional vector from the subdifferential of the indicator of the feasible set. Together with analogous inequalities in y, they are used to establish a monotonicity property for uniformly convex–concave saddle-point problems.",
      "notations": {
        "f": "Function that is mu-uniformly convex in x and mu-uniformly concave in y (from context)",
        "\\vx_1": "Point in the x-domain (feasible set \\gX)",
        "\\vx_2": "Point in the x-domain (feasible set \\gX)",
        "\\vy_1": "Point in the y-domain",
        "\\vy_2": "Point in the y-domain",
        "\\nabla_x f(\\vx_1,\\vy_1)": "Gradient of f with respect to x evaluated at (\\vx_1, \\vy_1)",
        "\\nabla_x f(\\vx_2, \\vy_2)": "Gradient of f with respect to x evaluated at (\\vx_2, \\vy_2)",
        "\\vu_1": "A vector in the normal cone to \\gX at \\vx_1 (i.e., an element of \\partial \\gI_{\\gX}(\\vx_1))",
        "\\vu_2": "A vector in the normal cone to \\gX at \\vx_2 (i.e., an element of \\partial \\gI_{\\gX}(\\vx_2))",
        "\\partial \\gI_{\\gX}(\\vx_1)": "Subdifferential (normal cone) of the indicator function of \\gX at \\vx_1",
        "\\partial \\gI_{\\gX}(\\vx_2)": "Subdifferential (normal cone) of the indicator function of \\gX at \\vx_2",
        "\\gI_{\\gX}": "Indicator function of the set \\gX (0 on \\gX, +\\infty outside)",
        "\\gX": "Feasible set for the x-variable",
        "\\mu": "Uniform convexity/concavity modulus",
        "p": "NOT MENTIONED"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:42:39.865148"
    },
    {
      "label": "<<FORMULA_0435>>",
      "formula": "-f(\\vx_1,\\vy_2) + f(\\vx_1,\\vy_1) &\\ge \\langle -\\nabla_y f(\\vx_1,\\vy_1) +\\vv_1, \\vy_2 - \\vy_1 \\rangle + \\frac{\\mu}{p} \\Vert \\vy_2 - \\vy_1 \\Vert^p, \\quad \\vv_1\\in \\partial \\gI_{\\gY}(\\vy_1);  \\\\\n    -f(\\vx_2,\\vy_1) + f(\\vx_2,\\vy_2) &\\ge  \\langle -\\nabla_y f(\\vx_2,\\vy_2) + \\vv_2, \\vy_1 - \\vy_2 \\rangle + \\frac{p}{\\mu} \\Vert \\vy_2 - \\vy_1 \\Vert^p, \\quad \\vv_2 \\in \\partial \\gI_{\\gY} (\\vy_2).",
      "raw_latex": "\\begin{align*}\n    -f(\\vx_1,\\vy_2) + f(\\vx_1,\\vy_1) &\\ge \\langle -\\nabla_y f(\\vx_1,\\vy_1) +\\vv_1, \\vy_2 - \\vy_1 \\rangle + \\frac{\\mu}{p} \\Vert \\vy_2 - \\vy_1 \\Vert^p, \\quad \\vv_1\\in \\partial \\gI_{\\gY}(\\vy_1);  \\\\\n    -f(\\vx_2,\\vy_1) + f(\\vx_2,\\vy_2) &\\ge  \\langle -\\nabla_y f(\\vx_2,\\vy_2) + \\vv_2, \\vy_1 - \\vy_2 \\rangle + \\frac{p}{\\mu} \\Vert \\vy_2 - \\vy_1 \\Vert^p, \\quad \\vv_2 \\in \\partial \\gI_{\\gY} (\\vy_2).\n\\end{align*}",
      "formula_type": "align*",
      "line_number": 2175,
      "is_formula": true,
      "high_level_explanation": "These two inequalities bound how the function value changes when varying the y-variable while holding x fixed, using a first-order term in y plus a p-power regularization term. The vectors v1 and v2 come from the subdifferential of the indicator of the feasible set Y and capture the effect of y-constraints. The constants μ and p quantify the uniform convexity/concavity structure that yields the ||y2 − y1||^p terms. Such bounds are typically combined to establish a monotonicity property or stability inequality.",
      "notations": {
        "f": "Function that is μ-uniformly convex in x and μ-uniformly concave in y (as stated in the lemma)",
        "\\vx_1": "NOT MENTIONED",
        "\\vx_2": "NOT MENTIONED",
        "\\vy_1": "NOT MENTIONED",
        "\\vy_2": "NOT MENTIONED",
        "\\vv_1": "A vector in the subdifferential of the indicator function of \\gY at \\vy_1",
        "\\vv_2": "A vector in the subdifferential of the indicator function of \\gY at \\vy_2",
        "\\partial \\gI_{\\gY}(\\vy_1)": "Subdifferential of the indicator function of the set \\gY at \\vy_1",
        "\\partial \\gI_{\\gY} (\\vy_2)": "Subdifferential of the indicator function of the set \\gY at \\vy_2",
        "\\gI_{\\gY}": "Indicator function of the set \\gY (0 on \\gY and +∞ outside)",
        "\\gY": "NOT MENTIONED",
        "\\mu": "Modulus in the uniform convexity/concavity assumption",
        "p": "NOT MENTIONED"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:42:10.830640"
    },
    {
      "label": "<<FORMULA_0497>>",
      "formula": "&\\quad f(\\vx^{\\rm out},\\vy^*(\\vx^{\\rm out})) - f(\\vx^*(\\vy^{\\rm out}),\\vy^{\\rm out}) \\\\\n &= f(\\vx^{\\rm out},\\vy^*(\\vx^{\\rm out})) -  f(\\vx^{\\rm out},\\vy^{\\rm out}) - f(\\vx^{\\rm out},\\vy^{\\rm out}) -  f(\\vx^*(\\vy^{\\rm out}),\\vy^{\\rm out}) \\\\\n &\\le \\langle \\nabla_x f(\\vx^{\\rm out},\\vy^{\\rm out}) + \\vu^{\\rm out}, \\vx^*(\\vy^{\\rm out}) -\\vx^{\\rm out} \\rangle \\\\\n &\\quad + \\langle - \\nabla_y f(\\vx^{\\rm out},\\vy^{\\rm out}) + \\vv^{\\rm out},  \\vy^*(\\vx^{\\rm out}) - \\vy^{\\rm out} \\rangle.",
      "raw_latex": "\\begin{align*}\n &\\quad f(\\vx^{\\rm out},\\vy^*(\\vx^{\\rm out})) - f(\\vx^*(\\vy^{\\rm out}),\\vy^{\\rm out}) \\\\\n &= f(\\vx^{\\rm out},\\vy^*(\\vx^{\\rm out})) -  f(\\vx^{\\rm out},\\vy^{\\rm out}) - f(\\vx^{\\rm out},\\vy^{\\rm out}) -  f(\\vx^*(\\vy^{\\rm out}),\\vy^{\\rm out}) \\\\\n &\\le \\langle \\nabla_x f(\\vx^{\\rm out},\\vy^{\\rm out}) + \\vu^{\\rm out}, \\vx^*(\\vy^{\\rm out}) -\\vx^{\\rm out} \\rangle \\\\\n &\\quad + \\langle - \\nabla_y f(\\vx^{\\rm out},\\vy^{\\rm out}) + \\vv^{\\rm out},  \\vy^*(\\vx^{\\rm out}) - \\vy^{\\rm out} \\rangle.\n\\end{align*}",
      "formula_type": "align*",
      "line_number": 2344,
      "is_formula": true,
      "high_level_explanation": "The display decomposes the primal–dual gap between the two best-response evaluations of f and bounds it using first-order convexity/concavity inequalities around the pair (x^{out}, y^{out}). Specifically, it splits f(x^{out}, y*(x^{out})) − f(x*(y^{out}), y^{out}) into two directional differences and upper-bounds each by an inner product between a (sub)gradient term and the displacement to the corresponding best response. The vectors u^{out} and v^{out} account for inexactness or perturbations in the x- and y-side gradients, respectively. This establishes a computable upper bound on the gap in terms of gradients at (x^{out}, y^{out}) and the distances to the best responses.",
      "notations": {
        "f": "NOT MENTIONED",
        "\\vx^{\\rm out}": "NOT MENTIONED",
        "\\vy^{\\rm out}": "NOT MENTIONED",
        "\\vy^*(\\vx^{\\rm out})": "Maximizer of f(\\vx^{\\rm out}, \\vy) over \\vy (i.e., arg max_{\\vy \\in \\gY} f(\\vx^{\\rm out}, \\vy))",
        "\\vx^*(\\vy^{\\rm out})": "NOT MENTIONED",
        "\\vu^{\\rm out}": "NOT MENTIONED",
        "\\vv^{\\rm out}": "NOT MENTIONED"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:41:06.131652"
    },
    {
      "label": "<<FORMULA_0508>>",
      "formula": "\\label{eq:cont-mid}\n\\begin{split}\n     \\Vert \\vx^*(\\vy_1; \\bar \\vx) - \\vx^*(\\vy_2; \\bar \\vx) \\Vert^2 &\\le \\frac{\\ell +2 \\gamma D}{\\gamma/2} \\Vert \\vy_1 - \\vy_2 \\Vert, \\quad \\forall \\vy_1,\\vy_2 \\in \\gY; \\\\\n    \\Vert \\vy^*(\\vx_1; \\bar \\vx) - \\vy^*(\\vx_2; \\bar \\vx) \\Vert^2 &\\le \\frac{\\ell}{\\mu_y} \\Vert \\vx_1 - \\vx_2 \\Vert, \\quad \\forall \\vx_1,\\vx_2 \\in \\gX,\n\\end{split}\n    % \\Vert  \\vx^*(\\vy_1; \\bar\\vx , \\bar \\vy) - \\vx^*(\\vy_2; \\bar\\vx , \\bar \\vy) \\Vert^2 &\\le \\frac{\\ell - 2 \\rho D}{\\rho} \\Vert \\vy_1 - \\vy_2 \\Vert, \\quad \\forall \\vy_1, \\vy_2 \\in \\gY; \\\\",
      "raw_latex": "\\begin{align} \\label{eq:cont-mid}\n\\begin{split}\n     \\Vert \\vx^*(\\vy_1; \\bar \\vx) - \\vx^*(\\vy_2; \\bar \\vx) \\Vert^2 &\\le \\frac{\\ell +2 \\gamma D}{\\gamma/2} \\Vert \\vy_1 - \\vy_2 \\Vert, \\quad \\forall \\vy_1,\\vy_2 \\in \\gY; \\\\\n    \\Vert \\vy^*(\\vx_1; \\bar \\vx) - \\vy^*(\\vx_2; \\bar \\vx) \\Vert^2 &\\le \\frac{\\ell}{\\mu_y} \\Vert \\vx_1 - \\vx_2 \\Vert, \\quad \\forall \\vx_1,\\vx_2 \\in \\gX,\n\\end{split}\n    % \\Vert  \\vx^*(\\vy_1; \\bar\\vx , \\bar \\vy) - \\vx^*(\\vy_2; \\bar\\vx , \\bar \\vy) \\Vert^2 &\\le \\frac{\\ell - 2 \\rho D}{\\rho} \\Vert \\vy_1 - \\vy_2 \\Vert, \\quad \\forall \\vy_1, \\vy_2 \\in \\gY; \\\\\n\\end{align}",
      "formula_type": "align",
      "line_number": 2369,
      "is_formula": true,
      "high_level_explanation": "The inequalities provide continuity bounds for the solution mappings that return optimal responses in a minimax (or saddle-point) problem. They state that the squared distance between best responses changes at most linearly with the distance between the corresponding inputs, implying Hölder-1/2 continuity of these mappings. The constants governing these bounds depend on problem parameters (ℓ, γ, D, μ_y) and the statements hold uniformly over all inputs in the feasible sets X and Y.",
      "notations": {
        "\\vx^*(\\vy_1; \\bar \\vx)": "NOT MENTIONED",
        "\\vx^*(\\vy_2; \\bar \\vx)": "NOT MENTIONED",
        "\\vy^*(\\vx_1; \\bar \\vx)": "NOT MENTIONED",
        "\\vy^*(\\vx_2; \\bar \\vx)": "NOT MENTIONED",
        "\\bar \\vx": "NOT MENTIONED",
        "\\gY": "NOT MENTIONED",
        "\\gX": "NOT MENTIONED",
        "\\ell": "NOT MENTIONED",
        "\\gamma": "NOT MENTIONED",
        "D": "NOT MENTIONED",
        "\\mu_y": "NOT MENTIONED",
        "\\vy_1": "Arbitrary element of \\gY",
        "\\vy_2": "Arbitrary element of \\gY",
        "\\vx_1": "Arbitrary element of \\gX",
        "\\vx_2": "Arbitrary element of \\gX"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:40:39.388917"
    },
    {
      "label": "<<FORMULA_0511>>",
      "formula": "\\label{eq:xxx-plug}\n\\begin{split}\n\\Vert \\vx^{\\rm out} - \\vx^*(\\bar \\vx) \\Vert &\\le \\Vert \\vx^{\\rm out} - \\vx^*(\\hat \\vy; \\bar \\vx) \\Vert +  \\frac{(\\ell + 2 \\gamma D )^{1/2}}{(\\gamma/2)^{1/2}} \\sqrt{ \\Vert \\hat \\vy - \\vy^*(\\bar \\vx) \\Vert} \\\\\n&\\le \\frac{1}{\\gamma/2^{1/2}} \\sqrt{ \\Vert \\nabla_x g(\\vx^{\\rm out},\\hat \\vy; \\bar \\vx) + \\vu^{\\rm out} \\Vert } + \\frac{(\\ell + 2 \\gamma D )^{1/2}}{(\\gamma/2)^{1/2}} \\sqrt{ \\Vert \\hat \\vy - \\vy^*(\\bar \\vx) \\Vert},\n\\end{split}",
      "raw_latex": "\\begin{align} \\label{eq:xxx-plug}\n\\begin{split}\n\\Vert \\vx^{\\rm out} - \\vx^*(\\bar \\vx) \\Vert &\\le \\Vert \\vx^{\\rm out} - \\vx^*(\\hat \\vy; \\bar \\vx) \\Vert +  \\frac{(\\ell + 2 \\gamma D )^{1/2}}{(\\gamma/2)^{1/2}} \\sqrt{ \\Vert \\hat \\vy - \\vy^*(\\bar \\vx) \\Vert} \\\\\n&\\le \\frac{1}{\\gamma/2^{1/2}} \\sqrt{ \\Vert \\nabla_x g(\\vx^{\\rm out},\\hat \\vy; \\bar \\vx) + \\vu^{\\rm out} \\Vert } + \\frac{(\\ell + 2 \\gamma D )^{1/2}}{(\\gamma/2)^{1/2}} \\sqrt{ \\Vert \\hat \\vy - \\vy^*(\\bar \\vx) \\Vert},\n\\end{split}\n\\end{align}",
      "formula_type": "align",
      "line_number": 2378,
      "is_formula": true,
      "high_level_explanation": "This pair of inequalities upper-bounds the distance between the returned primal iterate x_out and the target solution x*(bar x). The first inequality uses the Lipschitz continuity of the solution map y ↦ x*(y; bar x) to translate the y-approximation error ||hat y − y*(bar x)|| into a bound on ||x_out − x*(bar x)||. The second inequality relates the primal distance to a stationarity residual measured by the x-gradient of g plus the perturbation u_out. Overall, it shows how inaccuracies in the y-variable and in first-order optimality jointly control the final primal error.",
      "notations": {
        "\\vx^{\\rm out}": "NOT MENTIONED",
        "\\bar \\vx": "NOT MENTIONED",
        "\\vx^*(\\bar \\vx)": "NOT MENTIONED",
        "\\vx^*(\\hat \\vy; \\bar \\vx)": "NOT MENTIONED",
        "\\hat \\vy": "NOT MENTIONED",
        "\\vy^*(\\bar \\vx)": "NOT MENTIONED",
        "\\ell": "NOT MENTIONED",
        "\\gamma": "NOT MENTIONED",
        "D": "NOT MENTIONED",
        "\\nabla_x g(\\vx^{\\rm out},\\hat \\vy; \\bar \\vx)": "x-gradient of g evaluated at (\\vx^{\\rm out}, \\hat \\vy; \\bar \\vx)",
        "g": "NOT MENTIONED",
        "\\vu^{\\rm out}": "NOT MENTIONED"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:42:21.972702"
    },
    {
      "label": "<<FORMULA_0513>>",
      "formula": "&\\quad \\left \\Vert \\nabla \\Phi(\\vx^{\\rm out}) + \\gamma \\Vert \\vx^{\\rm out} - \\bar \\vx \\Vert (\\vx^{\\rm out} - \\bar \\vx) + \\vu^{\\rm out}  \\right \\Vert = \\Vert \\nabla_x g(\\vx^{\\rm out}, \\vy^*(\\vx^{\\rm out}; \\bar \\vx) ;\\bar \\vx) - \\vu^{\\rm out}  \\Vert \\\\\n    &\\le     \\Vert \\nabla_x g(\\vx^{\\rm out}, \\vy^*(\\bar \\vx) ;\\bar \\vx) + \\vu^{\\rm out}  \\Vert + \\frac{(\\ell + 2 \\rho D) \\ell^{1/2}}{\\mu_y^{1/2}} \\sqrt{\\Vert \\vx^{\\rm out}  - \\vx^*(\\bar \\vx) \\Vert} \\\\\n    &\\le \\Vert \\nabla_x g(\\vx^{\\rm out}, \\hat \\vy; \\bar \\vx) + \\vu^{\\rm out} \\Vert + (\\ell +2 \\rho D) \\Vert \\hat \\vy - \\vy^*(\\bar \\vx) \\Vert + \\frac{(\\ell + 2 \\rho D) \\ell^{1/2}}{\\mu_y^{1/2}} \\sqrt{\\Vert \\vx^{\\rm out}  - \\vx^*(\\bar \\vx) \\Vert},",
      "raw_latex": "\\begin{align*}\n    &\\quad \\left \\Vert \\nabla \\Phi(\\vx^{\\rm out}) + \\gamma \\Vert \\vx^{\\rm out} - \\bar \\vx \\Vert (\\vx^{\\rm out} - \\bar \\vx) + \\vu^{\\rm out}  \\right \\Vert = \\Vert \\nabla_x g(\\vx^{\\rm out}, \\vy^*(\\vx^{\\rm out}; \\bar \\vx) ;\\bar \\vx) - \\vu^{\\rm out}  \\Vert \\\\\n    &\\le     \\Vert \\nabla_x g(\\vx^{\\rm out}, \\vy^*(\\bar \\vx) ;\\bar \\vx) + \\vu^{\\rm out}  \\Vert + \\frac{(\\ell + 2 \\rho D) \\ell^{1/2}}{\\mu_y^{1/2}} \\sqrt{\\Vert \\vx^{\\rm out}  - \\vx^*(\\bar \\vx) \\Vert} \\\\\n    &\\le \\Vert \\nabla_x g(\\vx^{\\rm out}, \\hat \\vy; \\bar \\vx) + \\vu^{\\rm out} \\Vert + (\\ell +2 \\rho D) \\Vert \\hat \\vy - \\vy^*(\\bar \\vx) \\Vert + \\frac{(\\ell + 2 \\rho D) \\ell^{1/2}}{\\mu_y^{1/2}} \\sqrt{\\Vert \\vx^{\\rm out}  - \\vx^*(\\bar \\vx) \\Vert},\n\\end{align*}",
      "formula_type": "align*",
      "line_number": 2385,
      "is_formula": true,
      "high_level_explanation": "This set of inequalities relates the proximal-gradient residual of a value function Φ at the point x_out (with proximal center bar x and parameter γ) to the x-gradient of an auxiliary function g evaluated at an associated inner optimizer y*. The first line rewrites the residual exactly via the gradient of g, and the following lines upper-bound it using continuity/Lipschitz properties of the solution map and by substituting an approximate inner solution \\hat y. The resulting bounds decompose the error into contributions from the inner-solution inaccuracy (\\|\\hat y - y^*(\\bar x)\\|) and from the distance of x_out to a target solution set (\\|x_out - x^*(\\bar x)\\|), scaled by problem-dependent constants.",
      "notations": {
        "\\Phi": "NOT MENTIONED",
        "\\vx^{\\rm out}": "NOT MENTIONED",
        "\\bar \\vx": "NOT MENTIONED",
        "\\gamma": "NOT MENTIONED",
        "\\vu^{\\rm out}": "NOT MENTIONED",
        "g": "NOT MENTIONED",
        "\\vy^*(\\vx^{\\rm out}; \\bar \\vx)": "NOT MENTIONED",
        "\\vy^*(\\bar \\vx)": "NOT MENTIONED",
        "\\hat \\vy": "NOT MENTIONED",
        "\\vx^*(\\bar \\vx)": "NOT MENTIONED",
        "\\ell": "NOT MENTIONED",
        "\\rho": "NOT MENTIONED",
        "D": "NOT MENTIONED",
        "\\mu_y": "NOT MENTIONED"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:40:51.868452"
    },
    {
      "label": "<<FORMULA_0515>>",
      "formula": "\\label{eq:prox-x-1}\n\\begin{split}\n&\\quad   \\Vert \\nabla_x g(\\vx^{\\rm out}, \\vy^*(\\vx^{\\rm out}; \\bar \\vx) ;\\bar \\vx) + \\vu^{\\rm out}  \\Vert \\\\\n&\\\\ge 6 (\\ell + 2 \\gamma D) \\zeta_2 +  \\frac{(\\ell + 2 \\rho D) \\ell^{1/2}}{\\mu_y^{1/2} (\\gamma/2)^{1/4}} \\left( \n   6 (\\ell+2\\gamma D) \\zeta_2\n    \\right)^{1/4} \\\\\n    &\\quad + (\\ell + 2\\rho D) \\zeta_2 +  \\frac{(\\ell + 2 \\rho D) \\ell^{1/2} (\\ell + 2 \\gamma D)^{1/4}}{\\mu_y^{1/2} (\\gamma/2)^{1/4}} (\\zeta_2)^{1/2},\n\\end{split}",
      "raw_latex": "\\begin{align} \\label{eq:prox-x-1}\n\\begin{split}\n&\\quad   \\Vert \\nabla_x g(\\vx^{\\rm out}, \\vy^*(\\vx^{\\rm out}; \\bar \\vx) ;\\bar \\vx) + \\vu^{\\rm out}  \\Vert \\\\\n&\\\\ge 6 (\\ell + 2 \\gamma D) \\zeta_2 +  \\frac{(\\ell + 2 \\rho D) \\ell^{1/2}}{\\mu_y^{1/2} (\\gamma/2)^{1/4}} \\left( \n   6 (\\ell+2\\gamma D) \\zeta_2\n    \\right)^{1/4} \\\\\n    &\\quad + (\\ell + 2\\rho D) \\zeta_2 +  \\frac{(\\ell + 2 \\rho D) \\ell^{1/2} (\\ell + 2 \\gamma D)^{1/4}}{\\mu_y^{1/2} (\\gamma/2)^{1/4}} (\\zeta_2)^{1/2},\n\\end{split}\n\\end{align}",
      "formula_type": "align",
      "line_number": 2392,
      "is_formula": true,
      "high_level_explanation": "This inequality bounds the magnitude of the x-gradient residual of g at the algorithm’s output point x^{out} (evaluated at the best-response y^*(x^{out}; ̅x)) plus the inexactness vector u^{out} in terms of the accuracy parameter ζ_2. The right-hand side is a combination of linear, square-root, and fourth-root terms in ζ_2 with coefficients determined by problem constants (ℓ, ρ, γ, μ_y, D). It is obtained by plugging previously derived Lipschitz/strong-convexity-based estimates into earlier bounds and is used to certify the quality of an inexact proximal oracle.",
      "notations": {
        "\\vx^{\\rm out}": "Output primal iterate produced by the algorithm",
        "\\vy^*(\\vx^{\\rm out}; \\bar \\vx)": "NOT MENTIONED",
        "\\bar \\vx": "NOT MENTIONED",
        "\\vu^{\\rm out}": "NOT MENTIONED",
        "g": "NOT MENTIONED",
        "\\ell": "NOT MENTIONED",
        "\\gamma": "NOT MENTIONED",
        "D": "NOT MENTIONED",
        "\\rho": "NOT MENTIONED",
        "\\mu_y": "NOT MENTIONED",
        "\\zeta_2": "Accuracy/tolerance parameter used to bound inner-solution error (e.g., \\Vert \\vy' - \\vy^*(\\vx) \\Vert \\le \\zeta_2)"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:42:14.812252"
    },
    {
      "label": "<<FORMULA_0525>>",
      "formula": "\\Vert  \\vx^*(\\vy_1; \\bar\\vx , \\bar \\vy) - \\vx^*(\\vy_2; \\bar\\vx , \\bar \\vy) \\Vert^2 &\\le \\frac{\\gamma /2 }{\\ell + 2 \\gamma D} \\Vert \\vy_1 - \\vy_2 \\Vert, \\quad \\forall \\vy_1, \\vy_2 \\in \\gY,\n    % \\\\\n    % \\Vert \\vx^*(\\vy_1; \\bar \\vx) - \\vx^*(\\vy_2; \\bar \\vx) \\Vert^2 &\\le \\frac{\\ell +2 \\rho D}{\\rho} \\Vert \\vy_1 - \\vy_2 \\Vert, \\quad \\forall \\vy_1,\\vy_2 \\in \\gY; \\\\\n    % \\Vert \\vy^*(\\vx_1; \\bar \\vx) - \\vy^*(\\vx_2; \\bar \\vx) \\Vert^2 &\\le \\frac{\\ell}{\\mu_y} \\Vert \\vx_1 - \\vx_2 \\Vert, \\quad \\forall \\vx_1,\\vx_2 \\in \\gX.",
      "raw_latex": "\\begin{align*}\n    \\Vert  \\vx^*(\\vy_1; \\bar\\vx , \\bar \\vy) - \\vx^*(\\vy_2; \\bar\\vx , \\bar \\vy) \\Vert^2 &\\le \\frac{\\gamma /2 }{\\ell + 2 \\gamma D} \\Vert \\vy_1 - \\vy_2 \\Vert, \\quad \\forall \\vy_1, \\vy_2 \\in \\gY,\n    % \\\\\n    % \\Vert \\vx^*(\\vy_1; \\bar \\vx) - \\vx^*(\\vy_2; \\bar \\vx) \\Vert^2 &\\le \\frac{\\ell +2 \\rho D}{\\rho} \\Vert \\vy_1 - \\vy_2 \\Vert, \\quad \\forall \\vy_1,\\vy_2 \\in \\gY; \\\\\n    % \\Vert \\vy^*(\\vx_1; \\bar \\vx) - \\vy^*(\\vx_2; \\bar \\vx) \\Vert^2 &\\le \\frac{\\ell}{\\mu_y} \\Vert \\vx_1 - \\vx_2 \\Vert, \\quad \\forall \\vx_1,\\vx_2 \\in \\gX.\n\\end{align*}",
      "formula_type": "align*",
      "line_number": 2425,
      "is_formula": true,
      "high_level_explanation": "The inequality bounds how much the x-solution of the inner problem changes when the parameter y changes. Specifically, the squared distance between x*(y1; bar x, bar y) and x*(y2; bar x, bar y) is at most ((gamma/2)/(ell + 2 gamma D)) times the distance between y1 and y2, for all y1, y2 in the set Y. Equivalently, the solution map y ↦ x*(y; bar x, bar y) is 1/2-Hölder continuous with modulus sqrt((gamma/2)/(ell + 2 gamma D)).",
      "notations": {
        "\\vx^*(\\vy_1; \\bar\\vx , \\bar \\vy)": "Minimizer in x of h(x, vy_1; bar x, bar y) (as defined in the context)",
        "\\vx^*(\\vy_2; \\bar\\vx , \\bar \\vy)": "Minimizer in x of h(x, vy_2; bar x, bar y) (as defined in the context)",
        "\\vy_1": "Arbitrary element of \\gY",
        "\\vy_2": "Arbitrary element of \\gY",
        "\\bar\\vx": "NOT MENTIONED",
        "\\bar \\vy": "NOT MENTIONED",
        "\\gamma": "NOT MENTIONED",
        "\\ell": "NOT MENTIONED",
        "D": "NOT MENTIONED",
        "\\gY": "NOT MENTIONED"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:40:57.432306"
    },
    {
      "label": "<<FORMULA_0527>>",
      "formula": "\\label{eq:prox-xy-1}\n\\begin{split}\n    &\\quad \\left \\Vert \\nabla \\Psi(\\vy^{\\rm out}; \\bar \\vx) + \\gamma \\Vert \\vy^{\\rm out} - \\bar \\vy \\Vert ( \\vy^{\\rm out} - \\bar \\vy) + \\vv^{\\rm out} \\right \\Vert = \\Vert \\nabla_y h(\\vx^*(\\vy^{\\rm out}; \\bar \\vx, \\bar \\vy),\\vy^{\\rm out}; \\bar \\vx, \\bar \\vy) + \\vv^{\\rm out} \\Vert \\\\\n    &\\le  \\Vert \\nabla_y h(\\vx^*(\\bar \\vx, \\bar \\vy), \\vy^{\\rm out};\\bar \\vx, \\bar \\vy) + \\vv^{\\rm out} \\Vert +  \\frac{(\\ell +2 \\gamma D)^{3/2}}{(\\gamma/2)^{1/2}} \\Vert \\vy^{\\rm out} - \\vy^*(\\bar \\vx, \\bar \\vy) \\Vert \\\\\n    &\\le \\Vert \\nabla_y h(\\vx^{\\rm out}, \\vy^{\\rm out}; \\bar \\vx, \\bar \\vy) +\\vv^{\\rm out} \\Vert + (\\ell +2 \\gamma D) \\Vert \\vx^{\\rm out} - \\vx^*(\\bar \\vx, \\bar \\vy) \\Vert \\\\\n    &\\quad + \\frac{(\\ell +2 \\gamma D)^{3/2}}{(\\gamma/2)^{1/2}} \\sqrt{\\Vert \\vy^{\\rm out} - \\vy^*(\\bar \\vx, \\bar \\vy) \\Vert} \\\\\n    &\\le 6 (\\ell + 2 \\gamma D) \\zeta_3\n    + \\frac{\\ell + 2\\gamma D}{(\\gamma/2)^{1/2}}   \\left( 6 (\\ell+2 \\gamma D) \\zeta_3 \\right)^{1/2}  +\\frac{(\\ell +2 \\gamma D)^{3/2}}{(\\gamma/2)^{3/4}} \\left( 6 (\\ell+2 \\gamma D) \\zeta_3 \\right)^{1/4},\n\\end{split}",
      "raw_latex": "\\begin{align} \\label{eq:prox-xy-1}\n\\begin{split}\n    &\\quad \\left \\Vert \\nabla \\Psi(\\vy^{\\rm out}; \\bar \\vx) + \\gamma \\Vert \\vy^{\\rm out} - \\bar \\vy \\Vert ( \\vy^{\\rm out} - \\bar \\vy) + \\vv^{\\rm out} \\right \\Vert = \\Vert \\nabla_y h(\\vx^*(\\vy^{\\rm out}; \\bar \\vx, \\bar \\vy),\\vy^{\\rm out}; \\bar \\vx, \\bar \\vy) + \\vv^{\\rm out} \\Vert \\\\\n    &\\le  \\Vert \\nabla_y h(\\vx^*(\\bar \\vx, \\bar \\vy), \\vy^{\\rm out};\\bar \\vx, \\bar \\vy) + \\vv^{\\rm out} \\Vert +  \\frac{(\\ell +2 \\gamma D)^{3/2}}{(\\gamma/2)^{1/2}} \\Vert \\vy^{\\rm out} - \\vy^*(\\bar \\vx, \\bar \\vy) \\Vert \\\\\n    &\\le \\Vert \\nabla_y h(\\vx^{\\rm out}, \\vy^{\\rm out}; \\bar \\vx, \\bar \\vy) +\\vv^{\\rm out} \\Vert + (\\ell +2 \\gamma D) \\Vert \\vx^{\\rm out} - \\vx^*(\\bar \\vx, \\bar \\vy) \\Vert \\\\\n    &\\quad + \\frac{(\\ell +2 \\gamma D)^{3/2}}{(\\gamma/2)^{1/2}} \\sqrt{\\Vert \\vy^{\\rm out} - \\vy^*(\\bar \\vx, \\bar \\vy) \\Vert} \\\\\n    &\\le 6 (\\ell + 2 \\gamma D) \\zeta_3\n    + \\frac{\\ell + 2\\gamma D}{(\\gamma/2)^{1/2}}   \\left( 6 (\\ell+2 \\gamma D) \\zeta_3 \\right)^{1/2}  +\\frac{(\\ell +2 \\gamma D)^{3/2}}{(\\gamma/2)^{3/4}} \\left( 6 (\\ell+2 \\gamma D) \\zeta_3 \\right)^{1/4},\n\\end{split}\n\\end{align}",
      "formula_type": "align",
      "line_number": 2433,
      "is_formula": true,
      "high_level_explanation": "This chain of (in)equalities upper-bounds the proximal-gradient residual of the y-side objective Ψ (with a norm-weighted displacement term scaled by γ) by rewriting it as the y-gradient of the regularized saddle function h evaluated at its x-minimizer and then progressively relaxing it using continuity/Lipschitz properties. The bounds successively replace unknown optimal points with available algorithmic outputs and convert distances to a target accuracy level. The final line yields an explicit upper bound in terms of ℓ, γ, D, and ζ₃, certifying that the inner procedure provides an inexact proximal oracle for Ψ at the prescribed tolerance.",
      "notations": {
        "\\Psi": "NOT MENTIONED",
        "\\vy^{\\rm out": "NOT MENTIONED",
        "\\vy^{\\rm out}": "NOT MENTIONED",
        "\\bar \\vx": "NOT MENTIONED",
        "\\bar \\vy": "NOT MENTIONED",
        "\\vv^{\\rm out}": "NOT MENTIONED",
        "h": "NOT MENTIONED",
        "\\vx^*(\\vy^{\\rm out}; \\bar \\vx, \\bar \\vy)": "Minimizer over x of h(x, \\vy^{\\rm out}; \\bar \\vx, \\bar \\vy) (i.e., \\arg\\min_{\\vx} h(\\vx, \\vy^{\\rm out}; \\bar \\vx, \\bar \\vy))",
        "\\vx^*(\\bar \\vx, \\bar \\vy)": "x-component of a saddle-point pair for h(\\vx,\\vy; \\bar \\vx, \\bar \\vy), i.e., the minimizer in the pair (\\arg\\min_{\\vx} \\max_{\\vy} h(\\vx,\\vy; \\bar \\vx, \\bar \\vy))",
        "\\vy^*(\\bar \\vx, \\bar \\vy)": "y-component of a saddle-point pair for h(\\vx,\\vy; \\bar \\vx, \\bar \\vy), i.e., the maximizer in the pair (\\arg\\min_{\\vx} \\max_{\\vy} h(\\vx,\\vy; \\bar \\vx, \\bar \\vy))",
        "\\vx^{\\rm out}": "NOT MENTIONED",
        "\\ell": "NOT MENTIONED",
        "D": "NOT MENTIONED",
        "\\gamma": "NOT MENTIONED",
        "\\zeta_3": "NOT MENTIONED"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:41:43.815151"
    },
    {
      "label": "<<FORMULA_0574>>",
      "formula": "\\langle \\mF(\\bar \\vz) + \\nabla \\mF(\\vz_{\\rm ss})(\\vz - \\bar \\vz) + \\frac{\\gamma}{2} \\Vert \\vz - \\bar \\vz \\Vert (\\vz - \\bar \\vz), \\vz' - \\vz  \\rangle \\ge 0, ~\\forall \\vz' \\in \\gZ; \\\\\n    \\vu =  -\\left(\\mF(\\bar\\vz) + \\nabla \\mF(\\vz_{\\rm ss}) (\\vz - \\bar \\vz) + \\frac{\\gamma}{2} \\Vert \\vz - \\bar \\vz \\Vert \n    (\\vz- \\bar \\vz)  \\right) \\in \n    \\begin{bmatrix}\n        \\partial \\gI_{\\gX} (\\vx) \\\\\n        -\\partial \\gI_{\\gY}(\\vy)\n    \\end{bmatrix}.",
      "raw_latex": "\\begin{align*}\n    \\langle \\mF(\\bar \\vz) + \\nabla \\mF(\\vz_{\\rm ss})(\\vz - \\bar \\vz) + \\frac{\\gamma}{2} \\Vert \\vz - \\bar \\vz \\Vert (\\vz - \\bar \\vz), \\vz' - \\vz  \\rangle \\ge 0, ~\\forall \\vz' \\in \\gZ; \\\\\n    \\vu =  -\\left(\\mF(\\bar\\vz) + \\nabla \\mF(\\vz_{\\rm ss}) (\\vz - \\bar \\vz) + \\frac{\\gamma}{2} \\Vert \\vz - \\bar \\vz \\Vert \n    (\\vz- \\bar \\vz)  \\right) \\in \n    \\begin{bmatrix}\n        \\partial \\gI_{\\gX} (\\vx) \\\\\n        -\\partial \\gI_{\\gY}(\\vy)\n    \\end{bmatrix}.\n\\end{align*}",
      "formula_type": "align*",
      "line_number": 2644,
      "is_formula": true,
      "high_level_explanation": "This pair of relations gives the optimality/KKT conditions for a constrained variational inequality defined by a cubic-regularized, linearized model of the operator. The operator F is linearized at the snapshot point z_ss, and a cubic term with parameter γ regularizes the model around the reference point bar z. The first line is a variational inequality stating that the residual has a nonnegative inner product with any feasible direction. The second line is the equivalent normal-cone inclusion: the negative residual equals an element of the product normal cone to the constraint sets.",
      "notations": {
        "\\mF": "Saddle-point operator mapping z = (x, y) to [∇_x f(x, y); −∇_y f(x, y)] (given in context)",
        "\\bar \\vz": "NOT MENTIONED",
        "\\vz_{\\rm ss}": "Snapshot point (given in context)",
        "\\gamma": "Regularization parameter for the cubic term (given in context)",
        "\\nabla \\mF(\\vz_{\\rm ss})": "Jacobian of \\mF evaluated at the snapshot point \\vz_{\\rm ss}",
        "\\vz": "Stacked variable (\\vx, \\vy)",
        "\\vz'": "Arbitrary test point in the feasible set",
        "\\gZ": "Feasible set for \\vz",
        "\\gX": "Constraint set for \\vx (given in context)",
        "\\gY": "Constraint set for \\vy (given in context)",
        "\\partial \\gI_{\\gX} (\\vx)": "Subdifferential of the indicator of \\gX at \\vx (normal cone to \\gX at \\vx)",
        "-\\partial \\gI_{\\gY}(\\vy)": "Negative subdifferential of the indicator of \\gY at \\vy (negative normal cone to \\gY at \\vy)",
        "\\vu": "Certificate/residual vector belonging to the product normal cone",
        "\\vx": "x-variable (minimization side)",
        "\\vy": "y-variable (maximization side)"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:42:31.522384"
    },
    {
      "label": "<<FORMULA_0589>>",
      "formula": "\\label{eq:eg-1}\n\\begin{split}\n     &\\quad \\eta_t \\langle \\mF(\\vz_{t+1/2}), \\vz_{t+1/2} - \\vz \\rangle \\\\\n    &= \\eta_t \\langle \\mF(\\vz_{t+1/2}), \\vz_{t+1} - \\vz \\rangle + \\eta_t \\langle \\mF(\\vz_{t+1/2}), \\vz_{t+1/2} - \\vz_{t+1}   \\rangle \\\\ \n    &= \\eta_t \\langle \\mF(\\vz_{t+1/2}), \\vz_{t+1} - \\vz \\rangle + \\eta_t \\langle \\tilde \\mF(\\vz_{t+1/2}), \\vz_{t+1/2} - \\vz_{t+1}   \\rangle \\\\\n    &\\quad + \\eta_t \\langle \\mF(\\vz_{t+1/2}) - \\tilde \\mF(\\vz_{t+1/2}), \\vz_{t+1/2} - \\vz_{t+1} \\rangle  \\\\\n    &\\\\ge \\langle \\vz_{t+1} - \\vz_t, \\vz - \\vz_{t+1} \\rangle + \\langle  \\vz_{t+1/2} - \\vz_t, \\vz_{t+1} - \\vz_{t+1/2}  \\rangle \\\\\n    &\\quad + \\eta_t \\langle \\mF(\\vz_{t+1/2}) - \\tilde \\mF(\\vz_{t+1/2}), \\vz_{t+1/2} - \\vz_{t+1} \\rangle.\n\\end{split}",
      "raw_latex": "\\begin{align} \\label{eq:eg-1}\n\\begin{split}\n     &\\quad \\eta_t \\langle \\mF(\\vz_{t+1/2}), \\vz_{t+1/2} - \\vz \\rangle \\\\\n    &= \\eta_t \\langle \\mF(\\vz_{t+1/2}), \\vz_{t+1} - \\vz \\rangle + \\eta_t \\langle \\mF(\\vz_{t+1/2}), \\vz_{t+1/2} - \\vz_{t+1}   \\rangle \\\\ \n    &= \\eta_t \\langle \\mF(\\vz_{t+1/2}), \\vz_{t+1} - \\vz \\rangle + \\eta_t \\langle \\tilde \\mF(\\vz_{t+1/2}), \\vz_{t+1/2} - \\vz_{t+1}   \\rangle \\\\\n    &\\quad + \\eta_t \\langle \\mF(\\vz_{t+1/2}) - \\tilde \\mF(\\vz_{t+1/2}), \\vz_{t+1/2} - \\vz_{t+1} \\rangle  \\\\\n    &\\\\ge \\langle \\vz_{t+1} - \\vz_t, \\vz - \\vz_{t+1} \\rangle + \\langle  \\vz_{t+1/2} - \\vz_t, \\vz_{t+1} - \\vz_{t+1/2}  \\rangle \\\\\n    &\\quad + \\eta_t \\langle \\mF(\\vz_{t+1/2}) - \\tilde \\mF(\\vz_{t+1/2}), \\vz_{t+1/2} - \\vz_{t+1} \\rangle.\n\\end{split}\n\\end{align}",
      "formula_type": "align",
      "line_number": 2687,
      "is_formula": true,
      "high_level_explanation": "The expression decomposes the inner product term involving the operator evaluated at the half-step iterate into a sum of parts, then adds and subtracts an approximate operator to isolate an approximation error. Using first-order optimality conditions from the extragradient and lazy Newton updates, it lower-bounds this quantity by telescoping inner products of iterate differences plus an error term capturing the mismatch between the true operator and its approximation. This step is standard in analyzing extragradient-type methods with inexact or reused second-order information, preparing the expression for subsequent norm-based bounds.",
      "notations": {
        "\\eta_t": "NOT MENTIONED",
        "\\mF": "NOT MENTIONED",
        "\\tilde \\mF": "NOT MENTIONED",
        "\\vz_{t+1/2}": "NOT MENTIONED",
        "\\vz_{t+1}": "NOT MENTIONED",
        "\\vz_t": "NOT MENTIONED",
        "\\vz": "NOT MENTIONED"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:40:19.850558"
    },
    {
      "label": "<<FORMULA_0591>>",
      "formula": "&\\quad \\eta_t \\langle \\mF(\\vz_{t+1/2}), \\vz_{t+1/2} - \\vz \\rangle \\\\\n    &\\le \\frac{1}{2} \\Vert \\vz - \\vz_t \\Vert^2 \\bcancel{- \\frac{1}{2} \\Vert \\vz_{t+1} - \\vz_t \\Vert^2} - \\frac{1}{2} \\Vert \\vz - \\vz_{t+1} \\Vert^2 \\\\\n    &\\quad + \\bcancel{\\frac{1}{2} \\Vert \\vz_{t+1} - \\vz_t \\Vert^2} - \\frac{1}{2} \\Vert \\vz_{t+1/2} - \\vz_t \\Vert^2 - \\frac{1}{2} \\Vert \\vz_{t+1} - \\vz_{t+1/2} \\Vert^2 \\\\\n    &\\quad + \\underbrace{\\eta_t^2 \\Vert \\mF(\\vz_{t+1/2}) - \\tilde \\mF(\\vz_{t+1/2}) \\Vert^2}_{(*)} + \\frac{4}{1} \\Vert \\vz_{t+1} - \\vz_{t+1/2} \\Vert^2.\n    %  - \\frac{\\eta_t}{2} \\Vert \\Vert \n    % + \\frac{\\eta_t}{2} \\Vert \\vz - \\vz_{t+1} \\Vert^2 + \n    % \\frac{\\eta_t}{2} \\Vert \\vz - \\vz_t \\Vert^2 \\bcancel{-\\frac{\\eta_t}{2} \\Vert \\vz_{t+1} - \\vz_t \\Vert^2}  \\\\\n    % &\\quad + \\frac{\\eta_t}{2} \\Vert \\vz_{t+1/2} - \\vz_t \\Vert^2 + \\frac{\\eta_t}{2} \\Vert \\vz_{t+1/2} - \\vz_{t+1} \\Vert^2 \\bcancel {- \\frac{\\eta_t}{2} \\Vert \\vz_{t} - \\vz_{t+1} \\Vert^2} \\\\",
      "raw_latex": "\\begin{align*}\n    &\\quad \\eta_t \\langle \\mF(\\vz_{t+1/2}), \\vz_{t+1/2} - \\vz \\rangle \\\\\n    &\\le \\frac{1}{2} \\Vert \\vz - \\vz_t \\Vert^2 \\bcancel{- \\frac{1}{2} \\Vert \\vz_{t+1} - \\vz_t \\Vert^2} - \\frac{1}{2} \\Vert \\vz - \\vz_{t+1} \\Vert^2 \\\\\n    &\\quad + \\bcancel{\\frac{1}{2} \\Vert \\vz_{t+1} - \\vz_t \\Vert^2} - \\frac{1}{2} \\Vert \\vz_{t+1/2} - \\vz_t \\Vert^2 - \\frac{1}{2} \\Vert \\vz_{t+1} - \\vz_{t+1/2} \\Vert^2 \\\\\n    &\\quad + \\underbrace{\\eta_t^2 \\Vert \\mF(\\vz_{t+1/2}) - \\tilde \\mF(\\vz_{t+1/2}) \\Vert^2}_{(*)} + \\frac{4}{1} \\Vert \\vz_{t+1} - \\vz_{t+1/2} \\Vert^2.\n    %  - \\frac{\\eta_t}{2} \\Vert \\Vert \n    % + \\frac{\\eta_t}{2} \\Vert \\vz - \\vz_{t+1} \\Vert^2 + \n    % \\frac{\\eta_t}{2} \\Vert \\vz - \\vz_t \\Vert^2 \\bcancel{-\\frac{\\eta_t}{2} \\Vert \\vz_{t+1} - \\vz_t \\Vert^2}  \\\\\n    % &\\quad + \\frac{\\eta_t}{2} \\Vert \\vz_{t+1/2} - \\vz_t \\Vert^2 + \\frac{\\eta_t}{2} \\Vert \\vz_{t+1/2} - \\vz_{t+1} \\Vert^2 \\bcancel {- \\frac{\\eta_t}{2} \\Vert \\vz_{t} - \\vz_{t+1} \\Vert^2} \\\\\n\\end{align*}",
      "formula_type": "align*",
      "line_number": 2698,
      "is_formula": true,
      "high_level_explanation": "This inequality is a progress bound derived from the optimality conditions of an extragradient step followed by a lazy Newton step. It upper-bounds the term η_t⟨F(z_{t+1/2}), z_{t+1/2} − z⟩ by a telescoping difference of squared distances between iterates, minus step-length penalties, plus an error term (*) from replacing F with its approximation and a residual proportional to ||z_{t+1} − z_{t+1/2}||^2. The struck-out terms indicate exact cancellations, and the underbraced term (*) is later controlled via Young’s inequality and smoothness (Hessian-Lipschitz) assumptions.",
      "notations": {
        "\\eta_t": "NOT MENTIONED",
        "\\mF": "NOT MENTIONED",
        "\\tilde \\mF": "NOT MENTIONED",
        "\\vz_{t+1/2}": "Intermediate point produced by the extragradient step at iteration t",
        "\\vz_{t+1}": "Updated iterate after the lazy Newton step at iteration t",
        "\\vz_t": "Current iterate at iteration t",
        "\\vz": "NOT MENTIONED"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:41:06.628263"
    },
    {
      "label": "<<FORMULA_0592>>",
      "formula": "&\\quad \\eta_t^2 \\Vert \\mF(\\vz_{t+1/2}) - \\tilde \\mF(\\vz_{t+1/2}) \\Vert^2 \\\\\n    &\\le 2 \\eta_t^2 \\Vert \\mF(\\vz_{t+1/2}) - \\mF(\\vz_t) - \\nabla \\mF(\\vz_t) (\\vz_{t+1/2} - \\vz_t) \\Vert^2 \\\\\n    &\\quad + 2 \\eta_t^2 \\Vert (\\nabla \\mF(\\vz_t) - \\nabla \\mF(\\vz_{\\pi(t)}) (\\vz_{t+1/2} - \\vz_t) \\Vert^2 \\\\\n    &\\le \\frac{\\eta_t^2 \\rho^2}{2} \\Vert \\vz_{t+1/2} - \\vz_t \\Vert^4  + 2 \\eta_t^2 \\rho^2 \\Vert \\vz_t - \\vz_{\\pi(t)} \\Vert^2 \\Vert \\vz_{t+1/2} - \\vz_t \\Vert^2  \\\\\n    &= \\frac{\\rho^2}{8 \\gamma^2} \\Vert \\vz_{t+1/2} - \\vz_t \\Vert^2 + \\frac{\\rho^2}{2\\gamma^2} \\Vert \\vz_t - \\vz_{\\pi(t)} \\Vert^2.",
      "raw_latex": "\\begin{align*}\n    &\\quad \\eta_t^2 \\Vert \\mF(\\vz_{t+1/2}) - \\tilde \\mF(\\vz_{t+1/2}) \\Vert^2 \\\\\n    &\\le 2 \\eta_t^2 \\Vert \\mF(\\vz_{t+1/2}) - \\mF(\\vz_t) - \\nabla \\mF(\\vz_t) (\\vz_{t+1/2} - \\vz_t) \\Vert^2 \\\\\n    &\\quad + 2 \\eta_t^2 \\Vert (\\nabla \\mF(\\vz_t) - \\nabla \\mF(\\vz_{\\pi(t)}) (\\vz_{t+1/2} - \\vz_t) \\Vert^2 \\\\\n    &\\le \\frac{\\eta_t^2 \\rho^2}{2} \\Vert \\vz_{t+1/2} - \\vz_t \\Vert^4  + 2 \\eta_t^2 \\rho^2 \\Vert \\vz_t - \\vz_{\\pi(t)} \\Vert^2 \\Vert \\vz_{t+1/2} - \\vz_t \\Vert^2  \\\\\n    &= \\frac{\\rho^2}{8 \\gamma^2} \\Vert \\vz_{t+1/2} - \\vz_t \\Vert^2 + \\frac{\\rho^2}{2\\gamma^2} \\Vert \\vz_t - \\vz_{\\pi(t)} \\Vert^2.\n\\end{align*}",
      "formula_type": "align*",
      "line_number": 2709,
      "is_formula": true,
      "high_level_explanation": "This chain of inequalities bounds the squared discrepancy between the true operator value at the extragradient half-step, F(z_{t+1/2}), and its lazy-Newton surrogate, by splitting the error into a local Taylor remainder around z_t and an error due to using a stale Jacobian at z_{\\pi(t)}. Using the Hessian Lipschitzness (with constant ρ) and Young's inequality, the bound is converted to terms depending on the step sizes between consecutive/stale iterates. The final line simplifies these bounds to be proportional to ||z_{t+1/2} − z_t||^2 and ||z_t − z_{\\pi(t)}||^2 with coefficients involving ρ and γ.",
      "notations": {
        "\\eta_t": "NOT MENTIONED",
        "\\mF": "NOT MENTIONED",
        "\\tilde \\mF": "Approximation of the operator used in the lazy Newton step",
        "\\vz_{t+1/2}": "Intermediate (half-step) iterate produced by the extragradient step",
        "\\vz_t": "Current iterate at iteration t",
        "\\vz_{\\pi(t)}": "Iterate indexed by \\pi(t)",
        "\\pi(t)": "NOT MENTIONED",
        "\\rho": "Lipschitz constant of the Hessian (from Assumption \\ref{asm:Hess-lip})",
        "\\gamma": "NOT MENTIONED"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:40:56.639857"
    }
  ],
  "metadata": {
    "model": "gpt-5",
    "context_words": 300,
    "max_formulas": 20,
    "timestamp": "2025-10-31T16:42:39.868301",
    "total_formulas_in_paper": 20,
    "formulas_selected_for_analysis": 20,
    "skipped_by_length_limit": 0,
    "formulas_explained": 20,
    "notations_skipped": 0,
    "failed": 0
  },
  "skipped_notations": [],
  "failed": []
}