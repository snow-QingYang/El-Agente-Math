{
  "formulas": [
    {
      "label": "<<FORMULA_0012>>",
      "formula": "\\label{eq:exact_posterior_mean_and_kernel}\n\\mu_{f \\given \\v{y}}(\\cdot)\n&=\n\\mu(\\cdot) +\nk(\\cdot, \\v{x})(k(\\v{x}, \\v{x}) + \\m{\\Sigma})^{-1}\\del{\\v{y} - \\mu(\\v{x})},\n\\\\\nk_{f \\given \\v{y}}(\\cdot, \\cdot')\n&=\nk(\\cdot, \\cdot') - k(\\cdot, \\v{x})(k(\\v{x}, \\v{x}) + \\m{\\Sigma})^{-1}k(\\v{x}, \\cdot').",
      "raw_latex": "\\[\n\\label{eq:exact_posterior_mean_and_kernel}\n\\mu_{f \\given \\v{y}}(\\cdot)\n&=\n\\mu(\\cdot) +\nk(\\cdot, \\v{x})(k(\\v{x}, \\v{x}) + \\m{\\Sigma})^{-1}\\del{\\v{y} - \\mu(\\v{x})},\n\\\\\nk_{f \\given \\v{y}}(\\cdot, \\cdot')\n&=\nk(\\cdot, \\cdot') - k(\\cdot, \\v{x})(k(\\v{x}, \\v{x}) + \\m{\\Sigma})^{-1}k(\\v{x}, \\cdot').\n\\]",
      "formula_type": "display",
      "line_number": 150,
      "is_formula": true,
      "high_level_explanation": "These are the exact posterior mean and covariance of a Gaussian process after observing data under a Gaussian likelihood. The posterior mean equals the prior mean plus a data-dependent correction that projects the residuals through the kernel and the (kernel + noise) precision. The posterior covariance equals the prior kernel minus the part explained by the observations, yielding reduced uncertainty near observed inputs. Together, they define the updated GP that can be used for prediction at any test inputs.",
      "notations": {
        "\\mu_{f \\given \\v{y}}(\\cdot)": "Posterior mean of the function f given observations \\v{y} (as a function of a test input)",
        "k_{f \\given \\v{y}}(\\cdot, \\cdot')": "Posterior kernel (covariance) of f given \\v{y} between two test inputs",
        "\\mu(\\cdot)": "Prior mean function of the GP",
        "k(\\cdot, \\cdot')": "Prior kernel (covariance) function between two inputs",
        "\\v{x}": "Input locations where observations are given",
        "\\v{y}": "Observed outputs corresponding to \\v{x}",
        "\\m{\\Sigma}": "Observation noise covariance matrix in the Gaussian likelihood",
        "k(\\v{x}, \\v{x})": "Kernel Gram matrix evaluated on the training inputs \\v{x}",
        "k(\\cdot, \\v{x})": "Vector/matrix of covariances between a test input and each training input in \\v{x}",
        "k(\\v{x}, \\cdot')": "Vector/matrix of covariances between each training input in \\v{x} and a second test input",
        "(k(\\v{x}, \\v{x}) + \\m{\\Sigma})^{-1}": "Inverse of the training covariance plus observation noise (the training-data precision matrix)",
        "\\cdot": "Generic test input argument",
        "\\cdot'": "Second generic test input argument"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:08:32.100367"
    },
    {
      "label": "<<FORMULA_0015>>",
      "formula": "\\label{eq:matern_kernel_eigenvalues_sum}\n\\!\\!\\!\nk_{\\nu, \\kappa, \\sigma^2}(x, x') = \\frac{\\sigma^2}{C_{\\nu, \\kappa}} \\sum_{j=0}^{\\infty} \\Phi_{\\nu,\\kappa}(\\lambda_j) \\phi_j(x)\\phi_j(x')\n,\n&&\n\\Phi_{\\nu, \\kappa}(\\lambda) = \\begin{cases}\n    \\left((\\frac{2\\nu}{\\kappa^2} + \\lambda\\right)^{-\\nu-\\frac{d}){2}} & \\nu < \\infty\\\\\ne^{-\\frac{\\kappa^2}{2}\\lambda} & \\nu = \\infty\n\\end{cases}",
      "raw_latex": "\\[\n\\label{eq:matern_kernel_eigenvalues_sum}\n\\!\\!\\!\nk_{\\nu, \\kappa, \\sigma^2}(x, x') = \\frac{\\sigma^2}{C_{\\nu, \\kappa}} \\sum_{j=0}^{\\infty} \\Phi_{\\nu,\\kappa}(\\lambda_j) \\phi_j(x)\\phi_j(x')\n,\n&&\n\\Phi_{\\nu, \\kappa}(\\lambda) = \\begin{cases}\n    \\left((\\frac{2\\nu}{\\kappa^2} + \\lambda\\right)^{-\\nu-\\frac{d}){2}} & \\nu < \\infty\\\\\ne^{-\\frac{\\kappa^2}{2}\\lambda} & \\nu = \\infty\n\\end{cases}\n\\]",
      "formula_type": "display",
      "line_number": 172,
      "is_formula": true,
      "high_level_explanation": "This expression gives the eigenfunction expansion of a Matérn kernel on a Riemannian manifold, written as an infinite sum over Laplace–Beltrami eigenpairs. The spectral weighting function Φ determines how much each eigenmode contributes, with a rational-power form for finite ν and an exponential (heat kernel) form when ν is infinite, corresponding to the squared exponential limit. The constant C_{ν,κ} normalizes the kernel, and σ^2 sets an overall scale. In practice the sum is truncated since the coefficients decay rapidly with the eigenvalues.",
      "notations": {
        "k_{\\nu, \\kappa, \\sigma^2}(x, x')": "Covariance kernel (Matérn family) between inputs x and x' on the manifold",
        "x": "Input location on the domain",
        "x'": "Another input location on the domain",
        "\\sigma^2": "NOT MENTIONED",
        "C_{\\nu, \\kappa}": "Normalisation constant",
        "\\Phi_{\\nu,\\kappa}": "Spectral weighting function (eigenvalue response) defining the Matérn kernel in the Laplace–Beltrami eigenbasis",
        "\\lambda_j": "j-th eigenvalue of the Laplace–Beltrami operator on the domain",
        "\\phi_j(x)": "j-th eigenfunction of the Laplace–Beltrami operator evaluated at x",
        "\\phi_j(x')": "j-th eigenfunction of the Laplace–Beltrami operator evaluated at x'",
        "d": "Dimension of the domain",
        "\\nu": "NOT MENTIONED",
        "\\kappa": "NOT MENTIONED",
        "\\lambda": "Eigenvalue variable used in the definition of \\Phi"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:08:59.523588"
    },
    {
      "label": "<<FORMULA_0034>>",
      "formula": "\\label{eq:variational_posterior_mean_and_kernel}\n\\mu_{\\v{z}, \\v{m}, \\v{S}}(\\cdot)\n&=\n\\mu(\\cdot)\n+\nk(\\cdot, \\v{z})\nk(\\v{z}, \\v{z})^{-1}\n(\\v{m}-\\mu(\\v{z}))\n,\n\\\\\nk_{\\v{z}, \\v{m}, \\v{S}}(\\cdot, \\cdot')\n&=\nk(\\cdot, \\cdot')\n-\nk(\\cdot, \\v{z})\nk(\\v{z}, \\v{z})^{-1}\n(k(\\v{z}, \\v{z}) - \\m{S})\nk(\\v{z}, \\v{z})^{-1}\nk(\\v{z}, \\cdot')\n.",
      "raw_latex": "\\[\n\\label{eq:variational_posterior_mean_and_kernel}\n\\mu_{\\v{z}, \\v{m}, \\v{S}}(\\cdot)\n&=\n\\mu(\\cdot)\n+\nk(\\cdot, \\v{z})\nk(\\v{z}, \\v{z})^{-1}\n(\\v{m}-\\mu(\\v{z}))\n,\n\\\\\nk_{\\v{z}, \\v{m}, \\v{S}}(\\cdot, \\cdot')\n&=\nk(\\cdot, \\cdot')\n-\nk(\\cdot, \\v{z})\nk(\\v{z}, \\v{z})^{-1}\n(k(\\v{z}, \\v{z}) - \\m{S})\nk(\\v{z}, \\v{z})^{-1}\nk(\\v{z}, \\cdot')\n.\n\\]",
      "formula_type": "display",
      "line_number": 218,
      "is_formula": true,
      "high_level_explanation": "These equations define the mean and covariance (kernel) of a variational sparse Gaussian process parameterised by inducing locations, a variational mean, and a variational covariance. Starting from a prior GP with mean μ and kernel k, the posterior mean adjusts the prior by the cross-covariance with the inducing points and the offset between the variational mean and the prior mean at those points. The covariance is the prior covariance reduced by a term that depends on the kernel Gram matrix at the inducing points and the variational covariance S.",
      "notations": {
        "\\mu_{\\v{z}, \\v{m}, \\v{S}}(\\cdot)": "Mean function of the variational sparse GP parameterised by inducing locations \\v{z}, variational mean \\v{m}, and covariance \\m{S}",
        "k_{\\v{z}, \\v{m}, \\v{S}}(\\cdot, \\cdot')": "Covariance (kernel) function of the variational sparse GP parameterised by \\v{z}, \\v{m}, and \\m{S}",
        "\\mu(\\cdot)": "Prior GP mean function",
        "k(\\cdot, \\cdot')": "Prior GP covariance kernel",
        "\\v{z}": "Inducing locations (pseudo-inputs)",
        "\\v{m}": "Mean vector of the inducing-variable distribution",
        "\\m{S}": "Covariance matrix of the inducing-variable distribution",
        "k(\\v{z}, \\v{z})": "Kernel Gram matrix evaluated at the inducing locations",
        "k(\\cdot, \\v{z})": "Cross-covariance between a test input and the inducing locations",
        "k(\\v{z}, \\cdot')": "Cross-covariance between the inducing locations and a test input",
        "\\mu(\\v{z})": "Prior mean evaluated at the inducing locations"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:08:30.333699"
    },
    {
      "label": "<<FORMULA_0037>>",
      "formula": "\\label{eqn:eucl_deep_gp_vf}\nF_{\\v{\\theta}}\n=\nf_{\\v{z}^L, \\v{m}^L, \\m{S}^L}\n\\circ\n\\ldots\n\\circ\nf_{\\v{z}^1, \\v{m}^1, \\m{S}^1}\n,\n&&\n\\v{\\theta}\n=\n\\cbr{\\v{z}^l, \\v{m}^l, \\m{S}^l}_{l=1}^L\n.",
      "raw_latex": "\\[ \\label{eqn:eucl_deep_gp_vf}\nF_{\\v{\\theta}}\n=\nf_{\\v{z}^L, \\v{m}^L, \\m{S}^L}\n\\circ\n\\ldots\n\\circ\nf_{\\v{z}^1, \\v{m}^1, \\m{S}^1}\n,\n&&\n\\v{\\theta}\n=\n\\cbr{\\v{z}^l, \\v{m}^l, \\m{S}^l}_{l=1}^L\n.\n\\]",
      "formula_type": "display",
      "line_number": 243,
      "is_formula": true,
      "high_level_explanation": "This equation defines the variational-family deep Gaussian process F_theta as a composition of L sparse GP layers. Each layer f is parameterized by inducing locations, an inducing mean, and an inducing covariance. The vector theta collects all these per-layer variational parameters across the L layers, specifying the particular member of the variational family.",
      "notations": {
        "F_{\\v{\\theta}}": "Deep Gaussian process in the variational family obtained by composing L sparse GP layers parameterized by \\v{\\theta}.",
        "f_{\\v{z}^L, \\v{m}^L, \\m{S}^L}": "Sparse GP defining the L-th layer with inducing locations \\v{z}^L, inducing mean vector \\v{m}^L, and inducing covariance matrix \\m{S}^L.",
        "f_{\\v{z}^1, \\v{m}^1, \\m{S}^1}": "Sparse GP defining the first layer with inducing locations \\v{z}^1, inducing mean vector \\v{m}^1, and inducing covariance matrix \\m{S}^1.",
        "\\v{\\theta}": "Collection of variational parameters across all layers: \\cbr{\\v{z}^l, \\v{m}^l, \\m{S}^l}_{l=1}^L.",
        "\\v{z}^l": "Inducing locations (pseudo-inputs) for layer l.",
        "\\v{m}^l": "Mean vector of the inducing variable distribution for layer l.",
        "\\m{S}^l": "Covariance matrix of the inducing variable distribution for layer l.",
        "L": "Number of layers in the deep Gaussian process.",
        "\\cbr{\\v{z}^l, \\v{m}^l, \\m{S}^l}_{l=1}^L": "Collection of per-layer sparse GP parameters for layers 1 through L."
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:09:28.826192"
    },
    {
      "label": "<<FORMULA_0040>>",
      "formula": "\\label{eq:euclidean_deep_gp_elbo}\n\\mathrm{ELBO}\n=\n{\\sum}_{i=1}^n\n{\\E}_{F(x_i) \\sim p(F_{\\v{\\theta}}(x_i))}\n\\\\ln p(y_i| F(x_i))\n-\n{\\sum}_{l=1}^L\n\\KL(q(\\v{u}^l) \\,\\Vert\\, p(\\v{u}^l)),",
      "raw_latex": "\\begin{equation}\\label{eq:euclidean_deep_gp_elbo}\n\\mathrm{ELBO}\n=\n{\\sum}_{i=1}^n\n{\\E}_{F(x_i) \\sim p(F_{\\v{\\theta}}(x_i))}\n\\\\ln p(y_i| F(x_i))\n-\n{\\sum}_{l=1}^L\n\\KL(q(\\v{u}^l) \\,\\Vert\\, p(\\v{u}^l)),\n\\end{equation}",
      "formula_type": "equation",
      "line_number": 259,
      "is_formula": true,
      "high_level_explanation": "This expression is the evidence lower bound (ELBO) used to train a deep Gaussian process with a sparse variational approximation. The first term is the expected log-likelihood of each observation under the variational predictive distribution of the function values at the inputs. The second term sums, across layers, the KL divergences between the variational and prior distributions of the inducing variables, acting as a regularizer. Maximizing this ELBO fits the variational parameters by balancing data fit and model complexity.",
      "notations": {
        "\\mathrm{ELBO}": "Evidence lower bound on the log marginal likelihood",
        "n": "Number of training inputs",
        "x_i": "i-th input",
        "y_i": "Observed output corresponding to x_i",
        "F": "Deep Gaussian process (random function) being modeled",
        "F_{\\v{\\theta}}": "Variational deep Gaussian process composed of sparse GP layers parameterized by \\v{\\theta}",
        "\\v{\\theta}": "Collection of variational parameters for all layers (inducing locations, mean vectors, and covariance matrices)",
        "p(F_{\\v{\\theta}}(x_i))": "Predictive distribution of the function value at x_i under the variational model",
        "p(y_i| F(x_i))": "Likelihood of observation y_i given the function value at x_i",
        "L": "Number of layers in the deep Gaussian process",
        "\\KL": "Kullback–Leibler divergence",
        "q(\\v{u}^l)": "Variational distribution over inducing variables at layer l",
        "p(\\v{u}^l)": "Prior distribution over inducing variables at layer l",
        "\\v{u}^l": "Inducing variables (pseudo-observations) at layer l"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:09:58.778548"
    },
    {
      "label": "<<FORMULA_0074>>",
      "formula": "\\v{k}_{\\nu, \\kappa, \\sigma^2}(x, x') = \\frac{\\sigma^2}{C_{\\nu, \\kappa}} \\sum_{j=0}^{\\infty} \\P^Thi_{\\nu,\\kappa}(\\lambda_j) s_j(x) \\otimes s_j(x')\\label{eq:hodge_matern_kernel}.",
      "raw_latex": "\\begin{equation}\n    \\v{k}_{\\nu, \\kappa, \\sigma^2}(x, x') = \\frac{\\sigma^2}{C_{\\nu, \\kappa}} \\sum_{j=0}^{\\infty} \\P^Thi_{\\nu,\\kappa}(\\lambda_j) s_j(x) \\otimes s_j(x')\\label{eq:hodge_matern_kernel}.\n\\end{equation}",
      "formula_type": "equation",
      "line_number": 342,
      "is_formula": true,
      "high_level_explanation": "This equation defines the Hodge Matérn kernel for vector fields on a manifold as an eigen-expansion over the Hodge Laplacian’s eigenfields. Each mode is weighted by a Matérn spectral function evaluated at the corresponding eigenvalue and scaled by a hyperparameter factor. The outer product of eigenfields makes the kernel matrix-valued, providing covariances between vector components at two points. The normalization constant and hyperparameters control the overall scale and shape of the spectrum.",
      "notations": {
        "\\v{k}_{\\nu, \\kappa, \\sigma^2": "Hodge Matérn kernel (vector-valued covariance function) parameterized by \\nu, \\kappa, and \\sigma^2",
        "x": "Point on the manifold X",
        "x'": "Another point on the manifold X",
        "\\sigma^2": "Hyperparameter",
        "C_{\\nu, \\kappa}": "NOT MENTIONED",
        "\\P^Thi_{\\nu,\\kappa": "Spectral weighting function used in the Matérn eigen-expansion (as defined in the referenced equation)",
        "\\lambda_j": "j-th eigenvalue of the Hodge Laplacian on X",
        "s_j": "j-th eigenfield (eigenvector field) of the Hodge Laplacian on X",
        "\\otimes": "Tensor (outer) product producing a matrix-valued kernel",
        "\\nu": "Hyperparameter",
        "\\kappa": "Hyperparameter"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:09:52.303538"
    },
    {
      "label": "<<FORMULA_0080>>",
      "formula": "\\v{m}^l = (\\v{m}^l_1, \\ldots, \\v{m}^l_m)\n,\n&&\n\\v{m}^l_j \\in T_{z^l_j} X\n,\n&&\n\\operatorname{im}(\\m{S}^l) \\subseteq T_{z^l_1} X \\x \\ldots \\x T_{z^l_m} X\n,",
      "raw_latex": "\\[\n\\v{m}^l = (\\v{m}^l_1, \\ldots, \\v{m}^l_m)\n,\n&&\n\\v{m}^l_j \\in T_{z^l_j} X\n,\n&&\n\\operatorname{im}(\\m{S}^l) \\subseteq T_{z^l_1} X \\x \\ldots \\x T_{z^l_m} X\n,\n\\]",
      "formula_type": "display",
      "line_number": 409,
      "is_formula": true,
      "high_level_explanation": "The expression specifies that the parameter vector m^l is a concatenation of m components, with each component m^l_j constrained to lie in the tangent space of the manifold X at the corresponding point z^l_j. It further requires the image of the matrix/operator S^l to be contained within the Cartesian product of these tangent spaces. These constraints ensure that the pseudo-observations parameterized by (m^l, S^l) are valid tangent vectors at the inducing locations on X, consistent with Gaussian vector fields defined on manifolds.",
      "notations": {
        "\\v{m}^l": "Stacked parameter vector at layer l composed of m components",
        "\\v{m}^l_j": "j-th component of \\v{m}^l; a tangent vector at z^l_j",
        "z^l_j": "Inducing location on the manifold X (a point on X)",
        "T_{z^l_j} X": "Tangent space of X at the point z^l_j",
        "\\m{S}^l": "Positive semi-definite matrix parameter at layer l whose image is constrained to lie in the product tangent space",
        "\\operatorname{im}(\\m{S}^l)": "Image of the linear operator \\m{S}^l",
        "X": "Underlying manifold",
        "T_{z^l_1} X \\x \\ldots \\x T_{z^l_m} X": "Cartesian product of the tangent spaces at the inducing locations"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:09:54.726463"
    },
    {
      "label": "<<FORMULA_0082>>",
      "formula": "\\v{m}^l = P_{\\v{z}^l} \\widetilde{\\v{m}}^l\n,\n&&\n\\m{S}^l = P_{\\v{z}^l} \\widetilde{\\m{S}}^l P_{\\v{z}^l}^{\\top}\n,\n&&\nP_{\\v{z}^l} = P_{z^l_1} \\oplus \\ldots \\oplus P_{z^l_m}\n,",
      "raw_latex": "\\[\n\\v{m}^l = P_{\\v{z}^l} \\widetilde{\\v{m}}^l\n,\n&&\n\\m{S}^l = P_{\\v{z}^l} \\widetilde{\\m{S}}^l P_{\\v{z}^l}^{\\top}\n,\n&&\nP_{\\v{z}^l} = P_{z^l_1} \\oplus \\ldots \\oplus P_{z^l_m}\n,\n\\]",
      "formula_type": "display",
      "line_number": 422,
      "is_formula": true,
      "high_level_explanation": "These equalities enforce that the variational mean and covariance for the l-th layer’s inducing variables lie in the appropriate tangent spaces on the manifold. Unconstrained parameters are projected by a block-diagonal projector so that the resulting mean and covariance are tangent at each inducing location. The projector is the direct sum of pointwise projections at the inducing points, and the covariance is symmetrically projected via left and right multiplication by the projector and its transpose.",
      "notations": {
        "\\v{m}^l": "Mean vector of inducing pseudo-observations at layer l, constrained to tangent spaces",
        "\\m{S}^l": "Covariance matrix of inducing pseudo-observations at layer l, with image in the product of tangent spaces",
        "\\widetilde{\\v{m}}^l": "Unconstrained mean parameter before projection (optimised in ambient coordinates)",
        "\\widetilde{\\m{S}}^l": "Unconstrained positive semi-definite matrix before projection",
        "P_{\\v{z}^l}": "Block-diagonal projection operator onto the product of tangent spaces at the inducing locations in \\v{z}^l",
        "P_{\\v{z}^l}^{\\top}": "Matrix transpose of the block-diagonal projector P_{\\v{z}^l}",
        "P_{z^l_1}": "Projection from the ambient space onto the tangent space at the first inducing location z^l_1",
        "P_{z^l_m}": "Projection from the ambient space onto the tangent space at the m-th inducing location z^l_m",
        "\\oplus": "Direct sum, used here to form a block-diagonal matrix from individual projectors",
        "\\v{z}^l": "Collection of inducing locations at layer l on the manifold (z^l_1, ..., z^l_m)",
        "z^l_1": "First inducing location on the manifold at layer l",
        "z^l_m": "m-th inducing location on the manifold at layer l",
        "l": "Layer index in the deep Gaussian process",
        "m": "Number of inducing locations in the layer"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:09:48.232957"
    },
    {
      "label": "<<FORMULA_0089>>",
      "formula": "\\label{eqn:sequential_sampling}\n\\hat{F}(x_i) = \\hat{f}^{L}_i\n,\n&&\n\\hat{f}^{l}_i = \\exp_{\\hat{f}^{l-1}_i}(g_{\\v{z}^l, \\v{m}^l, \\m{S}^l}(\\hat{f}^{l-1}_i))\n\\t{~for~}\n1 \\leq l \\leq L\n,\n&&\n\\hat{f}^{0}_i = x_i\n,",
      "raw_latex": "\\[ \\label{eqn:sequential_sampling}\n\\hat{F}(x_i) = \\hat{f}^{L}_i\n,\n&&\n\\hat{f}^{l}_i = \\exp_{\\hat{f}^{l-1}_i}(g_{\\v{z}^l, \\v{m}^l, \\m{S}^l}(\\hat{f}^{l-1}_i))\n\\t{~for~}\n1 \\leq l \\leq L\n,\n&&\n\\hat{f}^{0}_i = x_i\n,\n\\]",
      "formula_type": "display",
      "line_number": 441,
      "is_formula": true,
      "high_level_explanation": "This expression specifies a sequential sampling procedure for a deep model on a manifold. Starting from the input x_i, one iteratively applies L layers: at each layer l, a sparse Gaussian vector field g_{·} evaluated at the current point produces a tangent vector, which is mapped back to the manifold using the Riemannian exponential map at that point. The final state after L steps, \\hat{f}^{L}_i, is the desired sample \\hat{F}(x_i).",
      "notations": {
        "\\hat{F}(x_i)": "Desired sample at x_i obtained after L sequential updates",
        "\\hat{f}^{L}_i": "Final layer output (sample) for input i",
        "\\hat{f}^{l}_i": "Intermediate sample at layer l for input i",
        "\\hat{f}^{0}_i": "Initial state for input i; set equal to x_i",
        "x_i": "NOT MENTIONED",
        "\\exp_{\\hat{f}^{l-1}_i}": "Riemannian exponential map at \\hat{f}^{l-1}_i that maps a tangent vector to a point on the manifold",
        "g_{\\v{z}^l, \\v{m}^l, \\m{S}^l}": "Sparse Gaussian vector field at layer l parameterised by inducing locations and variational parameters",
        "\\v{z}^l": "Inducing locations for layer l on the manifold X",
        "\\v{m}^l": "Mean pseudo-observations for layer l, with components in the appropriate tangent spaces",
        "\\m{S}^l": "Covariance of pseudo-observations for layer l (positive semi-definite), acting within the tangent spaces",
        "l": "NOT MENTIONED",
        "L": "NOT MENTIONED"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:09:41.385847"
    },
    {
      "label": "<<FORMULA_0128>>",
      "formula": "\\label{eq:scalar_matern_kernel_addition_theorem}\n    k_{\\nu, \\kappa, \\sigma^2}(x, x') = \\frac{\\sigma^2}{C_{\\nu, \\kappa}} \\\\prod_{k=0}^{K} \\Phi_{\\nu,\\kappa}(\\lambda_k) c_{k, d} C_k^{(\\alpha)}(x\\cdot x').",
      "raw_latex": "\\begin{equation}\\label{eq:scalar_matern_kernel_addition_theorem}\n    k_{\\nu, \\kappa, \\sigma^2}(x, x') = \\frac{\\sigma^2}{C_{\\nu, \\kappa}} \\\\prod_{k=0}^{K} \\Phi_{\\nu,\\kappa}(\\lambda_k) c_{k, d} C_k^{(\\alpha)}(x\\cdot x').\n\\end{equation}",
      "formula_type": "equation",
      "line_number": 718,
      "is_formula": true,
      "high_level_explanation": "This expression gives the scalar Matérn kernel on the sphere as a finite expansion over spectral degrees 0 through K, using the addition theorem to write it in terms of Gegenbauer polynomials evaluated at the dot product of the two points. Each term is weighted by the spectral factor Φ and a known constant c_{k,d}, and the whole kernel is scaled by the variance σ² and a normalization constant C_{ν,κ}. The dot product is taken after embedding the sphere S_d into R^{d+1} as the unit sphere. The truncation to K terms accelerates computation while approximating the infinite spectral expansion.",
      "notations": {
        "k_{\\nu, \\kappa, \\sigma^2}(x, x')": "Scalar Matérn kernel on the sphere S_d evaluated at points x and x'",
        "\\nu": "Smoothness parameter of the Matérn kernel",
        "\\kappa": "NOT MENTIONED",
        "\\sigma^2": "Kernel variance parameter",
        "C_{\\nu, \\kappa}": "NOT MENTIONED",
        "K": "Truncation level; include harmonics/eigenvalues up to the K-th",
        "\\Phi_{\\nu,\\kappa}": "NOT MENTIONED",
        "\\lambda_k": "Eigenvalue associated with spectral degree k of the negative Laplace–Beltrami operator on S_d",
        "c_{k, d}": "Known absolute constants from the addition theorem depending on degree k and dimension d",
        "C_k^{(\\alpha)}(x\\cdot x')": "Gegenbauer polynomial of degree k (parameterized by \\alpha) evaluated at the embedded dot product x·x'",
        "x": "Point on the unit sphere S_d (embedded in R^{d+1})",
        "x'": "Point on the unit sphere S_d (embedded in R^{d+1})",
        "d": "Intrinsic dimension of the sphere S_d",
        "\\alpha": "NOT MENTIONED",
        "k": "Spectral index/degree used in the expansion"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:09:30.899329"
    },
    {
      "label": "<<FORMULA_0132>>",
      "formula": "\\v{k}^{\\mathrm{div}}_{\\nu, \\kappa, \\sigma^2}(x, x') &= \\frac{\\sigma^2}{C^{\\mathrm{div}}_{\\nu, \\kappa}} \\sum_{k=0}^{K} \\frac{\\Phi_{\\nu,\\kappa}(\\lambda_k)}{\\lambda_k} \\frac{k \\\\times \\alpha}{\\alpha}(\\nabla_x \\otimes \\nabla_{x'})C_k^{(\\alpha)}(x\\cdot x')  \\\\\n    \\v{k}^{\\mathrm{curl}}_{\\nu, \\kappa, \\sigma^2}(x, x') &= \\frac{\\sigma^2}{C^{\\mathrm{curl}}_{\\nu, \\kappa}} \\sum_{k=0}^{K} \\frac{\\Phi_{\\nu,\\kappa}(\\lambda_k)}{\\lambda_k} \\frac{k + \\alpha}{\\alpha}(\\star\\nabla_x \\otimes \\star\\nabla_{x'})C_k^{(\\alpha)}(x\\cdot x').",
      "raw_latex": "\\begin{align}\n    \\v{k}^{\\mathrm{div}}_{\\nu, \\kappa, \\sigma^2}(x, x') &= \\frac{\\sigma^2}{C^{\\mathrm{div}}_{\\nu, \\kappa}} \\sum_{k=0}^{K} \\frac{\\Phi_{\\nu,\\kappa}(\\lambda_k)}{\\lambda_k} \\frac{k \\\\times \\alpha}{\\alpha}(\\nabla_x \\otimes \\nabla_{x'})C_k^{(\\alpha)}(x\\cdot x')  \\\\\n    \\v{k}^{\\mathrm{curl}}_{\\nu, \\kappa, \\sigma^2}(x, x') &= \\frac{\\sigma^2}{C^{\\mathrm{curl}}_{\\nu, \\kappa}} \\sum_{k=0}^{K} \\frac{\\Phi_{\\nu,\\kappa}(\\lambda_k)}{\\lambda_k} \\frac{k + \\alpha}{\\alpha}(\\star\\nabla_x \\otimes \\star\\nabla_{x'})C_k^{(\\alpha)}(x\\cdot x').\n\\end{align}",
      "formula_type": "align",
      "line_number": 724,
      "is_formula": true,
      "high_level_explanation": "These two expressions give the divergence-free and curl-free components of a Hodge–Matérn vector kernel on the sphere using the addition theorem for spherical harmonics. Each component is obtained by applying (star-)gradient operators at x and x' to Gegenbauer polynomials of the inner product x·x', and summing over harmonic degrees up to K with weights Φ(λ_k)/λ_k and normalizing constants. The factors depending on k and α modulate the contribution of each harmonic degree for the divergence or curl decomposition, while σ^2 sets the overall variance.",
      "notations": {
        "\\v{k}^{\\mathrm{div}}_{\\nu, \\kappa, \\sigma^2}(x, x')": "Divergence-free component of the Hodge–Matérn vector kernel evaluated at inputs x and x'",
        "\\v{k}^{\\mathrm{curl}}_{\\nu, \\kappa, \\sigma^2}(x, x')": "Curl-free component of the Hodge–Matérn vector kernel evaluated at inputs x and x'",
        "\\sigma^2": "Kernel variance (overall amplitude)",
        "C^{\\mathrm{div}}_{\\nu, \\kappa}": "NOT MENTIONED",
        "C^{\\mathrm{curl}}_{\\nu, \\kappa}": "NOT MENTIONED",
        "K": "Truncation level of the harmonic expansion (maximum degree included in the sum)",
        "\\Phi_{\\nu,\\kappa}(\\lambda_k)": "NOT MENTIONED",
        "\\lambda_k": "k-th eigenvalue of the negative Laplace–Beltrami operator on the sphere",
        "k": "Harmonic degree (summation index)",
        "\\alpha": "NOT MENTIONED",
        "\\nabla_x": "Gradient with respect to x on the manifold",
        "\\nabla_{x'}": "Gradient with respect to x' on the manifold",
        "\\otimes": "Tensor (outer) product operator",
        "\\star": "Hodge star operator",
        "C_k^{(\\alpha)}(x\\cdot x')": "Gegenbauer polynomial of degree k and parameter α evaluated at the inner product of x and x'",
        "x": "Point on the sphere (input location)",
        "x'": "Point on the sphere (input location)",
        "x\\cdot x'": "Euclidean inner product after embedding the sphere into R^{d+1}",
        "\\nu": "Smoothness parameter of the Matérn kernel",
        "\\kappa": "NOT MENTIONED"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:09:00.441497"
    },
    {
      "label": "<<FORMULA_0139>>",
      "formula": "\\mu_{\\v{z}, \\v{m'}, \\v{S'}}(\\cdot)\n&=\n\\mu(\\cdot)\n+\nk(\\cdot, \\v{z})\nk(\\v{z}, \\v{z})^{-1}\n(\\m{L}\\v{m}'-\\mu(\\v{z}))\n\\\\\n\\label{eq:whitened_variational_posterior_mean}\n&=\n\\mu(\\cdot)\n+\nk(\\cdot, \\v{z})\n\\m{L}^{-\\top}\n(\\v{m}' - \\m{L}^{-1}\\mu(\\v{z}))\n,\n\\\\\nk_{\\v{z}, \\v{m}', \\v{S}'}(\\cdot, \\cdot')\n&=\nk(\\cdot, \\cdot')\n-\nk(\\cdot, \\v{z})\nk(\\v{z}, \\v{z})^{-1}\n(k(\\v{z}, \\v{z}) - \\m{L}\\m{S}'\\m{L}^\\top)\nk(\\v{z}, \\v{z})^{-1}\nk(\\v{z}, \\cdot')\n\\\\\n\\label{eq:whitened_variational_posterior_kernel}\n&=\nk(\\cdot, \\cdot')\n-\nk(\\cdot, \\v{z})\n\\m{L}^{-\\top}\n(\\m{I} - \\m{S'})\n\\m{L}^{-1}\nk(\\v{z}, \\cdot').",
      "raw_latex": "\\begin{align}\n\\mu_{\\v{z}, \\v{m'}, \\v{S'}}(\\cdot)\n&=\n\\mu(\\cdot)\n+\nk(\\cdot, \\v{z})\nk(\\v{z}, \\v{z})^{-1}\n(\\m{L}\\v{m}'-\\mu(\\v{z}))\n\\\\\n\\label{eq:whitened_variational_posterior_mean}\n&=\n\\mu(\\cdot)\n+\nk(\\cdot, \\v{z})\n\\m{L}^{-\\top}\n(\\v{m}' - \\m{L}^{-1}\\mu(\\v{z}))\n,\n\\\\\nk_{\\v{z}, \\v{m}', \\v{S}'}(\\cdot, \\cdot')\n&=\nk(\\cdot, \\cdot')\n-\nk(\\cdot, \\v{z})\nk(\\v{z}, \\v{z})^{-1}\n(k(\\v{z}, \\v{z}) - \\m{L}\\m{S}'\\m{L}^\\top)\nk(\\v{z}, \\v{z})^{-1}\nk(\\v{z}, \\cdot')\n\\\\\n\\label{eq:whitened_variational_posterior_kernel}\n&=\nk(\\cdot, \\cdot')\n-\nk(\\cdot, \\v{z})\n\\m{L}^{-\\top}\n(\\m{I} - \\m{S'})\n\\m{L}^{-1}\nk(\\v{z}, \\cdot').\n\\end{align}",
      "formula_type": "align",
      "line_number": 735,
      "is_formula": true,
      "high_level_explanation": "These equations give the mean function and covariance kernel of the variational posterior Gaussian process when using inducing points with a whitened parameterization. The first form uses the inverse of the inducing-point Gram matrix k(z, z), while the second rewrites the same expressions using the Cholesky factor L so that the variational parameters (m', S') are whitened, improving numerical stability and optimization. The resulting GP posterior is parameterized by the inducing locations z and the whitened variational mean and covariance.",
      "notations": {
        "\\mu_{\\v{z}, \\v{m'}, \\v{S'}}(\\cdot)": "Variational posterior mean function parameterized by inducing locations and whitened variational parameters",
        "k_{\\v{z}, \\v{m}', \\v{S}'}(\\cdot, \\cdot')": "Variational posterior kernel (covariance function) parameterized by inducing locations and whitened variational parameters",
        "\\mu(\\cdot)": "Prior GP mean function",
        "k(\\cdot, \\cdot')": "Prior GP covariance kernel between two inputs",
        "\\v{z}": "Inducing inputs (locations) used for the variational approximation",
        "k(\\v{z}, \\v{z})": "Kernel Gram matrix evaluated at the inducing inputs",
        "k(\\cdot, \\v{z})": "Cross-covariance between a test input and the inducing inputs",
        "k(\\v{z}, \\cdot')": "Cross-covariance between the inducing inputs and a test input",
        "\\m{L}": "Lower Cholesky factor of k(\\v{z}, \\v{z})",
        "\\m{L}^{-1}": "Inverse of the Cholesky factor L",
        "\\m{L}^{-\\top}": "Inverse transpose of the Cholesky factor L",
        "\\v{m'}": "Whitened variational mean of the inducing variables",
        "\\v{m}'": "Whitened variational mean of the inducing variables",
        "\\m{S'}": "Whitened variational covariance of the inducing variables",
        "\\m{S}'": "Whitened variational covariance of the inducing variables",
        "\\mu(\\v{z})": "Prior mean evaluated at the inducing inputs",
        "\\m{I}": "Identity matrix of size equal to the number of inducing inputs"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:08:25.743109"
    },
    {
      "label": "<<FORMULA_0147>>",
      "formula": "\\mathrm{colatitude} = \\arccos\\left(1 - \\frac{2i + 1}{n}\\right),\n    &&\n    \\mathrm{longitude} = \\frac{2\\pi i}{\\phi},\n    &&\n    \\phi = \\frac{1 + \\sqrt{5}}{2}.",
      "raw_latex": "\\begin{align}\n    \\mathrm{colatitude} = \\arccos\\left(1 - \\frac{2i + 1}{n}\\right),\n    &&\n    \\mathrm{longitude} = \\frac{2\\pi i}{\\phi},\n    &&\n    \\phi = \\frac{1 + \\sqrt{5}}{2}.\n\\end{align}",
      "formula_type": "align",
      "line_number": 812,
      "is_formula": true,
      "high_level_explanation": "These equations specify the spherical coordinates of the i-th point in a Fibonacci lattice of n points on the unit sphere. The colatitude is chosen to yield equal-area spacing across latitudinal bands, while the longitude increments by a golden-ratio-based angle to avoid alignment and produce near-uniform coverage. Here φ is set to the golden ratio.",
      "notations": {
        "\\mathrm{colatitude}": "Colatitude angle of the i-th point on the sphere",
        "\\mathrm{longitude}": "Longitude angle of the i-th point on the sphere",
        "i": "Index of the i-th point in the grid",
        "n": "Total number of points in the grid",
        "\\phi": "Golden ratio, (1 + sqrt(5))/2"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:09:32.394368"
    },
    {
      "label": "<<FORMULA_0150>>",
      "formula": "Y_{2,3}(\\theta, \\phi) &= \\sqrt{\\frac{105}{32\\pi}} \\sin^3\\theta \\sin(3\\phi),\n    \\\\\n    Y_{1,2}(\\theta, \\phi) &= \\sqrt{\\frac{15}{8\\pi}} \\\\cos\\theta \\sin(2\\phi)\n    \\\\\n    R(x) &= (x_1, -x_3, x_2)\n    \\\\\n    \\varphi(x) &= (\\operatorname{atan2}(x_2, x_1), \\arccos(x_3)),",
      "raw_latex": "\\begin{align}\n    Y_{2,3}(\\theta, \\phi) &= \\sqrt{\\frac{105}{32\\pi}} \\sin^3\\theta \\sin(3\\phi),\n    \\\\\n    Y_{1,2}(\\theta, \\phi) &= \\sqrt{\\frac{15}{8\\pi}} \\\\cos\\theta \\sin(2\\phi)\n    \\\\\n    R(x) &= (x_1, -x_3, x_2)\n    \\\\\n    \\varphi(x) &= (\\operatorname{atan2}(x_2, x_1), \\arccos(x_3)),\n\\end{align}",
      "formula_type": "align",
      "line_number": 825,
      "is_formula": true,
      "high_level_explanation": "These equations define two specific spherical harmonic functions on the 2-sphere in terms of the angular variables (theta, phi), a fixed linear rotation/permutation R acting on 3D Cartesian coordinates, and a coordinate map varphi that converts a Cartesian point x on the unit sphere to spherical angles. The spherical harmonics Y_{2,3} and Y_{1,2} are given in real form using sine and cosine factors of theta and phi. The map varphi = (atan2(x2, x1), arccos(x3)) returns the azimuth then the colatitude, which (as noted in the paper) swaps the conventional order (theta, phi) used in Y.",
      "notations": {
        "Y_{2,3}(\\theta, \\phi)": "Spherical harmonic (real form) indexed by (2, 3), evaluated at angles (theta, phi)",
        "Y_{1,2}(\\theta, \\phi)": "Spherical harmonic (real form) indexed by (1, 2), evaluated at angles (theta, phi)",
        "\\theta": "Colatitude angle on the sphere",
        "\\phi": "Longitude (azimuth) angle on the sphere",
        "R(x)": "Fixed linear permutation/rotation of a 3D vector: (x1, -x3, x2)",
        "x": "Point on the unit sphere embedded in R^3",
        "x_1": "First Cartesian coordinate of x",
        "x_2": "Second Cartesian coordinate of x",
        "x_3": "Third Cartesian coordinate of x",
        "\\varphi(x)": "Map from Cartesian to spherical angles returning (phi, theta) = (atan2(x2, x1), arccos(x3)); order is swapped relative to (theta, phi)",
        "\\operatorname{atan2}": "Two-argument arctangent returning the azimuth angle from (y, x) = (x2, x1)"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:08:54.606370"
    },
    {
      "label": "<<FORMULA_0155>>",
      "formula": "\\alpha_\\text{hodge} &= \\underbrace{(m^2 + m)/2}_{\\text{covariance}} + \\underbrace{m}_\\text{mean}\\\\\n    \\alpha_\\text{euclidean} = \\alpha_\\text{projected} &= 3\\cdot((m^2 + m)/2+m)",
      "raw_latex": "\\begin{align}\n    \\alpha_\\text{hodge} &= \\underbrace{(m^2 + m)/2}_{\\text{covariance}} + \\underbrace{m}_\\text{mean}\\\\\n    \\alpha_\\text{euclidean} = \\alpha_\\text{projected} &= 3\\cdot((m^2 + m)/2+m)\n\\end{align}",
      "formula_type": "align",
      "line_number": 842,
      "is_formula": true,
      "high_level_explanation": "These formulas give the number of variational parameters α in a hidden layer as a function of the number of inducing variables m. For a Hodge GVF, the count is the sum of the distinct covariance parameters (m(m+1)/2) and the mean parameters (m). For the Euclidean and projected variants, the same per-process parameter count is multiplied by 3.",
      "notations": {
        "\\alpha_\\text{hodge}": "Number of variational parameters in a hidden layer for a Hodge GVF",
        "\\alpha_\\text{euclidean}": "Number of variational parameters in a hidden layer for a Euclidean vector-valued GP",
        "\\alpha_\\text{projected}": "Number of variational parameters in a hidden layer for a projected GVF",
        "m": "Number of inducing variables per GVF"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:09:20.717416"
    },
    {
      "label": "<<FORMULA_0165>>",
      "formula": "\\m{\\Psi}_{i:j}(\\cdot) = \\left(\\sqrt{\\frac{\\sigma^2}{C_{\\nu, \\kappa}}\\Phi_{\\nu,\\kappa}(\\lambda_{i})}\\phi_{i}(\\cdot), \\dots, \\sqrt{\\frac{\\sigma^2}{C_{\\nu, \\kappa}}\\Phi_{\\nu,\\kappa}(\\lambda_j)}\\phi_l(\\cdot)\\right)^{\\top}\n,",
      "raw_latex": "\\[\n\\m{\\Psi}_{i:j}(\\cdot) = \\left(\\sqrt{\\frac{\\sigma^2}{C_{\\nu, \\kappa}}\\Phi_{\\nu,\\kappa}(\\lambda_{i})}\\phi_{i}(\\cdot), \\dots, \\sqrt{\\frac{\\sigma^2}{C_{\\nu, \\kappa}}\\Phi_{\\nu,\\kappa}(\\lambda_j)}\\phi_l(\\cdot)\\right)^{\\top}\n,\n\\]",
      "formula_type": "display",
      "line_number": 1049,
      "is_formula": true,
      "high_level_explanation": "This equation defines a feature map Ψ_{i:j}(·) as a column vector whose components are basis functions φ_t(·) (for t from i to j) scaled by the square root of kernel-specific spectral weights. The scaling combines the Matérn kernel’s prior variance σ^2, a constant C_{ν,κ}, and a function Φ_{ν,κ}(λ_t) evaluated at eigenvalues λ_t. This construction enables writing the (truncated) Matérn kernel as an inner product of these features in later derivations. Note: the last component shows φ_l(·), which appears to be a typographical l and likely intended to match the upper index j.",
      "notations": {
        "\\m{\\Psi}_{i:j}(\\cdot)": "Column vector of scaled basis functions from index i to j; each component \\phi_t(\\cdot) is multiplied by the square root of (\\sigma^2/C_{\\nu,\\kappa})\\,\\Phi_{\\nu,\\kappa}(\\lambda_t)",
        "\\sigma^2": "Prior variance hyperparameter of the Matérn Gaussian process",
        "C_{\\nu, \\kappa}": "NOT MENTIONED",
        "\\Phi_{\\nu,\\kappa}(\\lambda_{i})": "NOT MENTIONED",
        "\\Phi_{\\nu,\\kappa}(\\lambda_j)": "NOT MENTIONED",
        "\\lambda_{i}": "NOT MENTIONED",
        "\\lambda_j": "NOT MENTIONED",
        "\\phi_{i}(\\cdot)": "NOT MENTIONED",
        "\\phi_l(\\cdot)": "NOT MENTIONED"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:08:40.779405"
    },
    {
      "label": "<<FORMULA_0168>>",
      "formula": "k(\\cdot, \\v{\\zeta})\\m{L}^{-\\top}\n    &=\n    \\left(\\phi_0(\\cdot), \\dots, \\phi_M(\\cdot)\\right)^\\top\\mathrm{diag}\\left(\\frac{1}{\\frac{\\sigma^2}{C_{\\nu, \\kappa}}\\Phi_{\\nu,\\kappa}(\\lambda_{0})}, \\dots, \\frac{1}{\\frac{\\sigma^2}{C_{\\nu, \\kappa}}\\Phi_{\\nu,\\kappa}(\\lambda_{M})}\\right)^{-1/2}\n    \\\\\n    &=\n    \\left(\\phi_0(\\cdot), \\dots, \\phi_M(\\cdot)\\right)^\\top\n    \\mathrm{diag}\\left(\\sqrt{\\frac{\\sigma^2}{C_{\\nu, \\kappa}}\\Phi_{\\nu,\\kappa}(\\lambda_{0})}, \\dots, \\sqrt{\\frac{\\sigma^2}{C_{\\nu, \\kappa}}\\Phi_{\\nu,\\kappa}(\\lambda_{M})}\\right)\n    \\\\\n    &=\n    \\m{\\Psi}_{0: M}(\\cdot)^{\\top},",
      "raw_latex": "\\begin{align}\n    k(\\cdot, \\v{\\zeta})\\m{L}^{-\\top}\n    &=\n    \\left(\\phi_0(\\cdot), \\dots, \\phi_M(\\cdot)\\right)^\\top\\mathrm{diag}\\left(\\frac{1}{\\frac{\\sigma^2}{C_{\\nu, \\kappa}}\\Phi_{\\nu,\\kappa}(\\lambda_{0})}, \\dots, \\frac{1}{\\frac{\\sigma^2}{C_{\\nu, \\kappa}}\\Phi_{\\nu,\\kappa}(\\lambda_{M})}\\right)^{-1/2}\n    \\\\\n    &=\n    \\left(\\phi_0(\\cdot), \\dots, \\phi_M(\\cdot)\\right)^\\top\n    \\mathrm{diag}\\left(\\sqrt{\\frac{\\sigma^2}{C_{\\nu, \\kappa}}\\Phi_{\\nu,\\kappa}(\\lambda_{0})}, \\dots, \\sqrt{\\frac{\\sigma^2}{C_{\\nu, \\kappa}}\\Phi_{\\nu,\\kappa}(\\lambda_{M})}\\right)\n    \\\\\n    &=\n    \\m{\\Psi}_{0: M}(\\cdot)^{\\top},\n\\end{align}",
      "formula_type": "align",
      "line_number": 1055,
      "is_formula": true,
      "high_level_explanation": "This sequence of equalities rewrites the whitened cross-covariance vector k(·, ζ) L^{-T} in the eigenfunction basis. Multiplying by the inverse square-root of a diagonal matrix of reciprocal weights is equivalent to multiplying by a diagonal matrix whose entries are the square roots of the corresponding weights. Using the earlier definition of the scaled feature map Ψ, the expression reduces to Ψ_{0:M}(·) transposed. Intuitively, the whitening maps the cross-covariance into a vector of scaled eigenfunctions.",
      "notations": {
        "k(\\cdot, \\v{\\zeta})": "NOT MENTIONED",
        "\\m{L}": "NOT MENTIONED",
        "\\sigma^2": "Prior variance hyperparameter of the Matérn kernel",
        "C_{\\nu, \\kappa}": "NOT MENTIONED",
        "\\Phi_{\\nu,\\kappa}(\\lambda_{0})": "NOT MENTIONED",
        "\\Phi_{\\nu,\\kappa}(\\lambda_{M})": "NOT MENTIONED",
        "\\lambda_{0}": "NOT MENTIONED",
        "\\lambda_{M}": "NOT MENTIONED",
        "M": "Number of interdomain inducing variables",
        "\\phi_0(\\cdot)": "Eigenfunction with index 0 used in the spectral approximation",
        "\\phi_M(\\cdot)": "Eigenfunction with index M used in the spectral approximation",
        "\\m{\\Psi}_{0: M}(\\cdot)": "Vector of scaled eigenfunctions from index 0 to M: (sqrt((\\sigma^2/C_{\\nu, \\kappa})\\,\\Phi_{\\nu,\\kappa}(\\lambda_0))\\,\\phi_0(\\cdot), \\dots, sqrt((\\sigma^2/C_{\\nu, \\kappa})\\,\\Phi_{\\nu,\\kappa}(\\lambda_M))\\,\\phi_M(\\cdot))^\\top"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:08:44.161185"
    },
    {
      "label": "<<FORMULA_0169>>",
      "formula": "k_{\\v{\\zeta}, \\v{m}', \\m{S}'}(\\cdot, \\cdot')\n    &=\n    k(\\cdot, \\cdot')\n    -\n    k(\\cdot, \\v{\\zeta})\n    \\m{L}^{-\\top}\n    (\\m{I} - \\m{S'})\n    \\m{L}^{-1}\n    k(\\v{\\zeta}, \\cdot').\n    \\\\\n    &=\n    \\m{\\Psi}_{0:K}^\\top(\\cdot)\\m{\\Psi}_{0:K}(\\cdot')\n    -\n    \\m{\\Psi}_{0:M}^\\top(\\cdot)\n    (\\m{I} - \\m{S'})\n    \\m{\\Psi}_{0:M}(\\cdot')\n    \\\\\n    &=\n    \\m{\\Psi}_{M+1:K}^\\top(\\cdot)\\m{\\Psi}_{M+1:K}(\\cdot')\n    +\n    \\m{\\Psi}_{0:M}^\\top(\\cdot)\\m{S'}\\m{\\Psi}_{0:M}(\\cdot')",
      "raw_latex": "\\begin{align}\n    k_{\\v{\\zeta}, \\v{m}', \\m{S}'}(\\cdot, \\cdot')\n    &=\n    k(\\cdot, \\cdot')\n    -\n    k(\\cdot, \\v{\\zeta})\n    \\m{L}^{-\\top}\n    (\\m{I} - \\m{S'})\n    \\m{L}^{-1}\n    k(\\v{\\zeta}, \\cdot').\n    \\\\\n    &=\n    \\m{\\Psi}_{0:K}^\\top(\\cdot)\\m{\\Psi}_{0:K}(\\cdot')\n    -\n    \\m{\\Psi}_{0:M}^\\top(\\cdot)\n    (\\m{I} - \\m{S'})\n    \\m{\\Psi}_{0:M}(\\cdot')\n    \\\\\n    &=\n    \\m{\\Psi}_{M+1:K}^\\top(\\cdot)\\m{\\Psi}_{M+1:K}(\\cdot')\n    +\n    \\m{\\Psi}_{0:M}^\\top(\\cdot)\\m{S'}\\m{\\Psi}_{0:M}(\\cdot')\n\\end{align}",
      "formula_type": "align",
      "line_number": 1068,
      "is_formula": true,
      "high_level_explanation": "This expression gives the posterior (variational) kernel of a sparse Matérn Gaussian process in a whitened parameterization with interdomain inducing variables. It shows three equivalent forms: (i) prior kernel minus a correction term involving the whitening matrix and the variational covariance; (ii) a feature-space form using scaled eigenfunction features Ψ over 0:K and 0:M; and (iii) a decomposition where high-index components (M+1:K) remain as in the prior while low-index components (0:M) are shaped by the variational covariance S'. This highlights that only the first M basis coefficients are modified by variational parameters, whereas the remaining K−M components pass through unchanged.",
      "notations": {
        "k_{\\v{\\zeta}, \\v{m}', \\m{S}'}": "Posterior kernel under the sparse variational approximation, parameterized by interdomain inducing variables \\v{\\zeta} and variational parameters \\v{m}' and \\m{S}'.",
        "k": "Prior kernel function (manifold Matérn kernel).",
        "\\v{\\zeta}": "Set of interdomain inducing variables.",
        "\\v{m}'": "Variational parameter vector.",
        "\\m{S'}": "Variational covariance matrix.",
        "\\m{L}": "NOT MENTIONED",
        "\\m{I}": "Identity matrix.",
        "\\m{\\Psi}_{0:K}": "Stacked vector of scaled eigenfunction features for indices 0 through K (as defined in the text).",
        "\\m{\\Psi}_{0:M}": "Stacked vector of scaled eigenfunction features for indices 0 through M (as defined in the text).",
        "\\m{\\Psi}_{M+1:K}": "Stacked vector of scaled eigenfunction features for indices M+1 through K (as defined in the text).",
        "K": "Number of eigenfunctions used to approximate the manifold Matérn kernel.",
        "M": "Number of interdomain inducing variables."
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:08:31.874031"
    },
    {
      "label": "<<FORMULA_0179>>",
      "formula": "&k_{\\v{\\zeta}, \\v{m}', \\m{S}'}(\\cdot, \\cdot')\n\\\\\n=&\\m{\\Psi}_{M+1:K}^\\top(\\cdot)\\m{\\Psi}_{M+1:K}(\\cdot')\n+\n\\m{\\Psi}_{0:M}^\\top(\\cdot)\\m{S'}\\m{\\Psi}_{0:M}(\\cdot')\n+ \\m{\\Psi}_{M+1:K}^\\top(\\cdot)(\\m{D'} - \\m{I})\\m{\\Psi}_{M+1:K}(\\cdot') \\\\\n=&\\m{\\Psi}_{0:M}^\\top(\\cdot)\\m{S'}\\m{\\Psi}_{0:M}(\\cdot') + \\m{\\Psi}_{M+1:K}^\\top(\\cdot)\\m{D'}\\m{\\Psi}_{M+1:K}(\\cdot').",
      "raw_latex": "\\begin{align}\n&k_{\\v{\\zeta}, \\v{m}', \\m{S}'}(\\cdot, \\cdot')\n\\\\\n=&\\m{\\Psi}_{M+1:K}^\\top(\\cdot)\\m{\\Psi}_{M+1:K}(\\cdot')\n+\n\\m{\\Psi}_{0:M}^\\top(\\cdot)\\m{S'}\\m{\\Psi}_{0:M}(\\cdot')\n+ \\m{\\Psi}_{M+1:K}^\\top(\\cdot)(\\m{D'} - \\m{I})\\m{\\Psi}_{M+1:K}(\\cdot') \\\\\n=&\\m{\\Psi}_{0:M}^\\top(\\cdot)\\m{S'}\\m{\\Psi}_{0:M}(\\cdot') + \\m{\\Psi}_{M+1:K}^\\top(\\cdot)\\m{D'}\\m{\\Psi}_{M+1:K}(\\cdot').\n\\end{align}",
      "formula_type": "align",
      "line_number": 1103,
      "is_formula": true,
      "high_level_explanation": "This expression gives the variational posterior kernel after extending the variational family with an additional diagonal matrix for the residual eigenfunction subspace. It decomposes the covariance into two orthogonal parts: one over the first M eigenfunction features controlled by the variational covariance S', and one over the remaining features M+1 through K controlled by a diagonal matrix D'. The first line shows the prior contribution on the residual block plus an adjustment (D' − I), which simplifies to the final sum of the S' block and the D' block. This construction lets the variational parameters influence variance in both subspaces.",
      "notations": {
        "k_{\\v{\\zeta}, \\v{m}', \\m{S}'}(\\cdot, \\cdot')": "Variational posterior covariance kernel parametrised by the inducing variables \\v{\\zeta}, variational mean \\v{m}', and variational covariance \\m{S}'.",
        "\\v{\\zeta}": "Interdomain inducing variables (the collection for which the number is M).",
        "\\v{m}'": "Variational mean vector.",
        "\\m{S'}": "Variational covariance matrix.",
        "\\m{D'}": "Parametrised diagonal matrix for eigenfunctions not included in the original variational family (indices M+1 through K).",
        "\\m{I}": "Identity matrix.",
        "\\m{\\Psi}_{0:M}(\\cdot)": "Feature vector formed by the scaled eigenfunctions with indices 0 through M evaluated at the input; defined via (\\phi_0(\\cdot),\\ldots,\\phi_M(\\cdot)) with the kernel-dependent scaling shown in the context.",
        "\\m{\\Psi}_{0:M}^\\top(\\cdot)": "Row-vector (transpose) of the feature map for indices 0 through M at the input.",
        "\\m{\\Psi}_{M+1:K}(\\cdot')": "Feature vector for the eigenfunctions with indices M+1 through K evaluated at the second input.",
        "\\m{\\Psi}_{M+1:K}^\\top(\\cdot)": "Row-vector (transpose) of the feature map for indices M+1 through K at the first input."
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:08:56.193143"
    }
  ],
  "metadata": {
    "model": "gpt-5",
    "context_words": 300,
    "max_formulas": 20,
    "timestamp": "2025-10-31T17:09:58.781496",
    "total_formulas_in_paper": 20,
    "formulas_selected_for_analysis": 20,
    "skipped_by_length_limit": 0,
    "formulas_explained": 19,
    "notations_skipped": 1,
    "failed": 0
  },
  "skipped_notations": [
    {
      "label": "<<FORMULA_0093>>",
      "formula": "p(g_{\\v{\\zeta}, \\v{m}, \\v{S}}(\\cdot))\n=\n{\\E}_{\\v{u} \\sim q(\\v{u})} \\, p(g(\\cdot) \\given \\v{u}, \\v{\\zeta}),\n&&\nq(\\v{u}) = \\mathcal{N}(\\v{m}, \\m{S}),",
      "reason": "Classified as notation, not formula"
    }
  ],
  "failed": []
}