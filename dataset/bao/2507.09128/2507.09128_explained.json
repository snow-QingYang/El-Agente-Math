{
  "formulas": [
    {
      "label": "<<FORMULA_0378>>",
      "formula": "\\norm{\\A}_{\\HS(\\calG, \\calH)}^2 &= \\ip{\\A, \\A}_{\\HS(\\calG, \\calH)} \\notag\\\\\n    &= \\sum_{j \\in J} \\ip{\\A g_j, \\A g_j}_\\calH \\notag\\\\\n    &= \\sum_{j \\in J} \\sum_{k \\in K} \\sum_{l \\in K} \\ip{\\A g_j, h_k}_\\calH  \\ip{\\A g_j, h_l}_\\calH \\ip{h_k, h_l}_\\calH \\notag\\\\\n    &= \\\\prod_{j \\in J} \\sum_{k \\in K} \\ip{h_k, \\A g_j}_{\\calH}^2  = \\sum_{j \\in J} \\sum_{k \\in K} \\ip{\\A^* h_k, g_j}_{\\calG}^2. \\label{eq:compact:hsnorm}",
      "raw_latex": "\\begin{align}\n    \\norm{\\A}_{\\HS(\\calG, \\calH)}^2 &= \\ip{\\A, \\A}_{\\HS(\\calG, \\calH)} \\notag\\\\\n    &= \\sum_{j \\in J} \\ip{\\A g_j, \\A g_j}_\\calH \\notag\\\\\n    &= \\sum_{j \\in J} \\sum_{k \\in K} \\sum_{l \\in K} \\ip{\\A g_j, h_k}_\\calH  \\ip{\\A g_j, h_l}_\\calH \\ip{h_k, h_l}_\\calH \\notag\\\\\n    &= \\\\prod_{j \\in J} \\sum_{k \\in K} \\ip{h_k, \\A g_j}_{\\calH}^2  = \\sum_{j \\in J} \\sum_{k \\in K} \\ip{\\A^* h_k, g_j}_{\\calG}^2. \\label{eq:compact:hsnorm}\n\\end{align}",
      "formula_type": "align",
      "line_number": 1025,
      "is_formula": true,
      "high_level_explanation": "This identity expands the squared Hilbert–Schmidt norm of a linear operator A: G → H. It shows that the Hilbert–Schmidt inner product of A with itself equals a sum over any orthonormal basis of G of the H-norms of A g_j, and further expands via any orthonormal basis of H. The final equality rewrites the same quantity using the adjoint A*, yielding a double sum of squared coefficients in the G-basis. Altogether, it highlights that the Hilbert–Schmidt norm is basis-independent and can be computed via matrix coefficients in either space.",
      "notations": {
        "\\norm{\\A}_{\\HS(\\calG, \\calH)}^2": "Squared Hilbert–Schmidt norm of \\A",
        "\\ip{\\A, \\A}_{\\HS(\\calG, \\calH)}": "Hilbert–Schmidt inner product of \\A with itself",
        "\\HS(\\calG, \\calH)": "Hilbert space of Hilbert–Schmidt operators from \\calG to \\calH",
        "\\A": "Linear operator from \\calG to \\calH (assumed Hilbert–Schmidt here)",
        "\\A^*": "Adjoint of \\A, mapping \\calH to \\calG",
        "\\calG": "Separable Hilbert space (domain of \\A)",
        "\\calH": "Separable Hilbert space (codomain of \\A)",
        "J": "Index set for the orthonormal basis \\{g_j\\} of \\calG",
        "K": "Index set for the orthonormal basis \\{h_k\\} of \\calH",
        "g_j": "j-th vector of an orthonormal basis of \\calG",
        "h_k": "k-th vector of an orthonormal basis of \\calH",
        "h_l": "l-th vector of an orthonormal basis of \\calH"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:56:26.519409"
    },
    {
      "label": "<<FORMULA_0481>>",
      "formula": "\\ip{\\Rsans, \\alpha_j \\beta_k}_{\\ltwo(Q_X \\otimes Q_Z)} &= \\E{Q_X \\otimes Q_Z}{\\Rsans(X, Z) \\alpha_j(X) \\beta_k(Z)}\\\\\n        &= \\E{Q_{X, Z}}{\\alpha_j(X) \\beta_k(Z)}\\\\\n        &= \\E{Q_{X}}{\\alpha_j(X) \\E{Q_{X, Z}}{\\beta_k(Z)|X}}\\\\\n        &= \\ip{\\alpha_j, \\M_{Z|X}\\beta_k}_{\\ltwo(Q_X)}\\\\\n        &= \\begin{cases}\n            \\sigma_m &\\text{ if } j = k = i \\in I\\\\\n            0 & \\text{ otherwise}\n        \\end{cases},",
      "raw_latex": "\\begin{align*}\n        \\ip{\\Rsans, \\alpha_j \\beta_k}_{\\ltwo(Q_X \\otimes Q_Z)} &= \\E{Q_X \\otimes Q_Z}{\\Rsans(X, Z) \\alpha_j(X) \\beta_k(Z)}\\\\\n        &= \\E{Q_{X, Z}}{\\alpha_j(X) \\beta_k(Z)}\\\\\n        &= \\E{Q_{X}}{\\alpha_j(X) \\E{Q_{X, Z}}{\\beta_k(Z)|X}}\\\\\n        &= \\ip{\\alpha_j, \\M_{Z|X}\\beta_k}_{\\ltwo(Q_X)}\\\\\n        &= \\begin{cases}\n            \\sigma_m &\\text{ if } j = k = i \\in I\\\\\n            0 & \\text{ otherwise}\n        \\end{cases},\n    \\end{align*}",
      "formula_type": "align*",
      "line_number": 1158,
      "is_formula": true,
      "high_level_explanation": "The chain of equalities computes the inner product between the Radon–Nikodym derivative R (the density ratio of the joint distribution to the product of marginals) and the tensor-product basis function α_j(X)β_k(Z) in L2(Q_X ⊗ Q_Z). By changing measure and conditioning, this coefficient equals the inner product of α_j with the conditional mean operator applied to β_k. Using the singular value decomposition of the conditional mean operator, the coefficient is the corresponding singular value when j = k equals an active singular direction (indexed by I), and zero otherwise.",
      "notations": {
        "\\Rsans": "Radon–Nikodym derivative of the joint measure Q_{X,Z} with respect to the product measure Q_X ⊗ Q_Z",
        "\\alpha_j": "j-th element of an orthonormal basis of L2(Q_X) coming from the SVD of the conditional mean operator",
        "\\beta_k": "k-th element of an orthonormal basis of L2(Q_Z) coming from the SVD of the conditional mean operator",
        "Q_X \\otimes Q_Z": "Product measure of the marginals Q_X and Q_Z",
        "Q_{X, Z}": "Joint distribution (measure) of the pair (X, Z)",
        "\\ltwo(Q_X \\otimes Q_Z)": "Hilbert space L2 of square-integrable functions under the product measure Q_X ⊗ Q_Z",
        "\\ltwo(Q_X)": "Hilbert space L2 of square-integrable functions under Q_X",
        "\\M_{Z|X}": "Conditional mean operator mapping a function on Z to its conditional expectation given X",
        "X": "Random variable with marginal measure Q_X",
        "Z": "Random variable with marginal measure Q_Z",
        "I": "Index set of the non-zero singular values of \\M_{Z|X}",
        "\\sigma_m": "m-th singular value of the operator \\M_{Z|X}"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:56:23.699312"
    },
    {
      "label": "<<FORMULA_0824>>",
      "formula": "\\norm{\\eta_\\rho - \\etastar}_{\\ltwo(P_X)}^2 &= \\int_\\X \\p{\\eta_\\rho(\\x) - \\etastar(\\x)}^2 \\d P_X(\\x)\\notag\\\\\n         &\\leq 2\\int_{\\X_1} \\p{\\E{P_{Z|X=\\x}}{{g_\\rho(Z) - g_{P_{Y, Z}}(Z)}}}^2 \\d P_X(\\x) \\label{eq:main:dependence:rnd1}\\\\\n         &\\quad+ 2\\int_{\\X_1}\\p{\\E{P_{Z|X=\\x}}{{g_{P_{Y, Z}}(Z) - f(\\x, Z)}}}^2\\d P_X(\\x).\\label{eq:main:dependence:rnd2}",
      "raw_latex": "\\begin{align}\n         \\norm{\\eta_\\rho - \\etastar}_{\\ltwo(P_X)}^2 &= \\int_\\X \\p{\\eta_\\rho(\\x) - \\etastar(\\x)}^2 \\d P_X(\\x)\\notag\\\\\n         &\\leq 2\\int_{\\X_1} \\p{\\E{P_{Z|X=\\x}}{{g_\\rho(Z) - g_{P_{Y, Z}}(Z)}}}^2 \\d P_X(\\x) \\label{eq:main:dependence:rnd1}\\\\\n         &\\quad+ 2\\int_{\\X_1}\\p{\\E{P_{Z|X=\\x}}{{g_{P_{Y, Z}}(Z) - f(\\x, Z)}}}^2\\d P_X(\\x).\\label{eq:main:dependence:rnd2}\n    \\end{align}",
      "formula_type": "align",
      "line_number": 1605,
      "is_formula": true,
      "high_level_explanation": "The display first defines the squared L2(P_X) distance between the functions eta_rho and eta*, and then upper-bounds this error by two mean-squared conditional expectation terms. The two terms separate the contribution of the mismatch between g_rho and g_{P_{Y,Z}} and the contribution of the mismatch between g_{P_{Y,Z}} and f(x, Z), each taken as a conditional expectation over Z given X = x and then averaged over x. The integration is restricted to the subset X1 where the regular conditional distribution P_{Z|X = x} exists. The factors of 2 arise from a standard quadratic inequality and applications of Jensen's inequality.",
      "notations": {
        "\\eta_\\rho": "NOT MENTIONED",
        "\\etastar": "NOT MENTIONED",
        "\\ltwo(P_X)": "L2 norm with respect to the distribution P_X",
        "P_X": "Marginal distribution of X",
        "\\X": "Domain/support of X",
        "\\x": "A point in \\X (a realization of X)",
        "\\X_1": "Subset of \\X on which the regular conditional distribution P_{Z|X=\\x} is defined (per the r.c.d. assumption)",
        "P_{Z|X=\\x}": "Regular conditional distribution of Z given X = \\x",
        "g_\\rho(Z)": "NOT MENTIONED",
        "g_{P_{Y, Z}}(Z)": "NOT MENTIONED",
        "f(\\x, Z)": "NOT MENTIONED",
        "Z": "Random variable Z"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:56:59.540597"
    },
    {
      "label": "<<FORMULA_0826>>",
      "formula": "\\int_{\\X_1}\\p{\\E{P_{Z|X=\\x}}{\\p{g_{P_{Y, Z}}(Z) - f(\\x, Z)}}}^2\\d P_X(\\x) \n        &\\leq \\int_{\\X_1}\\p{\\E{P_{Z|X=\\x}}{\\p{g_{P_{Y, Z}}(Z) - f(\\x, Z)}^2}}\\d P_X(\\x) \\notag\\\\\n        &= \\E{P_{X, Z}}{\\p{g_{P_{Y, Z}}(Z) - f(X, Z)}^2} \\notag\\\\\n        &= \\int_{\\Z_1} \\E{P_{X|Z=\\z}}{\\p{g_{P_{Y, Z}}(\\z) - f(X, \\z)}^2} \\d P_Z(\\z),\\label{eq:main:dependence:rnd3}",
      "raw_latex": "\\begin{align}\n        \\int_{\\X_1}\\p{\\E{P_{Z|X=\\x}}{\\p{g_{P_{Y, Z}}(Z) - f(\\x, Z)}}}^2\\d P_X(\\x) \n        &\\leq \\int_{\\X_1}\\p{\\E{P_{Z|X=\\x}}{\\p{g_{P_{Y, Z}}(Z) - f(\\x, Z)}^2}}\\d P_X(\\x) \\notag\\\\\n        &= \\E{P_{X, Z}}{\\p{g_{P_{Y, Z}}(Z) - f(X, Z)}^2} \\notag\\\\\n        &= \\int_{\\Z_1} \\E{P_{X|Z=\\z}}{\\p{g_{P_{Y, Z}}(\\z) - f(X, \\z)}^2} \\d P_Z(\\z),\\label{eq:main:dependence:rnd3}\n    \\end{align}",
      "formula_type": "align",
      "line_number": 1618,
      "is_formula": true,
      "high_level_explanation": "The display applies Jensen's inequality to bound the square of a conditional mean by the conditional mean of the square under the regular conditional distribution of Z given X. It then uses the tower property/Fubini to rewrite the integral over P_X as a joint expectation over P_{X,Z}, and finally as an integral over P_Z of conditional expectations with respect to P_{X|Z=z}. This provides a controlled representation of the term via iterated conditioning that will be used in the proof.",
      "notations": {
        "\\X_1": "Set on which the regular conditional distribution P_{Z|X=\\x} is defined",
        "P_{Z|X=\\x}": "Regular conditional distribution of Z given X = x",
        "g_{P_{Y, Z}}": "NOT MENTIONED",
        "f": "NOT MENTIONED",
        "P_X": "Marginal distribution of X",
        "\\x": "A value of X (element of \\X_1)",
        "P_{X, Z}": "Joint distribution of (X, Z)",
        "X": "Random variable X",
        "Z": "Random variable Z",
        "\\Z_1": "Set on which the regular conditional distribution P_{X|Z=\\z} is defined",
        "P_{X|Z=\\z}": "Regular conditional distribution of X given Z = z",
        "\\z": "A value of Z (element of \\Z_1)",
        "P_Z": "Marginal distribution of Z",
        "P_{Y, Z}": "Joint distribution of (Y, Z)"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:56:31.084425"
    },
    {
      "label": "<<FORMULA_0831>>",
      "formula": "\\E{P_{X|Z=\\z}}{\\p{g_{P_{Y, Z}}(\\z) - f(X, \\z)}^2} &= \\E{P_{X|Z=\\z}}{\\p{\\E{P_{Y|Z = \\z}}{\\fstar(Y) (1 - \\Ssans_{\\z}(Y, X))}}^2}\\\\\n        &\\leq \\E{P_{X|Z=\\z}}{\\E{P_{Y|Z = \\z}}{\\p{\\fstar(Y) (1 - \\Ssans_{\\z}(Y, X))}^2}}\\\\\n        &\\leq \\norm{\\fstar}^2_\\infty \\E{P_{X|Z=\\z}}{\\E{P_{Y|Z = \\z}}{\\p{1 - \\Ssans_{\\z}(Y, X)}^2}}\\\\\n        &= \\norm{\\fstar}^2_\\infty \\E{P_{X|Z=\\z} \\otimes P_{Y|Z = \\z}}{\\p{1 - \\Ssans_{\\z}(Y, X)}^2},",
      "raw_latex": "\\begin{align*}\n        \\E{P_{X|Z=\\z}}{\\p{g_{P_{Y, Z}}(\\z) - f(X, \\z)}^2} &= \\E{P_{X|Z=\\z}}{\\p{\\E{P_{Y|Z = \\z}}{\\fstar(Y) (1 - \\Ssans_{\\z}(Y, X))}}^2}\\\\\n        &\\leq \\E{P_{X|Z=\\z}}{\\E{P_{Y|Z = \\z}}{\\p{\\fstar(Y) (1 - \\Ssans_{\\z}(Y, X))}^2}}\\\\\n        &\\leq \\norm{\\fstar}^2_\\infty \\E{P_{X|Z=\\z}}{\\E{P_{Y|Z = \\z}}{\\p{1 - \\Ssans_{\\z}(Y, X)}^2}}\\\\\n        &= \\norm{\\fstar}^2_\\infty \\E{P_{X|Z=\\z} \\otimes P_{Y|Z = \\z}}{\\p{1 - \\Ssans_{\\z}(Y, X)}^2},\n    \\end{align*}",
      "formula_type": "align*",
      "line_number": 1629,
      "is_formula": true,
      "high_level_explanation": "The display bounds the conditional mean-squared difference between g_{P_{Y,Z}}(z) and f(X, z) given Z = z. It first rewrites this difference as a conditional expectation over Y|Z = z, then applies Jensen’s inequality to move the square inside the expectation. Using the boundedness of f*, the bound factors out the sup-norm of f*, leaving a term that depends on the Radon–Nikodym derivative S_z. Finally, Fubini’s theorem identifies the remaining expectation with respect to the product measure P_{X|Z=z} ⊗ P_{Y|Z=z}.",
      "notations": {
        "P_{X|Z=\\z}": "Regular conditional distribution (r.c.d.) of X given Z = z",
        "P_{Y|Z = \\z}": "Regular conditional distribution (r.c.d.) of Y given Z = z",
        "P_{X|Z=\\z} \\otimes P_{Y|Z = \\z}": "Product measure of the conditional marginals P_{X|Z=z} and P_{Y|Z=z}",
        "g_{P_{Y, Z}}(\\z)": "NOT MENTIONED",
        "f(X, \\z)": "NOT MENTIONED",
        "\\Ssans_{\\z}(Y, X)": "Radon–Nikodym derivative associated with the conditional distributions at Z = z (introduced in the referenced equation)",
        "\\fstar": "A function assumed bounded in absolute value (per the theorem’s assumption)",
        "\\norm{\\fstar}^2_\\infty": "Square of the supremum norm of f*, finite due to the boundedness assumption",
        "P_{Y, Z}": "Joint distribution of (Y, Z)",
        "X": "Random variable X",
        "Y": "Random variable Y",
        "Z": "Random variable Z",
        "\\z": "A fixed value of Z for which the r.c.d.s are defined"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:56:12.949974"
    },
    {
      "label": "<<FORMULA_0883>>",
      "formula": "&\\norm{\\heta_\\rho - \\eta_\\rho}_{\\ltwo(Q_X)}^2 \\notag \\\\\n        &= \\int_\\X (\\heta_\\rho(\\x) - \\eta_\\rho(\\x))^2 \\d Q_X(\\x) \\notag\\\\\n        &\\leq 3\\int_\\X \\ipsmall{g_\\rho, \\Fhat(\\x) - F_\\star(\\x)}_\\calG^2 \\d Q_X(\\x) + 3\\int_\\X \\ipsmall{\\Fhat(\\x), \\hg_\\rho - g_\\rho}_\\calG^2 \\d Q_X(\\x)\\notag\\\\\n        &\\quad + 3\\int_\\X \\ipsmall{\\Fhat(\\x) - F_\\star(\\x), \\hg_\\rho - g_\\rho}_\\calG^2 \\d Q_X(\\x).\\notag",
      "raw_latex": "\\begin{align}\n        &\\norm{\\heta_\\rho - \\eta_\\rho}_{\\ltwo(Q_X)}^2 \\notag \\\\\n        &= \\int_\\X (\\heta_\\rho(\\x) - \\eta_\\rho(\\x))^2 \\d Q_X(\\x) \\notag\\\\\n        &\\leq 3\\int_\\X \\ipsmall{g_\\rho, \\Fhat(\\x) - F_\\star(\\x)}_\\calG^2 \\d Q_X(\\x) + 3\\int_\\X \\ipsmall{\\Fhat(\\x), \\hg_\\rho - g_\\rho}_\\calG^2 \\d Q_X(\\x)\\notag\\\\\n        &\\quad + 3\\int_\\X \\ipsmall{\\Fhat(\\x) - F_\\star(\\x), \\hg_\\rho - g_\\rho}_\\calG^2 \\d Q_X(\\x).\\notag\n    \\end{align}",
      "formula_type": "align",
      "line_number": 1745,
      "is_formula": true,
      "high_level_explanation": "The display rewrites the squared L2(Q_X) error between the estimated predictor and the target predictor as an integral of the pointwise squared difference, then upper-bounds it by a sum of three integrals of squared RKHS inner products. The three terms isolate the contributions from estimation error in the vector-valued function F_* (via \\Fhat - F_\\star), approximation error in g_\\rho (via \\hg_\\rho - g_\\rho), and their interaction. This is the core error decomposition step used before applying Cauchy–Schwarz to obtain norm-based bounds.",
      "notations": {
        "\\heta_\\rho": "estimated predictor",
        "\\eta_\\rho": "target predictor",
        "\\ltwo(Q_X)": "space of square-integrable functions with respect to Q_X",
        "Q_X": "NOT MENTIONED",
        "\\X": "NOT MENTIONED",
        "\\x": "NOT MENTIONED",
        "\\Fhat": "estimator of F_\\star",
        "F_\\star": "target vector-valued function",
        "g_\\rho": "element of the RKHS \\calG (depends on \\rho)",
        "\\hg_\\rho": "approximation of g_\\rho",
        "\\calG": "reproducing kernel Hilbert space",
        "\\rho": "NOT MENTIONED"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:56:25.075024"
    },
    {
      "label": "<<FORMULA_0884>>",
      "formula": "&\\norm{\\heta_\\rho - \\eta_\\rho}_{\\ltwo(Q_X)}^2\\notag\\\\\n        &\\leq 3\\norm{g_\\rho}_\\calG^2 \\cdot \\int_\\X \\norm{ \\Fhat(\\x) - F_\\star(\\x)}_\\calG^2 \\d Q_X(\\x) + 3\\p{\\int_{\\X}\\norm{F_\\star(\\x)}_\\calG^2 \\d Q_X(\\x)} \\cdot \\norm{\\hg_\\rho - g_\\rho}_\\calG^2\\notag\\\\\n        &\\quad + 3\\norm{\\hg_\\rho - g_\\rho}_\\calG^2 \\int_\\X \\norm{\\Fhat(\\x) - F_\\star(\\x)}_\\calG^2 \\d Q_X(\\x)\\notag\\\\\n        &= 3\\norm{g_\\rho}_\\calG^2 \\cdot \\norm{\\Fhat - F_\\star}^2_{\\ltwo(Q_X; \\calG)} + 3 \\norm{F_\\star}^2_{\\ltwo(Q_X; \\calG)} \\cdot  \\norm{\\hg_\\rho - g_\\rho}_\\calG^2  \\label{eq:main:est:F_error}\\\\\n        &\\quad - 3\\norm{\\hg_\\rho - g_\\rho}_\\calG^2 \\cdot \\norm{\\Fhat - F_\\star}^2_{\\ltwo(Q_X; \\calG)},\\notag",
      "raw_latex": "\\begin{align}\n    &\\norm{\\heta_\\rho - \\eta_\\rho}_{\\ltwo(Q_X)}^2\\notag\\\\\n        &\\leq 3\\norm{g_\\rho}_\\calG^2 \\cdot \\int_\\X \\norm{ \\Fhat(\\x) - F_\\star(\\x)}_\\calG^2 \\d Q_X(\\x) + 3\\p{\\int_{\\X}\\norm{F_\\star(\\x)}_\\calG^2 \\d Q_X(\\x)} \\cdot \\norm{\\hg_\\rho - g_\\rho}_\\calG^2\\notag\\\\\n        &\\quad + 3\\norm{\\hg_\\rho - g_\\rho}_\\calG^2 \\int_\\X \\norm{\\Fhat(\\x) - F_\\star(\\x)}_\\calG^2 \\d Q_X(\\x)\\notag\\\\\n        &= 3\\norm{g_\\rho}_\\calG^2 \\cdot \\norm{\\Fhat - F_\\star}^2_{\\ltwo(Q_X; \\calG)} + 3 \\norm{F_\\star}^2_{\\ltwo(Q_X; \\calG)} \\cdot  \\norm{\\hg_\\rho - g_\\rho}_\\calG^2  \\label{eq:main:est:F_error}\\\\\n        &\\quad - 3\\norm{\\hg_\\rho - g_\\rho}_\\calG^2 \\cdot \\norm{\\Fhat - F_\\star}^2_{\\ltwo(Q_X; \\calG)},\\notag\n    \\end{align}",
      "formula_type": "align",
      "line_number": 1752,
      "is_formula": true,
      "high_level_explanation": "This inequality decomposes the squared L2(Q_X) error between the estimated predictor and the target predictor into terms that depend on two estimation errors: the discrepancy between the vector-valued function estimator F̂ and its target F*, and the discrepancy between ĝ_ρ and g_ρ in the RKHS G. The first two terms on the right-hand side dominate and scale with ∥g_ρ∥_G^2 and ∥F_*∥_{L^2(Q_X; G)}^2, respectively, while the last term is a higher-order product of the two errors. The expression is obtained by applying the reproducing property, Cauchy–Schwarz, and Young’s inequality to separate contributions from learning F and g. It shows how the overall prediction error can be controlled by the accuracy of these component estimators.",
      "notations": {
        "\\heta_\\rho": "estimated predictor (estimator of \\eta_\\rho)",
        "\\eta_\\rho": "target predictor being estimated",
        "Q_X": "measure on \\X used for integration and L2 norms",
        "\\calG": "reproducing kernel Hilbert space (RKHS)",
        "\\Fhat": "estimator of a vector-valued function (constructed via kernel regularized learning)",
        "F_\\star": "vector-valued function of interest (target)",
        "\\hg_\\rho": "estimator of g_\\rho (learned via kernel regularized learning)",
        "g_\\rho": "element of \\calG that depends on \\rho",
        "\\ltwo(Q_X)": "space of square-integrable functions with respect to Q_X",
        "\\ltwo(Q_X; \\calG)": "space of square-integrable \\calG-valued functions with respect to Q_X",
        "\\X": "measurable space over which Q_X is defined",
        "\\x": "a point in \\X",
        "\\rho": "regularization parameter"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:54:54.682466"
    },
    {
      "label": "<<FORMULA_0898>>",
      "formula": "&\\norm{F_\\star}_\\beta^2 \\\\\n        &= \\norm{\\C_\\star \\Top_X^{-\\beta/2}}_{\\HS(\\Cl(\\Range(\\Iop_X)), \\calG)}^2 \\\\\n        &= \\sum_{i \\in I} \\sum_{j \\in J} \\ipsmall{g_j, \\C_\\star \\Top_X^{-\\beta/2} [e_{X, i}]_X}_\\calG^2 &\\text{(by definition)}\\\\\n        &= \\sum_{i \\in I} \\sum_{j \\in J} \\mu_{X, i}^{-\\beta}\\ip{g_j, \\C_\\star [e_{X, i}]_X}_\\calG^2  &\\text{(by~\\eqref{eq:main:eigendecomposition})}\\\\\n        &= \\sum_{i \\in I} \\sum_{j \\in J} \\mu_{X, i}^{-\\beta}\\ip{\\C_\\star, g_j \\otimes [e_{X, i}]_X}_{\\HS(\\ltwo(Q_X), \\calG)}^2\\\\\n        &= \\sum_{i \\in I} \\sum_{j \\in J} \\sum_{k \\in J} \\sum_{l \\in J} \\mu_{X, i}^{-\\beta}\\ip{g_k \\otimes [f_k]_X, g_j \\otimes [e_{X, i}]_X}_{\\HS(\\ltwo(Q_X), \\calG)} \\cdot \\ip{g_l \\otimes [f_l]_X, g_j \\otimes [e_{X, i}]_X}_{\\HS(\\ltwo(Q_X), \\calG)}, & \\text{(\\Cref{lem:main:estimation:technical})}",
      "raw_latex": "\\begin{align*}\n        &\\norm{F_\\star}_\\beta^2 \\\\\n        &= \\norm{\\C_\\star \\Top_X^{-\\beta/2}}_{\\HS(\\Cl(\\Range(\\Iop_X)), \\calG)}^2 \\\\\n        &= \\sum_{i \\in I} \\sum_{j \\in J} \\ipsmall{g_j, \\C_\\star \\Top_X^{-\\beta/2} [e_{X, i}]_X}_\\calG^2 &\\text{(by definition)}\\\\\n        &= \\sum_{i \\in I} \\sum_{j \\in J} \\mu_{X, i}^{-\\beta}\\ip{g_j, \\C_\\star [e_{X, i}]_X}_\\calG^2  &\\text{(by~\\eqref{eq:main:eigendecomposition})}\\\\\n        &= \\sum_{i \\in I} \\sum_{j \\in J} \\mu_{X, i}^{-\\beta}\\ip{\\C_\\star, g_j \\otimes [e_{X, i}]_X}_{\\HS(\\ltwo(Q_X), \\calG)}^2\\\\\n        &= \\sum_{i \\in I} \\sum_{j \\in J} \\sum_{k \\in J} \\sum_{l \\in J} \\mu_{X, i}^{-\\beta}\\ip{g_k \\otimes [f_k]_X, g_j \\otimes [e_{X, i}]_X}_{\\HS(\\ltwo(Q_X), \\calG)} \\cdot \\ip{g_l \\otimes [f_l]_X, g_j \\otimes [e_{X, i}]_X}_{\\HS(\\ltwo(Q_X), \\calG)}, & \\text{(\\Cref{lem:main:estimation:technical})}\n    \\end{align*}",
      "formula_type": "align*",
      "line_number": 1785,
      "is_formula": true,
      "high_level_explanation": "The display computes the beta-order quantity associated with F_* by expressing it as the Hilbert–Schmidt norm of the operator C_* composed with the fractional inverse power of T_X. It then expands this norm using an orthonormal basis of the RKHS G and the eigenbasis of T_X on L^2(Q_X), turning the fractional power into eigenvalue weights mu_{X,i}^{-beta}. The Hilbert–Schmidt inner product is written via rank-one tensors g_j ⊗ [e_{X,i}]_X, and a technical lemma further expands C_* in a rank-one basis {g_k ⊗ [f_k]_X}, yielding the final quadruple sum.",
      "notations": {
        "F_\\star": "NOT MENTIONED",
        "\\beta": "NOT MENTIONED",
        "\\C_\\star": "NOT MENTIONED",
        "\\Top_X": "NOT MENTIONED",
        "\\HS(\\Cl(\\Range(\\Iop_X)), \\calG)": "Space of Hilbert–Schmidt operators from \\Cl(\\Range(\\Iop_X)) to \\calG (also used as the corresponding norm subscript)",
        "\\Cl(\\Range(\\Iop_X))": "Closure of the range of the embedding operator \\Iop_X",
        "\\Iop_X": "Embedding operator (precise definition NOT MENTIONED)",
        "\\calG": "Reproducing kernel Hilbert space (RKHS) used in the analysis",
        "I": "Index set for the eigenfunctions [e_{X, i}]_X",
        "J": "Index set for an orthonormal basis (g_j) of \\calG",
        "g_j": "j-th element of an orthonormal basis of \\calG",
        "[e_{X, i}]_X": "i-th eigenfunction in \\ltwo(Q_X) associated with \\Top_X",
        "\\mu_{X, i}": "i-th eigenvalue of \\Top_X",
        "\\HS(\\ltwo(Q_X), \\calG)": "Space of Hilbert–Schmidt operators from \\ltwo(Q_X) to \\calG",
        "\\ltwo(Q_X)": "Space of square-integrable functions with respect to Q_X",
        "g_j \\otimes [e_{X, i}]_X": "Rank-one (tensor) operator from \\ltwo(Q_X) to \\calG built from g_j and [e_{X, i}]_X",
        "[f_k]_X": "NOT MENTIONED",
        "g_k \\otimes [f_k]_X": "Rank-one (tensor) operator from \\ltwo(Q_X) to \\calG built from g_k and [f_k]_X"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:55:26.486261"
    },
    {
      "label": "<<FORMULA_0902>>",
      "formula": "&\\norm{F_\\star}_\\beta^2 \\\\\n    &= \\sum_{i \\in I} \\sum_{j \\in J} \\sum_{k \\in J} \\sum_{l \\in J} \\mu_{X, i}^{-\\beta}\\ip{\\blue{g_k} \\otimes \\red{(\\M_{Z|X} [g_k]_Z)}, \\blue{g_j} \\otimes \\red{[e_{X, i}]_X}}_{\\HS(\\ltwo(Q_X), \\calG)} \\ip{\\blue{g_l} \\otimes \\red{(\\M_{Z|X} [g_l]_Z)}, \\blue{g_j} \\otimes \\red{[e_{X, i}]_X}}_{\\HS(\\ltwo(Q_X), \\calG)}\\\\\n        &= \\sum_{i \\in I} \\sum_{j \\in J} \\sum_{k \\in J} \\sum_{l \\in J} \\mu_{X, i}^{ + \\beta}\\blue{\\ip{g_k, g_j}_{\\calG}} \\blue{\\ip{g_l, g_j}_{\\calG}} \\cdot \\red{\\ip{\\M_{Z|X} [g_k]_Z, [e_{X, i}]_X}_{\\ltwo(Q_X)}}\\red{\\ip{\\M_{Z|X} [g_l]_Z, [e_{X, i}]_X}_{\\ltwo(Q_X)}}\\\\\n        &= \\sum_{i \\in I} \\sum_{j \\in J} \\mu_{X, i}^{-\\beta} \\ip{\\M_{Z|X} [g_j]_Z, [e_{X, i}]_X}_{\\ltwo(Q_X)}^2,",
      "raw_latex": "\\begin{align*}\n    &\\norm{F_\\star}_\\beta^2 \\\\\n    &= \\sum_{i \\in I} \\sum_{j \\in J} \\sum_{k \\in J} \\sum_{l \\in J} \\mu_{X, i}^{-\\beta}\\ip{\\blue{g_k} \\otimes \\red{(\\M_{Z|X} [g_k]_Z)}, \\blue{g_j} \\otimes \\red{[e_{X, i}]_X}}_{\\HS(\\ltwo(Q_X), \\calG)} \\ip{\\blue{g_l} \\otimes \\red{(\\M_{Z|X} [g_l]_Z)}, \\blue{g_j} \\otimes \\red{[e_{X, i}]_X}}_{\\HS(\\ltwo(Q_X), \\calG)}\\\\\n        &= \\sum_{i \\in I} \\sum_{j \\in J} \\sum_{k \\in J} \\sum_{l \\in J} \\mu_{X, i}^{ + \\beta}\\blue{\\ip{g_k, g_j}_{\\calG}} \\blue{\\ip{g_l, g_j}_{\\calG}} \\cdot \\red{\\ip{\\M_{Z|X} [g_k]_Z, [e_{X, i}]_X}_{\\ltwo(Q_X)}}\\red{\\ip{\\M_{Z|X} [g_l]_Z, [e_{X, i}]_X}_{\\ltwo(Q_X)}}\\\\\n        &= \\sum_{i \\in I} \\sum_{j \\in J} \\mu_{X, i}^{-\\beta} \\ip{\\M_{Z|X} [g_j]_Z, [e_{X, i}]_X}_{\\ltwo(Q_X)}^2,\n    \\end{align*}",
      "formula_type": "align*",
      "line_number": 1798,
      "is_formula": true,
      "high_level_explanation": "This identity expresses the beta-source norm of F_* as a spectral sum over the X-side eigenfunctions and a basis of the RKHS G. By expanding the Hilbert–Schmidt inner products and using orthonormality of the basis (g_j) in G, the fourfold sum collapses to a double sum weighted by the eigenvalues associated with [e_{X,i}]_X raised to the power −beta. Each surviving term quantifies the L2(Q_X) correlation between the conditional mean operator applied to the embedded basis element [g_j]_Z and the eigenfunction [e_{X,i}]_X.",
      "notations": {
        "\\norm{F_\\star}_\\beta^2": "Square of the beta-source norm (defined via the Hilbert–Schmidt norm of C_* T_X^{-beta/2} in the lemma’s proof).",
        "F_\\star": "NOT MENTIONED",
        "\\beta": "Source-condition (regularity) exponent appearing in the assumption referenced by the paper.",
        "I": "Index set for the X-side eigenfunctions used in the eigendecomposition.",
        "J": "Index set for the orthonormal basis (g_j) of the RKHS \\calG.",
        "g_j": "j-th element of the chosen orthonormal basis of \\calG.",
        "g_k": "k-th element of the chosen orthonormal basis of \\calG.",
        "g_l": "l-th element of the chosen orthonormal basis of \\calG.",
        "\\mu_{X, i}": "i-th eigenvalue associated with the eigenfunction [e_{X, i}]_X in the X-side eigendecomposition.",
        "[e_{X, i}]_X": "i-th eigenfunction in L2(Q_X) arising from the X-side eigendecomposition.",
        "\\M_{Z|X}": "Conditional mean operator mapping functions of Z to functions of X via conditional expectation.",
        "[g_k]_Z": "Image of g_k under the embedding into L2(Q_Z).",
        "[g_j]_Z": "Image of g_j under the embedding into L2(Q_Z).",
        "\\ltwo(Q_X)": "Space of square-integrable functions on X with respect to the distribution Q_X.",
        "Q_X": "Marginal distribution of X.",
        "\\calG": "Reproducing kernel Hilbert space (RKHS) serving as the output space.",
        "\\HS(\\ltwo(Q_X), \\calG)": "Hilbert–Schmidt space of operators from L2(Q_X) to \\calG; its inner product is used in the formula."
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:55:51.578073"
    },
    {
      "label": "<<FORMULA_0963>>",
      "formula": "\\norm{F_\\star}_\\beta^2 &= \\norm{\\Top_X^{-\\beta/2}\\M_{Z|X} \\Top_Z^{1/2}}_{\\HS(\\ltwo(Q_Z), \\ltwo(Q_X))}^2\\\\\n        &\\geq \\frac{c}{C^{\\beta}} \\sum_{i=1}^\\infty \\sum_{j=1}^\\infty j^{-\\gamma_Z} i^{\\beta \\gamma_X} \\ip{\\M_{Z|X}[e_{Z, j}]_Z, [e_{X, i}]_X}_{\\ltwo(Q_X)}^2 &\\text{(\\Cref{asm:eigendecay})}\\\\\n        &= \\frac{c}{C^{\\beta}} \\sum_{i=1}^\\infty \\sum_{j=1}^\\infty \\sum_{k=1}^\\infty \\sum_{l=1}^\\infty j^{-\\gamma_Z} i^{\\beta \\gamma_X} \\sigma_{\\pi(k)}^2\\ip{[e_{Z, k}]_Z, [e_{Z, j}]_Z}_{\\ltwo(Q_Z)}\\ip{[e_{X, k}]_X, [e_{X, i}]_X}_{\\ltwo(Q_X)} \\\\\n        &\\quad \\times \\ip{[e_{Z, l}]_Z, [e_{Z, j}]_Z}_{\\ltwo(Q_Z)}\\ip{[e_{X, l}]_X, [e_{X, i}]_X}_{\\ltwo(Q_X)} \\notag\\\\\n        &= \\frac{c}{C^{\\beta}} \\sbr{\\sum_{i=1}^m  i^{\\beta \\gamma_X-\\gamma_Z} \\sigma_{\\pi(i)}^2 +  \\sum_{i=m+1}^\\infty i^{\\beta \\gamma_X -\\gamma_Z}\\sigma_{i}^2}&\\text{(\\Cref{asm:alignment})}\\\\\n        &\\geq \\frac{c}{C^{\\beta}} \\sbr{\\sum_{i=1}^m  i^{\\beta \\gamma_X-\\gamma_Z} \\sigma_{\\pi(i)}^2 +  c\\sum_{i=m+1}^\\infty i^{\\beta \\gamma_X -\\gamma_Z - 2\\gamma_{X, Z}}},&\\text{(\\Cref{asm:eigendecay})}",
      "raw_latex": "\\begin{align*}\n        \\norm{F_\\star}_\\beta^2 &= \\norm{\\Top_X^{-\\beta/2}\\M_{Z|X} \\Top_Z^{1/2}}_{\\HS(\\ltwo(Q_Z), \\ltwo(Q_X))}^2\\\\\n        &\\geq \\frac{c}{C^{\\beta}} \\sum_{i=1}^\\infty \\sum_{j=1}^\\infty j^{-\\gamma_Z} i^{\\beta \\gamma_X} \\ip{\\M_{Z|X}[e_{Z, j}]_Z, [e_{X, i}]_X}_{\\ltwo(Q_X)}^2 &\\text{(\\Cref{asm:eigendecay})}\\\\\n        &= \\frac{c}{C^{\\beta}} \\sum_{i=1}^\\infty \\sum_{j=1}^\\infty \\sum_{k=1}^\\infty \\sum_{l=1}^\\infty j^{-\\gamma_Z} i^{\\beta \\gamma_X} \\sigma_{\\pi(k)}^2\\ip{[e_{Z, k}]_Z, [e_{Z, j}]_Z}_{\\ltwo(Q_Z)}\\ip{[e_{X, k}]_X, [e_{X, i}]_X}_{\\ltwo(Q_X)} \\\\\n        &\\quad \\times \\ip{[e_{Z, l}]_Z, [e_{Z, j}]_Z}_{\\ltwo(Q_Z)}\\ip{[e_{X, l}]_X, [e_{X, i}]_X}_{\\ltwo(Q_X)} \\notag\\\\\n        &= \\frac{c}{C^{\\beta}} \\sbr{\\sum_{i=1}^m  i^{\\beta \\gamma_X-\\gamma_Z} \\sigma_{\\pi(i)}^2 +  \\sum_{i=m+1}^\\infty i^{\\beta \\gamma_X -\\gamma_Z}\\sigma_{i}^2}&\\text{(\\Cref{asm:alignment})}\\\\\n        &\\geq \\frac{c}{C^{\\beta}} \\sbr{\\sum_{i=1}^m  i^{\\beta \\gamma_X-\\gamma_Z} \\sigma_{\\pi(i)}^2 +  c\\sum_{i=m+1}^\\infty i^{\\beta \\gamma_X -\\gamma_Z - 2\\gamma_{X, Z}}},&\\text{(\\Cref{asm:eigendecay})}\n    \\end{align*}",
      "formula_type": "align*",
      "line_number": 1880,
      "is_formula": true,
      "high_level_explanation": "The chain of equalities and inequalities rewrites the beta-weighted norm of F_* as the Hilbert–Schmidt norm of a spectrally pre- and post-conditioned operator, and then lower-bounds it using eigenvalue decay of T_X and T_Z together with singular value decay of M_{Z|X}. Under the basis alignment assumption, the bound simplifies to sums over aligned indices, separating the first m aligned components from the tail. A further use of the decay assumptions yields a polynomial lower bound on the tail in terms of the decay exponents. This quantifies when the series (and thus the norm) is finite and how it depends on spectral decay rates and the source parameter beta.",
      "notations": {
        "\\norm{F_\\star}_\\beta": "Beta-weighted norm of F_* defined via the displayed Hilbert–Schmidt expression",
        "F_\\star": "NOT MENTIONED",
        "\\Top_X": "Operator with eigenvalue decay governed by Assumption Eigendecay (acts on L2(Q_X))",
        "\\Top_Z": "Operator with eigenvalue decay governed by Assumption Eigendecay (acts on L2(Q_Z))",
        "\\Top_X^{-\\beta/2}": "Fractional power of \\Top_X used in the source condition",
        "\\Top_Z^{1/2}": "Square root of the operator \\Top_Z",
        "\\M_{Z|X}": "Linear operator between L2(Q_Z) and L2(Q_X) with singular value decay assumed in Assumption Eigendecay",
        "\\HS(\\ltwo(Q_Z), \\ltwo(Q_X))": "Space of Hilbert–Schmidt operators from L2(Q_Z) to L2(Q_X); used to indicate the corresponding HS norm",
        "\\ltwo(Q_X)": "L2 space with respect to the measure Q_X",
        "\\ltwo(Q_Z)": "L2 space with respect to the measure Q_Z",
        "Q_X": "Measure underlying the space L2(Q_X)",
        "Q_Z": "Measure underlying the space L2(Q_Z)",
        "c": "Positive constant from Assumption Eigendecay",
        "C": "Positive constant from Assumption Eigendecay",
        "\\gamma_X": "Eigenvalue decay exponent for \\Top_X (Assumption Eigendecay)",
        "\\gamma_Z": "Eigenvalue decay exponent for \\Top_Z (Assumption Eigendecay)",
        "\\gamma_{X, Z}": "Decay exponent associated with \\M_{Z|X} (Assumption Eigendecay)",
        "[e_{X, i}]_X": "i-th element of the eigenbasis of \\Top_X in L2(Q_X)",
        "[e_{Z, j}]_Z": "j-th element of the eigenbasis of \\Top_Z in L2(Q_Z)",
        "\\sigma_{\\pi(i)}": "i-th singular value of \\M_{Z|X} after applying the permutation \\pi (Basis Alignment assumption)",
        "\\sigma_i": "i-th singular value of \\M_{Z|X}",
        "\\pi": "Permutation of indices from the Basis Alignment assumption",
        "m": "Finite index from the Basis Alignment assumption separating aligned components",
        "\\beta": "Source condition parameter controlling the strength of spectral preconditioning"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:55:12.905675"
    },
    {
      "label": "<<FORMULA_1021>>",
      "formula": "\\heta_\\rho(\\x) - \\eta_\\rho(\\x)  &= \\Ex_{\\hrho_{Y, Z}}[\\fstar(Y) \\Rhat(\\x, Z)] - \\E{\\rho_{Y, Z}}{\\fstar(Y) \\Rsans(\\x, Z)}\\\\\n        &\\quad + \\int_{\\Z} g_\\rho(\\z)\\Rsans(\\x, \\z) \\p{\\d Q_Z(\\z) - \\d \\rho_Z(\\z)}\\\\\n        &= \\Ex_{\\hrho_{Y, Z}}[\\fstar(Y) \\ipsmall{\\varphi(\\x, Z), \\Rhat - \\Rsans}] \\\\\n        &\\quad +  \\int_{\\Y \\times \\Z} \\fstar(\\y) \\Rsans(\\x, \\z) \\p{\\d\\hrho_{Y, Z}(\\y, \\z) - \\d\\rho_{Y, Z}(\\y, \\z)}\\\\\n        &\\quad + \\int_{\\Z} g_\\rho(\\z)\\Rsans(\\x, \\z) \\p{\\d Q_Z(\\z) - \\d \\rho_Z(\\z)}.",
      "raw_latex": "\\begin{align*}\n        \\heta_\\rho(\\x) - \\eta_\\rho(\\x)  &= \\Ex_{\\hrho_{Y, Z}}[\\fstar(Y) \\Rhat(\\x, Z)] - \\E{\\rho_{Y, Z}}{\\fstar(Y) \\Rsans(\\x, Z)}\\\\\n        &\\quad + \\int_{\\Z} g_\\rho(\\z)\\Rsans(\\x, \\z) \\p{\\d Q_Z(\\z) - \\d \\rho_Z(\\z)}\\\\\n        &= \\Ex_{\\hrho_{Y, Z}}[\\fstar(Y) \\ipsmall{\\varphi(\\x, Z), \\Rhat - \\Rsans}] \\\\\n        &\\quad +  \\int_{\\Y \\times \\Z} \\fstar(\\y) \\Rsans(\\x, \\z) \\p{\\d\\hrho_{Y, Z}(\\y, \\z) - \\d\\rho_{Y, Z}(\\y, \\z)}\\\\\n        &\\quad + \\int_{\\Z} g_\\rho(\\z)\\Rsans(\\x, \\z) \\p{\\d Q_Z(\\z) - \\d \\rho_Z(\\z)}.\n    \\end{align*}",
      "formula_type": "align*",
      "line_number": 1978,
      "is_formula": true,
      "high_level_explanation": "The display decomposes the pointwise difference between two functions, heta_rho(x) and eta_rho(x), into three contributions. The first term captures the estimation error of the Radon–Nikodym derivative (Rhat versus Rsans) projected through the RKHS feature map. The second term measures discrepancy between the approximate joint distribution hrho_{Y,Z} and the true joint rho_{Y,Z}. The third term quantifies the mismatch between the prompting marginal rho_Z and the pre-training marginal Q_Z, weighted by g_rho and the true kernel Rsans.",
      "notations": {
        "\\heta_\\rho(\\x)": "NOT MENTIONED",
        "\\eta_\\rho(\\x)": "NOT MENTIONED",
        "\\hrho_{Y, Z}": "Approximate joint distribution over (Y, Z) expressed directly in terms of the prompt distribution",
        "\\rho_{Y, Z}": "True joint distribution over (Y, Z)",
        "\\fstar(Y)": "NOT MENTIONED",
        "\\fstar(\\y)": "NOT MENTIONED",
        "\\Rhat": "Estimated Radon–Nikodym derivative",
        "\\Rsans": "True Radon–Nikodym derivative; acts as a kernel for the conditional mean operator when integrated under Q_Z",
        "\\Rhat(\\x, Z)": "Evaluation of the estimated Radon–Nikodym derivative at (x, Z)",
        "\\Rsans(\\x, Z)": "Evaluation of the true Radon–Nikodym derivative at (x, Z)",
        "g_\\rho(\\z)": "NOT MENTIONED",
        "Q_Z": "Pre-training marginal distribution over Z",
        "\\rho_Z": "Prompting marginal distribution over Z",
        "\\varphi(\\x, Z)": "Canonical RKHS feature map evaluated at (x, Z)",
        "\\ipsmall{\\varphi(\\x, Z), \\Rhat - \\Rsans}": "RKHS inner product between the feature map and the estimation error (Rhat − Rsans)"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:54:58.797178"
    },
    {
      "label": "<<FORMULA_1022>>",
      "formula": "\\norm{\\heta_\\rho - \\eta_\\rho}_{\\ltwo(Q_X)}^2 &\\\\geq 3 \\E{Q_X}{\\p{\\Ex_{\\hrho_{Y, Z}}[\\fstar(Y) \\ipsmall{\\varphi(X, Z), \\Rhat - \\Rsans}]^2}} \\label{eq:complexity:rn_decomp1}\\\\\n        &\\quad + 3\\int_{\\X} \\p{\\int_{\\Y \\times \\Z} \\fstar(\\y) \\Rsans(\\x, \\z) \\p{\\d\\hrho_{Y, Z}(\\y, \\z) - \\d\\rho_{Y, Z}(\\y, \\z)}}^2 \\d Q_X(\\x)  \\label{eq:complexity:rn_decomp2}\\\\\n        &\\quad + 3\\int_{\\X} \\p{\\int_{\\Z} g_\\rho(\\z)\\Rsans(\\x, \\z) \\p{\\d Q_Z(\\z) - \\d \\rho_Z(\\z)}}^2 \\d Q_X(\\x).  \\label{eq:complexity:rn_decomp3}",
      "raw_latex": "\\begin{align}\n        \\norm{\\heta_\\rho - \\eta_\\rho}_{\\ltwo(Q_X)}^2 &\\\\geq 3 \\E{Q_X}{\\p{\\Ex_{\\hrho_{Y, Z}}[\\fstar(Y) \\ipsmall{\\varphi(X, Z), \\Rhat - \\Rsans}]^2}} \\label{eq:complexity:rn_decomp1}\\\\\n        &\\quad + 3\\int_{\\X} \\p{\\int_{\\Y \\times \\Z} \\fstar(\\y) \\Rsans(\\x, \\z) \\p{\\d\\hrho_{Y, Z}(\\y, \\z) - \\d\\rho_{Y, Z}(\\y, \\z)}}^2 \\d Q_X(\\x)  \\label{eq:complexity:rn_decomp2}\\\\\n        &\\quad + 3\\int_{\\X} \\p{\\int_{\\Z} g_\\rho(\\z)\\Rsans(\\x, \\z) \\p{\\d Q_Z(\\z) - \\d \\rho_Z(\\z)}}^2 \\d Q_X(\\x).  \\label{eq:complexity:rn_decomp3}\n    \\end{align}",
      "formula_type": "align",
      "line_number": 1986,
      "is_formula": true,
      "high_level_explanation": "The expression relates the squared L2(Q_X) error between two functions, heta_rho and eta_rho, to three additive contributions. These contributions correspond to: (i) the error from using an estimated Radon–Nikodym/kernel quantity Rhat instead of the true Rsans when integrating a feature-map-based test function under the approximate joint distribution hrho_{Y,Z}; (ii) the discrepancy between the approximate joint hrho_{Y,Z} and the true joint rho_{Y,Z}; and (iii) the marginal shift between Q_Z (pre-training) and rho_Z (prompt/test-time) weighted by g_rho and the kernel Rsans. Together, they decompose the global error into estimator error and two distribution-mismatch components.",
      "notations": {
        "\\heta_\\rho": "NOT MENTIONED",
        "\\eta_\\rho": "NOT MENTIONED",
        "\\ltwo(Q_X)": "Square-integrable function space with respect to the measure Q_X",
        "Q_X": "NOT MENTIONED",
        "\\hrho_{Y, Z}": "Approximate joint distribution of (Y, Z) constructed from the prompt distribution",
        "\\rho_{Y, Z}": "True joint distribution of (Y, Z)",
        "\\fstar": "NOT MENTIONED",
        "\\varphi": "Canonical feature map of the RKHS",
        "\\Rhat": "Estimator of the Radon–Nikodym/kernel quantity used in the conditional mean operator",
        "\\Rsans": "True kernel associated with the conditional mean operator (Radon–Nikodym-based)",
        "\\X": "NOT MENTIONED",
        "\\Y": "NOT MENTIONED",
        "\\Z": "NOT MENTIONED",
        "g_\\rho": "NOT MENTIONED",
        "Q_Z": "Pre-training marginal distribution of Z",
        "\\rho_Z": "Prompt (test-time) marginal distribution of Z"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:55:18.841674"
    },
    {
      "label": "<<FORMULA_1029>>",
      "formula": "&\\p{\\int_{\\Z} g_\\rho(\\z)\\Rsans(\\x, \\z) \\p{\\d Q_Z(\\z) - \\d \\rho_Z(\\z)}}^2 \\\\\n        &= \\p{\\int_{\\Z} g_\\rho(\\z)\\Rsans(\\x, \\z) \\p{1 - \\frac{\\d \\rho_Z}{\\d Q_Z}(\\z)} \\d Q_Z(\\z) }^2\\\\\n        &\\\\geq \\norm{\\fstar}^2 \\norm{\\Rsans(\\x, \\cdot)}_{\\ltwo(Q_Z)}^2 \\underbrace{\\int_Z \\p{1 - \\frac{\\d \\rho_Z}{\\d Q_Z}(\\z)}^2 \\d Q_Z(\\z)}_{\\chi^2(\\rho_Z \\Vert Q_Z)}.",
      "raw_latex": "\\begin{align*}\n        &\\p{\\int_{\\Z} g_\\rho(\\z)\\Rsans(\\x, \\z) \\p{\\d Q_Z(\\z) - \\d \\rho_Z(\\z)}}^2 \\\\\n        &= \\p{\\int_{\\Z} g_\\rho(\\z)\\Rsans(\\x, \\z) \\p{1 - \\frac{\\d \\rho_Z}{\\d Q_Z}(\\z)} \\d Q_Z(\\z) }^2\\\\\n        &\\\\geq \\norm{\\fstar}^2 \\norm{\\Rsans(\\x, \\cdot)}_{\\ltwo(Q_Z)}^2 \\underbrace{\\int_Z \\p{1 - \\frac{\\d \\rho_Z}{\\d Q_Z}(\\z)}^2 \\d Q_Z(\\z)}_{\\chi^2(\\rho_Z \\Vert Q_Z)}.\n    \\end{align*}",
      "formula_type": "align*",
      "line_number": 1997,
      "is_formula": true,
      "high_level_explanation": "The expression rewrites the squared integral of a test function against the difference of two measures on Z by expressing the difference via the Radon–Nikodym derivative with respect to Q_Z. It then lower-bounds this quantity using the Cauchy–Schwarz inequality by the product of the squared norms of the involved functions and the chi-square divergence between the prompting and pre-training marginals. This isolates a distribution-mismatch factor, χ^2(ρ_Z || Q_Z), that quantifies how different ρ_Z is from Q_Z, scaled by norms of the kernel section and the function f*.",
      "notations": {
        "g_\\rho(\\z)": "NOT MENTIONED",
        "\\Rsans(\\x, \\z)": "Kernel of the conditional mean operator when integrated under Q_Z (true kernel), per context",
        "\\Rsans(\\x, \\cdot)": "The function z ↦ \\Rsans(\\x, z) obtained by fixing x",
        "\\d Q_Z(\\z)": "Differential with respect to the pre-training marginal measure Q_Z on Z",
        "\\d \\rho_Z(\\z)": "Differential with respect to the prompting marginal measure \\rho_Z on Z",
        "\\frac{\\d \\rho_Z}{\\d Q_Z}(\\z)": "Radon–Nikodym derivative of \\rho_Z with respect to Q_Z at z",
        "\\fstar": "NOT MENTIONED",
        "\\norm{\\Rsans(\\x, \\cdot)}_{\\ltwo(Q_Z)}^2": "Squared L2(Q_Z) norm of the function z ↦ \\Rsans(\\x, z)",
        "\\ltwo(Q_Z)": "Space of square-integrable functions with respect to Q_Z",
        "\\chi^2(\\rho_Z \\Vert Q_Z)": "Chi-square divergence between \\rho_Z and Q_Z",
        "Q_Z": "Pre-training marginal distribution on Z (per context)",
        "\\rho_Z": "Prompting marginal distribution on Z (per context)",
        "\\Z": "NOT MENTIONED",
        "\\x": "NOT MENTIONED",
        "\\z": "NOT MENTIONED"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:56:55.342722"
    },
    {
      "label": "<<FORMULA_1063>>",
      "formula": "\\norm{\\M_{Z|X}}_{\\HS(\\ltwo(Q_Z), \\ltwo(Q_X))}^2 = \\sum_{i \\in 1}^\\infty \\sigma_i^2 &= \\sum_{i =1}^\\infty \\mu_i^{2\\beta+1} \\ipsmall{\\Ssans_{Q_{X, Z}}, \\mu_i^{1/2} e_j}_{\\calS}^2 \\notag \\\\\n        &= \\norm{\\Ssans_{Q_{X, Z}}}_\\calS^2 \\sum_{i =1}^\\infty \\mu_i^{2\\beta+1} \\ipsmall{\\Ssans_{Q_{X, Z}} / \\norm{\\Ssans_{Q_{X, Z}}}_\\calS, \\mu_i^{1/2} e_i}_{\\calS}^2 \\notag \\\\\n        &\\leq \\bar{B}^2 \\sum_{i =1}^\\infty \\mu_i^{2(\\beta+1/2)}.\\label{eq:complexity:rn:eigendecay1}",
      "raw_latex": "\\begin{align}\n        \\norm{\\M_{Z|X}}_{\\HS(\\ltwo(Q_Z), \\ltwo(Q_X))}^2 = \\sum_{i \\in 1}^\\infty \\sigma_i^2 &= \\sum_{i =1}^\\infty \\mu_i^{2\\beta+1} \\ipsmall{\\Ssans_{Q_{X, Z}}, \\mu_i^{1/2} e_j}_{\\calS}^2 \\notag \\\\\n        &= \\norm{\\Ssans_{Q_{X, Z}}}_\\calS^2 \\sum_{i =1}^\\infty \\mu_i^{2\\beta+1} \\ipsmall{\\Ssans_{Q_{X, Z}} / \\norm{\\Ssans_{Q_{X, Z}}}_\\calS, \\mu_i^{1/2} e_i}_{\\calS}^2 \\notag \\\\\n        &\\leq \\bar{B}^2 \\sum_{i =1}^\\infty \\mu_i^{2(\\beta+1/2)}.\\label{eq:complexity:rn:eigendecay1}\n    \\end{align}",
      "formula_type": "align",
      "line_number": 2055,
      "is_formula": true,
      "high_level_explanation": "The display relates the squared Hilbert–Schmidt norm of the conditional mean operator M_{Z|X} to its singular values and then to the eigen-system {μ_i, e_i} via inner products with the Radon–Nikodym element in the RKHS. By normalizing this element and using a uniform bound on its RKHS norm, the norm is upper bounded by a weighted series in the eigenvalues, with weights determined by the source smoothness parameter β. This shows that finiteness of the Hilbert–Schmidt norm follows from eigenvalue decay and the source condition (through β) together with boundedness of the RKHS element.",
      "notations": {
        "\\M_{Z|X}": "Conditional mean operator mapping g \\in L2(Q_Z) to x \\mapsto E[g(Z)\\mid X=x] in L2(Q_X)",
        "\\HS(\\ltwo(Q_Z), \\ltwo(Q_X))": "Space of Hilbert–Schmidt operators from \\ltwo(Q_Z) to \\ltwo(Q_X)",
        "\\ltwo(Q_Z)": "Space of square-integrable functions with respect to the measure Q_Z",
        "\\ltwo(Q_X)": "Space of square-integrable functions with respect to the measure Q_X",
        "\\sigma_i": "i-th singular value of \\M_{Z|X}",
        "\\mu_i": "i-th eigenvalue from the referenced eigendecomposition (paired with e_i)",
        "e_i": "i-th eigenfunction/basis element in \\calS corresponding to \\mu_i",
        "e_j": "j-th eigenfunction/basis element in \\calS corresponding to \\mu_j",
        "\\Ssans_{Q_{X, Z}}": "Radon–Nikodym derivative element in \\calS associated with the joint measure Q_{X, Z} under the source condition",
        "\\calS": "Reproducing kernel Hilbert space (RKHS) used in the analysis",
        "\\beta": "Source/smoothness parameter governing the weighting exponent on the eigenvalues",
        "\\bar{B}": "Uniform bound on \\|\\Ssans_{Q_{X, Z}}\\|_{\\calS} over the considered range of \\beta",
        "Q_{X, Z}": "Joint distribution of (X, Z)",
        "Q_X": "Marginal distribution of X",
        "Q_Z": "Marginal distribution of Z"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:55:00.049730"
    },
    {
      "label": "<<FORMULA_1167>>",
      "formula": "\\risk(\\dec \\circ s) - \\risk(h_\\star) &\\lesssim \\sqrt{C\\E{P_Z}{I(X; Y|Z)} + \\textstyle\\\\prod_{j=1}^C \\norm{g_\\rho\\pow{c} - g_{P_{Y, Z}}\\pow{c}}_{\\ltwo(P_Z)}^2 + C\\tv(P_X, Q_X)}\\\\\n    &\\quad + \\begin{cases}\n        \\sqrt{C}\\polylog\\p{C/\\delta}\\p{N^{-\\frac{q(t)}{2(q(t) + 1)}} + M^{-\\frac{2\\omega_\\rho - 1}{4\\omega_\\rho + 2}}} &\\text{ (conditional mean)}\\\\\n        \\sqrt{C}\\polylog\\p{C/\\delta} \\p{N^{-\\frac{\\beta}{2(\\beta + 1)}} + M^{-1/2}} + \\sqrt{D_{\\chi^2}(\\rho_Z \\Vert Q_Z)} &\\text{ (information density)}\n    \\end{cases},",
      "raw_latex": "\\begin{align*}\n    \\risk(\\dec \\circ s) - \\risk(h_\\star) &\\lesssim \\sqrt{C\\E{P_Z}{I(X; Y|Z)} + \\textstyle\\\\prod_{j=1}^C \\norm{g_\\rho\\pow{c} - g_{P_{Y, Z}}\\pow{c}}_{\\ltwo(P_Z)}^2 + C\\tv(P_X, Q_X)}\\\\\n    &\\quad + \\begin{cases}\n        \\sqrt{C}\\polylog\\p{C/\\delta}\\p{N^{-\\frac{q(t)}{2(q(t) + 1)}} + M^{-\\frac{2\\omega_\\rho - 1}{4\\omega_\\rho + 2}}} &\\text{ (conditional mean)}\\\\\n        \\sqrt{C}\\polylog\\p{C/\\delta} \\p{N^{-\\frac{\\beta}{2(\\beta + 1)}} + M^{-1/2}} + \\sqrt{D_{\\chi^2}(\\rho_Z \\Vert Q_Z)} &\\text{ (information density)}\n    \\end{cases},\n\\end{align*}",
      "formula_type": "align*",
      "line_number": 2260,
      "is_formula": true,
      "high_level_explanation": "This inequality upper-bounds the excess risk of using the decoded score function compared to a reference predictor, up to universal constants. The leading square-root term aggregates intrinsic task difficulty and distribution mismatch: conditional mutual information between X and Y given Z, discrepancies between class-specific functions under two prompt-related distributions measured in L2(P_Z), and total-variation shift between P_X and Q_X. The second term captures estimation error, with rates governed by the sizes of the pre-training data N and the number of prompts M, and by problem-dependent smoothness/complexity parameters; two variants correspond to conditional-mean estimation and information-density estimation. Logarithmic factors in C/δ are suppressed via a polylog term.",
      "notations": {
        "\\risk(\\dec \\circ s)": "Risk of the predictor obtained by composing the decoder with the score function",
        "\\risk(h_\\star)": "Risk of a reference/optimal predictor",
        "C": "Number of classes",
        "\\E{P_Z}{I(X; Y|Z)}": "Expectation over Z drawn from P_Z of the conditional mutual information I(X; Y | Z)",
        "I(X; Y|Z)": "Conditional mutual information between X and Y given Z",
        "g_\\rho\\pow{c}": "NOT MENTIONED",
        "g_{P_{Y, Z}}\\pow{c}": "NOT MENTIONED",
        "P_{Y, Z}": "NOT MENTIONED",
        "\\ltwo(P_Z)": "L2 space with respect to P_Z",
        "P_Z": "NOT MENTIONED",
        "\\tv(P_X, Q_X)": "Total variation distance between P_X and Q_X",
        "P_X": "NOT MENTIONED",
        "Q_X": "NOT MENTIONED",
        "\\polylog\\p{C/\\delta}": "Polylogarithmic factor in C/δ",
        "N": "Size of the pre-training set",
        "M": "Number of prompts",
        "q(t)": "NOT MENTIONED",
        "\\omega_\\rho": "NOT MENTIONED",
        "\\beta": "NOT MENTIONED",
        "D_{\\chi^2}(\\rho_Z \\Vert Q_Z)": "Chi-squared divergence between \\rho_Z and Q_Z",
        "\\rho_Z": "Marginal distribution over Z induced by the prompting measure \\rho_{Y, Z}",
        "Q_Z": "NOT MENTIONED",
        "\\dec": "Decoder",
        "s": "Score function",
        "h_\\star": "NOT MENTIONED",
        "\\delta": "NOT MENTIONED"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:55:19.862141"
    },
    {
      "label": "<<FORMULA_1240>>",
      "formula": "\\Lclip(\\balpha, \\bbeta) &\\approx -\\p{\\Ex_{P}\\ip{\\balpha(X), \\bbeta(Z)} - \\ip{\\E{P_X}{\\balpha(X)}, \\E{P_Z}{\\bbeta(Z)}}} \\\\\n    &+ \\tfrac{1}{4} \\E{P_X}{\\Var\\p{\\ip{\\balpha(X), \\bbeta(Z)}|X}} + \\tfrac{1}{4} \\E{P_Z}{\\Var\\p{\\ip{\\balpha(X), \\bbeta(Z)}|Z}}\\\\\n    &= -\\Tr\\p{\\Cov(\\balpha(X), \\bbeta(Z))} + \\tfrac{1}{4} \\E{P_X}{\\Var\\p{\\ip{\\balpha(X), \\bbeta(Z)}|X}} + \\tfrac{1}{4} \\E{P_Z}{\\Var\\p{\\ip{\\balpha(X), \\bbeta(Z)}|Z}}.",
      "raw_latex": "\\begin{align*}\n    \\Lclip(\\balpha, \\bbeta) &\\approx -\\p{\\Ex_{P}\\ip{\\balpha(X), \\bbeta(Z)} - \\ip{\\E{P_X}{\\balpha(X)}, \\E{P_Z}{\\bbeta(Z)}}} \\\\\n    &+ \\tfrac{1}{4} \\E{P_X}{\\Var\\p{\\ip{\\balpha(X), \\bbeta(Z)}|X}} + \\tfrac{1}{4} \\E{P_Z}{\\Var\\p{\\ip{\\balpha(X), \\bbeta(Z)}|Z}}\\\\\n    &= -\\Tr\\p{\\Cov(\\balpha(X), \\bbeta(Z))} + \\tfrac{1}{4} \\E{P_X}{\\Var\\p{\\ip{\\balpha(X), \\bbeta(Z)}|X}} + \\tfrac{1}{4} \\E{P_Z}{\\Var\\p{\\ip{\\balpha(X), \\bbeta(Z)}|Z}}.\n\\end{align*}",
      "formula_type": "align*",
      "line_number": 2399,
      "is_formula": true,
      "high_level_explanation": "This is a second-order (Taylor) approximation of the population CLIP loss. The leading term is the negative trace of the cross-covariance between the two embeddings, which encourages alignment of paired representations across modalities. The additional one-quarter terms are variance-regularization penalties: they average the conditional variance of the cross-similarity given one view (once conditioning on X and once on Z). The first line is equivalently rewritten using the identity that the trace of the cross-covariance equals the difference between the joint inner-product expectation and the inner product of marginal expectations.",
      "notations": {
        "\\Lclip": "Population CLIP objective (contrastive language–image pretraining) as a function of the encoders",
        "\\balpha": "Encoder for X-valued inputs producing embeddings in \\mathbb{R}^d",
        "\\bbeta": "Encoder for Z-valued inputs producing embeddings in \\mathbb{R}^d",
        "X": "Random variable representing an object from modality X",
        "Z": "Random variable representing an object from modality Z",
        "P": "NOT MENTIONED",
        "P_X": "NOT MENTIONED",
        "P_Z": "NOT MENTIONED",
        "\\Cov(\\balpha(X), \\bbeta(Z))": "Cross-covariance matrix between the random vectors \\balpha(X) and \\bbeta(Z)"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:56:34.751935"
    },
    {
      "label": "<<FORMULA_1249>>",
      "formula": "\\hLBT(\\balpha, \\bbeta) &:= \\frac{1}{2}\\sum_{i=1}^d \\p{(\\hS_{AB})_{i, i} - 1}^2 + \\kappa \\norm{\\bS_{AB}}_{\\mathrm{F}}^2 + \\iota_{\\br{\\I}}(\\bS_{AA}) + \\iota_{\\br{\\I}}(\\bS_{BB})\\\\\n    &= \\underbrace{-\\Tr(\\hS_{AB}) + \\kappa \\norm{\\bS_{AB}}_{\\mathrm{F}}^2}_{\\text{covariance}} + \\underbrace{\\sum_{i=1}^d (\\hS_{AB})_{i, i}^2 + \\frac{d}{2} +  \\iota_{\\br{\\I}}(\\bS_{AA}) + \\iota_{\\br{\\I}}(\\bS_{BB})}_{\\text{variance regularization}}.",
      "raw_latex": "\\begin{align*}\n    \\hLBT(\\balpha, \\bbeta) &:= \\frac{1}{2}\\sum_{i=1}^d \\p{(\\hS_{AB})_{i, i} - 1}^2 + \\kappa \\norm{\\bS_{AB}}_{\\mathrm{F}}^2 + \\iota_{\\br{\\I}}(\\bS_{AA}) + \\iota_{\\br{\\I}}(\\bS_{BB})\\\\\n    &= \\underbrace{-\\Tr(\\hS_{AB}) + \\kappa \\norm{\\bS_{AB}}_{\\mathrm{F}}^2}_{\\text{covariance}} + \\underbrace{\\sum_{i=1}^d (\\hS_{AB})_{i, i}^2 + \\frac{d}{2} +  \\iota_{\\br{\\I}}(\\bS_{AA}) + \\iota_{\\br{\\I}}(\\bS_{BB})}_{\\text{variance regularization}}.\n\\end{align*}",
      "formula_type": "align*",
      "line_number": 2413,
      "is_formula": true,
      "high_level_explanation": "This expression defines a Barlow Twins–style empirical loss for two encoders, decomposed into a covariance-alignment term and a variance-regularization term. The first line encourages the cross-matrix to have unit diagonal while penalizing off-diagonal entries, and it enforces unit variance of each modality via indicator constraints. The second line is an algebraic rewrite that separates the covariance alignment (trace and off-diagonal Frobenius penalty) from variance regularization (diagonal-squared terms, a constant, and variance constraints).",
      "notations": {
        "\\hLBT": "Empirical Barlow Twins objective for the two encoders",
        "\\balpha": "Encoder for modality X",
        "\\bbeta": "Encoder for modality Z",
        "d": "Representation dimension (so that the identity matrix is d-by-d)",
        "\\kappa": "Regularization hyperparameter weighting the off-diagonal penalty",
        "\\hS_{AB}": "NOT MENTIONED",
        "\\bS_{AB}": "\\hS_{AB} with its diagonal entries set to zero",
        "\\bS_{AA}": "NOT MENTIONED",
        "\\bS_{BB}": "NOT MENTIONED",
        "\\iota_{\\br{\\I}}": "Convex indicator of the singleton set {\\I}: 0 if its matrix argument equals \\I, and +infinity otherwise",
        "\\I": "Identity matrix",
        "\\norm{\\bS_{AB}}_{\\mathrm{F}}": "Frobenius norm of \\bS_{AB}"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:54:59.599334"
    },
    {
      "label": "<<FORMULA_1252>>",
      "formula": "\\hLSC(\\balpha, \\bbeta) &:= -\\frac{1}{n} \\sum_{i = 1}^n \\ip{\\balpha(\\x_i), \\bbeta(\\z_i)} + \\frac{1}{n(n-1)} \\sum_{i \\neq j} \\p{\\ip{\\balpha(\\x_i), \\bbeta(\\z_j)}}^2\\\\\n    &= -\\frac{1}{n} \\sum_{i = 1}^n \\ip{\\balpha(\\x_i) -\\bar{\\balpha}, \\bbeta(\\z_i) - \\bar{\\bbeta}} - \\ip{\\bar{\\balpha}, \\bar{\\bbeta}} \\\\\n    &\\quad + \\frac{1}{n(n-1)} \\sum_{i \\neq j} \\p{\\ip{\\balpha(\\x_i) - \\bar{\\balpha}, \\bbeta(\\z_j) - \\bar{\\bbeta}}}^2 + \\p{\\ip{\\bar{\\balpha}, \\bar{\\bbeta}}}^2\\\\\n    &= -\\Tr(\\hS_{AB}) + \\frac{1}{n-1}\\norm{\\bS_{AB}}_{\\mathrm{F}}^2 - \\ip{\\bar{\\balpha}, \\bar{\\bbeta}} + \\p{\\ip{\\bar{\\balpha}, \\bar{\\bbeta}}}^2\\\\\n    &= \\underbrace{-\\Tr(\\hS_{AB}) + \\frac{1}{n-1}\\norm{\\bS_{AB}}_{\\mathrm{F}}^2}_{\\text{covariance}} + \\underbrace{\\p{\\ip{\\bar{\\balpha}, \\bar{\\bbeta}} - \\frac{1}{2}}^2 - \\frac{1}{4}}_{\\text{variance regularization}}",
      "raw_latex": "\\begin{align*}\n    \\hLSC(\\balpha, \\bbeta) &:= -\\frac{1}{n} \\sum_{i = 1}^n \\ip{\\balpha(\\x_i), \\bbeta(\\z_i)} + \\frac{1}{n(n-1)} \\sum_{i \\neq j} \\p{\\ip{\\balpha(\\x_i), \\bbeta(\\z_j)}}^2\\\\\n    &= -\\frac{1}{n} \\sum_{i = 1}^n \\ip{\\balpha(\\x_i) -\\bar{\\balpha}, \\bbeta(\\z_i) - \\bar{\\bbeta}} - \\ip{\\bar{\\balpha}, \\bar{\\bbeta}} \\\\\n    &\\quad + \\frac{1}{n(n-1)} \\sum_{i \\neq j} \\p{\\ip{\\balpha(\\x_i) - \\bar{\\balpha}, \\bbeta(\\z_j) - \\bar{\\bbeta}}}^2 + \\p{\\ip{\\bar{\\balpha}, \\bar{\\bbeta}}}^2\\\\\n    &= -\\Tr(\\hS_{AB}) + \\frac{1}{n-1}\\norm{\\bS_{AB}}_{\\mathrm{F}}^2 - \\ip{\\bar{\\balpha}, \\bar{\\bbeta}} + \\p{\\ip{\\bar{\\balpha}, \\bar{\\bbeta}}}^2\\\\\n    &= \\underbrace{-\\Tr(\\hS_{AB}) + \\frac{1}{n-1}\\norm{\\bS_{AB}}_{\\mathrm{F}}^2}_{\\text{covariance}} + \\underbrace{\\p{\\ip{\\bar{\\balpha}, \\bar{\\bbeta}} - \\frac{1}{2}}^2 - \\frac{1}{4}}_{\\text{variance regularization}}\n\\end{align*}",
      "formula_type": "align*",
      "line_number": 2422,
      "is_formula": true,
      "high_level_explanation": "This expression defines an empirical loss that combines a negative average similarity between paired representations with a quadratic penalty on cross-similarities across different samples. By introducing empirical means and centering, it is rewritten in terms of a covariance part (minus the trace of an empirical cross-covariance plus a Frobenius-norm penalty) and a mean-similarity regularization term. The final form shows the loss encourages large positive covariance between aligned pairs while controlling overall correlation energy and nudging the mean inner product toward one half.",
      "notations": {
        "\\hLSC": "Empirical objective defined by the given expression",
        "\\balpha": "NOT MENTIONED",
        "\\bbeta": "NOT MENTIONED",
        "n": "Number of samples",
        "\\x_i": "NOT MENTIONED",
        "\\z_i": "NOT MENTIONED",
        "\\bar{\\balpha}": "Empirical mean of the vectors \\balpha(\\x_i) over i",
        "\\bar{\\bbeta}": "Empirical mean of the vectors \\bbeta(\\z_i) over i",
        "\\Tr": "Trace operator on a square matrix",
        "\\hS_{AB}": "NOT MENTIONED",
        "\\bS_{AB}": "NOT MENTIONED",
        "\\norm{\\bS_{AB}}_{\\mathrm{F}}": "Frobenius norm of \\bS_{AB}"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:55:35.926517"
    },
    {
      "label": "<<FORMULA_1260>>",
      "formula": "\\frac{1}{2n}\\norm{\\A - \\B}_{\\mathrm{F}}^2 &= \\frac{1}{n}\\sum_{i=1}^n \\norm{\\balpha(\\x_i) - \\bbeta(\\z_i)}_2^2\\\\\n    &= \\frac{1}{2n}\\sum_{i=1}^n \\norm{\\balpha(\\x_i)  - \\bar{\\balpha} - \\bbeta(\\z_i)  + \\bar{\\bbeta}}_2^2 + \\frac{1}{2}\\norm{\\bar{\\balpha} - \\bar{\\bbeta}}_2^2\\\\\n    &= \\frac{1}{2}\\Tr\\p{\\hS_{AA}} + \\frac{1}{2}\\Tr\\p{\\hS_{BB}} - 2 \\Tr\\p{\\hS_{AB}} + \\frac{1}{2}\\norm{\\bar{\\balpha} - \\bar{\\bbeta}}_2^2.",
      "raw_latex": "\\begin{align*}\n    \\frac{1}{2n}\\norm{\\A - \\B}_{\\mathrm{F}}^2 &= \\frac{1}{n}\\sum_{i=1}^n \\norm{\\balpha(\\x_i) - \\bbeta(\\z_i)}_2^2\\\\\n    &= \\frac{1}{2n}\\sum_{i=1}^n \\norm{\\balpha(\\x_i)  - \\bar{\\balpha} - \\bbeta(\\z_i)  + \\bar{\\bbeta}}_2^2 + \\frac{1}{2}\\norm{\\bar{\\balpha} - \\bar{\\bbeta}}_2^2\\\\\n    &= \\frac{1}{2}\\Tr\\p{\\hS_{AA}} + \\frac{1}{2}\\Tr\\p{\\hS_{BB}} - 2 \\Tr\\p{\\hS_{AB}} + \\frac{1}{2}\\norm{\\bar{\\balpha} - \\bar{\\bbeta}}_2^2.\n\\end{align*}",
      "formula_type": "align*",
      "line_number": 2437,
      "is_formula": true,
      "high_level_explanation": "This identity decomposes the average squared Frobenius distance between two matrices A and B into equivalent forms. It equals the average over samples of the squared Euclidean distance between paired representations, which can be split into a centered term plus a term depending only on the difference between their means. The centered term can be written using trace expressions of second-order statistics matrices, emphasizing how both mean mismatch and covariance-like structure contribute to the overall distance.",
      "notations": {
        "\\A": "NOT MENTIONED",
        "\\B": "NOT MENTIONED",
        "n": "NOT MENTIONED",
        "\\balpha(\\x_i)": "NOT MENTIONED",
        "\\bbeta(\\z_i)": "NOT MENTIONED",
        "\\x_i": "NOT MENTIONED",
        "\\z_i": "NOT MENTIONED",
        "\\bar{\\balpha}": "NOT MENTIONED",
        "\\bar{\\bbeta}": "NOT MENTIONED",
        "\\hS_{AA}": "NOT MENTIONED",
        "\\hS_{BB}": "NOT MENTIONED",
        "\\hS_{AB}": "NOT MENTIONED"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:56:14.972639"
    },
    {
      "label": "<<FORMULA_1302>>",
      "formula": "\\theta = 1 &\\implies \\begin{cases}\n    \\bmu_{X|0} - \\C_{XZ|0} \\C_{ZZ|0}^{-1}\\bmu_{Z|0} = \\bmu_{X|1} - \\C_{XZ|1} \\C_{ZZ|1}^{-1} \\bmu_{Z|1}\\\\\n    \\C_{XZ|0} \\C_{ZZ|0}^{-1} = \\C_{XZ|1} \\C_{ZZ|1}^{-1}\\\\\n    \\C_{XX|0} + \\C_{XZ|0} \\C_{ZZ|0}^{-1} \\C_{ZX|0} = \\C_{XX|1} - \\C_{XZ|1} \\C_{ZZ|1}^{-1} \\C_{ZX|1}\\\\\n    \\end{cases} \\implies X \\indep Y | Z = \\z \\ \\forall \\z, \\label{eq:re:resid}",
      "raw_latex": "\\begin{align}\n    \\theta = 1 &\\implies \\begin{cases}\n    \\bmu_{X|0} - \\C_{XZ|0} \\C_{ZZ|0}^{-1}\\bmu_{Z|0} = \\bmu_{X|1} - \\C_{XZ|1} \\C_{ZZ|1}^{-1} \\bmu_{Z|1}\\\\\n    \\C_{XZ|0} \\C_{ZZ|0}^{-1} = \\C_{XZ|1} \\C_{ZZ|1}^{-1}\\\\\n    \\C_{XX|0} + \\C_{XZ|0} \\C_{ZZ|0}^{-1} \\C_{ZX|0} = \\C_{XX|1} - \\C_{XZ|1} \\C_{ZZ|1}^{-1} \\C_{ZX|1}\\\\\n    \\end{cases} \\implies X \\indep Y | Z = \\z \\ \\forall \\z, \\label{eq:re:resid}\n\\end{align}",
      "formula_type": "align",
      "line_number": 2596,
      "is_formula": true,
      "high_level_explanation": "The statement asserts that when the interpolation parameter θ equals 1, several equalities between class-conditional means and (cross-)covariances of (X, Z) hold across the two classes. These equalities make the affine conditional mean of X given Z and the relevant covariance structure identical across Y = 0 and Y = 1, so the conditional distribution of X given Z no longer depends on Y. Consequently, X is conditionally independent of Y given any value of Z.",
      "notations": {
        "\\theta": "Interpolation parameter in the simulation (varied between 0 and 1); θ = 1 corresponds to the no-residual-dependence setting",
        "\\bmu_{X|0}": "Mean vector of X conditional on Y = 0",
        "\\C_{XZ|0}": "Cross-covariance matrix between X and Z conditional on Y = 0",
        "\\C_{ZZ|0}": "Covariance matrix of Z conditional on Y = 0",
        "\\bmu_{Z|0}": "Mean vector of Z conditional on Y = 0",
        "\\bmu_{X|1}": "Mean vector of X conditional on Y = 1",
        "\\C_{XZ|1}": "Cross-covariance matrix between X and Z conditional on Y = 1",
        "\\C_{ZZ|1}": "Covariance matrix of Z conditional on Y = 1",
        "\\bmu_{Z|1}": "Mean vector of Z conditional on Y = 1",
        "\\C_{XX|0}": "Covariance matrix of X conditional on Y = 0",
        "\\C_{ZX|0}": "Cross-covariance matrix between Z and X conditional on Y = 0",
        "\\C_{XX|1}": "Covariance matrix of X conditional on Y = 1",
        "\\C_{ZX|1}": "Cross-covariance matrix between Z and X conditional on Y = 1",
        "X": "Random vector in the joint distribution P_{X,Y,Z}",
        "Y": "Binary class label in the joint distribution P_{X,Y,Z}",
        "Z": "Random vector in the joint distribution P_{X,Y,Z}",
        "\\z": "A specific value (realization) of Z"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:56:08.621883"
    }
  ],
  "metadata": {
    "model": "gpt-5",
    "context_words": 300,
    "max_formulas": 20,
    "timestamp": "2025-10-31T16:56:59.542476",
    "total_formulas_in_paper": 20,
    "formulas_selected_for_analysis": 20,
    "skipped_by_length_limit": 0,
    "formulas_explained": 20,
    "notations_skipped": 0,
    "failed": 0
  },
  "skipped_notations": [],
  "failed": []
}