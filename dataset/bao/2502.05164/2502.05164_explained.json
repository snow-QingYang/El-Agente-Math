{
  "formulas": [
    {
      "label": "<<FORMULA_0054>>",
      "formula": "&f_\\text{opt}(\\tilde X)=\\E [ X \\mid \\tilde X ]  \\nonumber\\\\\n  & = \n  \\frac{\\int e^{\\langle x, \\tilde X_{\\paral} \\rangle / \\sigma_{Z}^2} \\:x\\, d S_x}\n       {\\int e^{\\langle x, \\tilde X_{\\paral} \\rangle / \\sigma_{Z}^2} \\: d S_x}\\\\\n  &= \n  \\frac{I_\\frac{d - 1}{2} \\left(R \\frac{\\lVert \\tilde X_{\\paral} \\rVert}{\\sigma_{Z}^2} \\right)}\n       {I_\\frac{d-1}{2} \\left(R \\frac{\\lVert \\tilde X_{\\paral} \\rVert}{\\sigma_{Z}^2} \\right)} \n  R \\frac{\\tilde X_{\\paral}}{\\lVert \\tilde X_{\\paral} \\rVert},",
      "raw_latex": "\\begin{align}\n&f_\\text{opt}(\\tilde X)=\\E [ X \\mid \\tilde X ]  \\nonumber\\\\\n  & = \n  \\frac{\\int e^{\\langle x, \\tilde X_{\\paral} \\rangle / \\sigma_{Z}^2} \\:x\\, d S_x}\n       {\\int e^{\\langle x, \\tilde X_{\\paral} \\rangle / \\sigma_{Z}^2} \\: d S_x}\\\\\n  &= \n  \\frac{I_\\frac{d - 1}{2} \\left(R \\frac{\\lVert \\tilde X_{\\paral} \\rVert}{\\sigma_{Z}^2} \\right)}\n       {I_\\frac{d-1}{2} \\left(R \\frac{\\lVert \\tilde X_{\\paral} \\rVert}{\\sigma_{Z}^2} \\right)} \n  R \\frac{\\tilde X_{\\paral}}{\\lVert \\tilde X_{\\paral} \\rVert},\n\\end{align}",
      "formula_type": "align",
      "line_number": 866,
      "is_formula": true,
      "high_level_explanation": "This expression gives the Bayes-optimal denoiser: the posterior mean of X given the observation \\tilde X. The second line expresses it as a normalized integral (posterior average) over points x on the sphere S, with weights proportional to exp(<x, \\tilde X_{\\paral}>/\\sigma_Z^2). The final line shows a closed form that points in the direction of \\tilde X_{\\paral} and is scaled by R, with the scaling written in terms of modified Bessel functions of the first kind depending on \\|\\tilde X_{\\paral}\\|, R, and \\sigma_Z^2.",
      "notations": {
        "f_\\text{opt}": "Bayes optimal predictor (denoiser), i.e., the posterior mean mapping \\tilde X to \\E[X\\mid\\tilde X]",
        "\\tilde X": "Observed (noisy) sample; the conditioning variable in the posterior expectation",
        "X": "Clean (uncorrupted) sample drawn from the manifold distribution",
        "\\tilde X_{\\paral}": "NOT MENTIONED",
        "\\sigma_{Z}^2": "Noise variance (of the corruption process producing \\tilde X)",
        "R": "Radius of the sphere S",
        "d": "Dimension of the linear subspace V containing the sphere",
        "I_\\frac{d - 1}{2}": "Modified Bessel function of the first kind of order (d−1)/2",
        "I_\\frac{d-1}{2}": "Modified Bessel function of the first kind of order (d−1)/2",
        "d S_x": "Surface-area measure element on the sphere S (with respect to x)"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:52:08.101864"
    },
    {
      "label": "<<FORMULA_0060>>",
      "formula": "&f_\\text{opt}(\\tilde X)=\\E[X|\\tilde X] \\nonumber\\\\\n    &= \\frac{\\sigma_{0}^2}{\\sigma_{0}^2 + \\sigma_{Z}^2} \\tilde X - \\frac{\\sigma_{Z}^2}{\\sigma_{0}^2 + \\sigma_{Z}^2} \\frac\n    {\\sum_{a } w_a\n         e^{\\langle \\mu_{a}, \\tilde X \\rangle /(\\sigma_0^2 +\\sigma_{Z}^2)}\n        \\:\\: \\mu_{a}\n        }\n    {\\sum_{a }w_a\n         e^{\\langle \\mu_{a}, \\tilde X \\rangle / (\\sigma_0^2 +\\sigma_{Z}^2)}}.",
      "raw_latex": "\\begin{align}\n    &f_\\text{opt}(\\tilde X)=\\E[X|\\tilde X] \\nonumber\\\\\n    &= \\frac{\\sigma_{0}^2}{\\sigma_{0}^2 + \\sigma_{Z}^2} \\tilde X - \\frac{\\sigma_{Z}^2}{\\sigma_{0}^2 + \\sigma_{Z}^2} \\frac\n    {\\sum_{a } w_a\n         e^{\\langle \\mu_{a}, \\tilde X \\rangle /(\\sigma_0^2 +\\sigma_{Z}^2)}\n        \\:\\: \\mu_{a}\n        }\n    {\\sum_{a }w_a\n         e^{\\langle \\mu_{a}, \\tilde X \\rangle / (\\sigma_0^2 +\\sigma_{Z}^2)}}.\n \\end{align}",
      "formula_type": "align",
      "line_number": 888,
      "is_formula": true,
      "high_level_explanation": "This expression gives the Bayes‑optimal denoiser: the posterior mean of the clean signal X given a noisy observation \\tilde X under an isotropic Gaussian mixture prior and Gaussian noise. The estimator is an affine combination of the observation and a softmax‑weighted average of the component centroids, where the softmax uses inner products with \\tilde X and temperature \\sigma_0^2 + \\sigma_Z^2. The coefficients depend on the signal variance \\sigma_0^2 and the noise variance \\sigma_Z^2.",
      "notations": {
        "f_\\text{opt}": "Bayes-optimal predictor (posterior mean estimator) mapping the observation \\tilde X to the estimate of X",
        "X": "Clean (uncorrupted) signal",
        "\\tilde X": "Noisy observation of X under the noise model p_{\\tilde X\\mid X}",
        "\\sigma_{0}^2": "Variance parameter of the clean-signal distribution p_X (e.g., within-component variance)",
        "\\sigma_{Z}^2": "Variance of the observation noise in p_{\\tilde X\\mid X}",
        "w_a": "Mixture weight of component a",
        "\\mu_{a}": "Centroid (mean) of mixture component a"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:52:55.750896"
    },
    {
      "label": "<<FORMULA_0062>>",
      "formula": "\\label{eq:bayes-case3-zerolimit}\n f_\\text{opt}(\\tilde X)=\\E [X \\mid \\tilde X]  = \\frac\n    {\\\\prod_{a } w_a\n         e^{\\langle \\mu_{a}, \\tilde X \\rangle / \\sigma_{Z}^2}\n        \\:\\: \\mu_{a}\n        }\n    {\\sum_{a }w_a\n         e^{\\langle \\mu_{a}, \\tilde X \\rangle / \\sigma_{Z}^2}}.",
      "raw_latex": "\\begin{equation}\n\\label{eq:bayes-case3-zerolimit}\n f_\\text{opt}(\\tilde X)=\\E [X \\mid \\tilde X]  = \\frac\n    {\\\\prod_{a } w_a\n         e^{\\langle \\mu_{a}, \\tilde X \\rangle / \\sigma_{Z}^2}\n        \\:\\: \\mu_{a}\n        }\n    {\\sum_{a }w_a\n         e^{\\langle \\mu_{a}, \\tilde X \\rangle / \\sigma_{Z}^2}}. \n\\end{equation}",
      "formula_type": "equation",
      "line_number": 901,
      "is_formula": true,
      "high_level_explanation": "This expression gives the Bayes-optimal denoiser, i.e., the posterior mean of the clean variable X given a noisy observation \\tilde X, for the Gaussian mixture (clustering) setting in a special limit. It forms a normalized weighted combination of the component representatives \\mu_a, with weights proportional to w_a times an exponential score e^{⟨\\mu_a, \\tilde X⟩/\\sigma_Z^2}. Intuitively, components whose mean aligns more with the observation \\tilde X (relative to the noise scale \\sigma_Z^2) receive higher weight. The result resembles a softmax-weighted average over centroids.",
      "notations": {
        "f_\\text{opt}(\\tilde X)": "Bayes-optimal denoiser evaluated at the observation \\tilde X",
        "\\E": "Expectation operator",
        "X": "Clean (unobserved) signal",
        "\\tilde X": "Observed noisy signal",
        "w_a": "NOT MENTIONED",
        "\\mu_{a}": "NOT MENTIONED",
        "\\sigma_{Z}^2": "NOT MENTIONED"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:53:00.616480"
    },
    {
      "label": "<<FORMULA_0172>>",
      "formula": "\\begin{aligned}\n      p_{X \\mid \\tilde X}(x \\mid \\tilde x) \n        & = \\frac{p_{\\tilde X \\mid X}(\\tilde x \\mid x) p_{X}(x)} { p_{\\tilde X}(\\tilde x) }\\\\\n        & = \\frac\n            {e ^{- \\lVert \\tilde x -  x \\rVert ^2 / 2 \\sigma_{Z}^2} p_X(x) }\n            {\\int e ^{- \\lVert \\tilde x -  x' \\rVert ^2 / 2 \\sigma_{Z}^2} p_X(x') dx'}.\n    \\end{aligned}",
      "raw_latex": "\\begin{equation*}\n    \\begin{aligned}\n      p_{X \\mid \\tilde X}(x \\mid \\tilde x) \n        & = \\frac{p_{\\tilde X \\mid X}(\\tilde x \\mid x) p_{X}(x)} { p_{\\tilde X}(\\tilde x) }\\\\\n        & = \\frac\n            {e ^{- \\lVert \\tilde x -  x \\rVert ^2 / 2 \\sigma_{Z}^2} p_X(x) }\n            {\\int e ^{- \\lVert \\tilde x -  x' \\rVert ^2 / 2 \\sigma_{Z}^2} p_X(x') dx'}.\n    \\end{aligned}\n    \\end{equation*}",
      "formula_type": "equation*",
      "line_number": 1504,
      "is_formula": true,
      "high_level_explanation": "This is Bayes’ rule for the posterior density of the clean variable X given a corrupted observation \\tilde X. The second line instantiates the likelihood as a Gaussian kernel, corresponding to an additive Gaussian noise model with variance sigma_Z^2. The numerator is the likelihood times the prior p_X, and the denominator is the marginal likelihood (a normalizing constant) obtained by integrating over all possible x'.",
      "notations": {
        "p_{X \\mid \\tilde X}": "Posterior density of X given \\tilde X",
        "p_{\\tilde X \\mid X}": "Likelihood of observing \\tilde X given X under the corruption model",
        "p_X": "Prior density of the clean variable X",
        "p_{\\tilde X}": "Marginal density (evidence) of the corrupted observation \\tilde X",
        "x": "A value of the clean variable X",
        "\\tilde x": "Observed (corrupted) value of X",
        "x'": "Dummy integration variable over values of X",
        "\\sigma_{Z}^2": "Variance of the additive Gaussian noise Z corrupting X"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:52:37.317351"
    },
    {
      "label": "<<FORMULA_0173>>",
      "formula": "\\begin{aligned}\n   \\E_{X \\mid \\tilde X} [ X \\mid \\tilde X ]\n              &= \\int x\\: p_{X \\mid \\tilde X}(x \\mid \\tilde X) dx \\\\\n              &= \n              \\frac\n            {\\int\n              x \\, e ^{- \\lVert \\tilde X -  x \\rVert ^2 / 2 \\sigma_{Z}^2} p_X(x) \n            dx}\n            {\\int \n              e ^{- \\lVert \\tilde X -  x \\rVert ^2 / 2 \\sigma_{Z}^2} p_X(x) \n            dx}.\n\\end{aligned}",
      "raw_latex": "\\begin{equation*}\n\\begin{aligned}\n   \\E_{X \\mid \\tilde X} [ X \\mid \\tilde X ]\n              &= \\int x\\: p_{X \\mid \\tilde X}(x \\mid \\tilde X) dx \\\\\n              &= \n              \\frac\n            {\\int\n              x \\, e ^{- \\lVert \\tilde X -  x \\rVert ^2 / 2 \\sigma_{Z}^2} p_X(x) \n            dx}\n            {\\int \n              e ^{- \\lVert \\tilde X -  x \\rVert ^2 / 2 \\sigma_{Z}^2} p_X(x) \n            dx}.\n\\end{aligned}\n\\end{equation*}",
      "formula_type": "equation*",
      "line_number": 1515,
      "is_formula": true,
      "high_level_explanation": "This expression gives the conditional expectation (posterior mean) of the clean variable X given a corrupted observation \\tilde X. The first equality expresses it as an integral of x against the posterior density p_{X\\mid \\tilde X}. The second equality, using Bayes’ rule with isotropic Gaussian corruption of variance \\sigma_Z^2 and prior p_X, shows it is a normalized Gaussian-kernel–weighted average of x. This is the Bayes-optimal denoiser under squared loss.",
      "notations": {
        "\\E_{X \\mid \\tilde X} [ X \\mid \\tilde X ]": "Posterior mean of X given \\tilde X",
        "p_{X \\mid \\tilde X}(x \\mid \\tilde X)": "Posterior probability density of X given \\tilde X, evaluated at x",
        "p_X(x)": "Prior probability density of X, evaluated at x",
        "\\sigma_{Z}^2": "Variance of the additive isotropic corruption noise Z",
        "X": "Underlying clean random input token",
        "\\tilde X": "Observed corrupted input token",
        "x": "Integration variable representing a possible value of X",
        "\\lVert \\tilde X -  x \\rVert": "Euclidean distance between \\tilde X and x",
        "e ^{- \\lVert \\tilde X -  x \\rVert ^2 / 2 \\sigma_{Z}^2}": "Gaussian likelihood kernel proportional to p(\\tilde X \\mid x)"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:51:25.953592"
    },
    {
      "label": "<<FORMULA_0174>>",
      "formula": "\\begin{aligned}\n  \\E \\left[ \\Vert X - f(\\tilde X) \\rVert^2 \\right] \n  & = \\E_{\\tilde X} \\left[ \n        \\E_{X \\mid \\tilde X} \\bigl[ \\lVert X - f(\\tilde X) \\rVert^2 \\mid \\tilde X \\bigr] \n      \\right] \\\\ \n  & = \\E_{\\tilde X} \\Bigl[ \n        \\E_{X\\mid \\tilde X} \\bigl[ \\lVert X - \\E [X \\mid \\tilde X] \\rVert^2 \\mid \\tilde X \\bigr]  \\\\\n         & \\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\: + \\lVert \\E [X \\mid \\tilde X] - f(\\tilde X) \\rVert^2 \\Bigr] \\\\\n  & \\ge \\E_{\\tilde X} \\\\geft[ \n        \\E_{X\\mid \\tilde X} \\bigl[ \\lVert X - \\E [X \\mid \\tilde X] \\rVert^2 \\mid \\tilde X \\bigr]\n    \\right] \\\\\n  & = \\E_{\\tilde X} \\left[ \\Tr{ \\Cov (X \\mid \\tilde X)} \\right].\n\\end{aligned}",
      "raw_latex": "\\begin{equation*}\n\\begin{aligned}\n  \\E \\left[ \\Vert X - f(\\tilde X) \\rVert^2 \\right] \n  & = \\E_{\\tilde X} \\left[ \n        \\E_{X \\mid \\tilde X} \\bigl[ \\lVert X - f(\\tilde X) \\rVert^2 \\mid \\tilde X \\bigr] \n      \\right] \\\\ \n  & = \\E_{\\tilde X} \\Bigl[ \n        \\E_{X\\mid \\tilde X} \\bigl[ \\lVert X - \\E [X \\mid \\tilde X] \\rVert^2 \\mid \\tilde X \\bigr]  \\\\\n         & \\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\: + \\lVert \\E [X \\mid \\tilde X] - f(\\tilde X) \\rVert^2 \\Bigr] \\\\\n  & \\ge \\E_{\\tilde X} \\\\geft[ \n        \\E_{X\\mid \\tilde X} \\bigl[ \\lVert X - \\E [X \\mid \\tilde X] \\rVert^2 \\mid \\tilde X \\bigr]\n    \\right] \\\\\n  & = \\E_{\\tilde X} \\left[ \\Tr{ \\Cov (X \\mid \\tilde X)} \\right].\n\\end{aligned}\n\\end{equation*}",
      "formula_type": "equation*",
      "line_number": 1542,
      "is_formula": true,
      "high_level_explanation": "This is the bias–variance (Bayes risk) decomposition for squared loss under conditioning on the corrupted observation. It rewrites the expected squared error of predicting X from the observation \\tilde X via a function f as the sum of the conditional variance term and the squared deviation between f(\\tilde X) and the conditional mean E[X | \\tilde X]. This yields a lower bound equal to the expected conditional variance, attained when f equals the posterior mean E[X | \\tilde X]. The last line expresses the conditional variance term as the trace of the conditional covariance.",
      "notations": {
        "X": "Random variable (clean token/input) to be predicted; capital X denotes a random variable in this appendix",
        "\\tilde X": "Corrupted observation of X (the tilde indicates corruption of the final token)",
        "f(\\tilde X)": "NOT MENTIONED",
        "\\E_{\\tilde X}": "Expectation with respect to the distribution of \\tilde X",
        "\\E_{X \\mid \\tilde X}": "Conditional expectation over X given \\tilde X",
        "\\E [X \\mid \\tilde X]": "Posterior mean (conditional mean) of X given \\tilde X",
        "\\Cov (X \\mid \\tilde X)": "Conditional covariance matrix of X given \\tilde X",
        "\\Tr": "Trace operator (sum of diagonal entries of a matrix)"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:51:52.875498"
    },
    {
      "label": "<<FORMULA_0183>>",
      "formula": "\\begin{aligned}\np_X(x)e^{- \\frac{\\lVert x - \\tilde x_{\\paral} \\rVert ^2} {2 \\sigma_{Z}^2}}\n  & \\propto e^{- \\frac{\\lVert x \\rVert ^2}{ 2 \\sigma_{0}^2}\n         - \\frac{\\lVert x - \\tilde x_{\\paral} \\rVert ^2 }{ 2 \\sigma_{Z}^2} }\\\\\n  & \\propto \\exp \n    \\left( - \\frac{\\lVert x - \\frac{\\sigma_{0}^2}{\\sigma_{0}^2 - \\sigma_{Z}^2} \\tilde x_{\\paral} \\rVert ^2}\n                   {2 \\frac{\\sigma_{0}^2 \\sigma_{Z}^2}{\\sigma_{0}^2 + \\sigma_{Z}^2}\n                  } \n    \\right). \n\\end{aligned}",
      "raw_latex": "\\begin{equation*}\n\\begin{aligned}\np_X(x)e^{- \\frac{\\lVert x - \\tilde x_{\\paral} \\rVert ^2} {2 \\sigma_{Z}^2}}\n  & \\propto e^{- \\frac{\\lVert x \\rVert ^2}{ 2 \\sigma_{0}^2}\n         - \\frac{\\lVert x - \\tilde x_{\\paral} \\rVert ^2 }{ 2 \\sigma_{Z}^2} }\\\\\n  & \\propto \\exp \n    \\left( - \\frac{\\lVert x - \\frac{\\sigma_{0}^2}{\\sigma_{0}^2 - \\sigma_{Z}^2} \\tilde x_{\\paral} \\rVert ^2}\n                   {2 \\frac{\\sigma_{0}^2 \\sigma_{Z}^2}{\\sigma_{0}^2 + \\sigma_{Z}^2}\n                  } \n    \\right). \n\\end{aligned}\n\\end{equation*}",
      "formula_type": "equation*",
      "line_number": 1589,
      "is_formula": true,
      "high_level_explanation": "This expression completes the square for the product of the prior density p_X(x) and a Gaussian factor arising from the likelihood of the corrupted observation. Up to a normalization constant, it shows that this product is proportional to a Gaussian in x whose covariance is (sigma_0^2 sigma_Z^2)/(sigma_0^2 + sigma_Z^2) and whose mean is a shrinkage of the component of the observation parallel to the signal subspace. The proportionality symbols indicate that x-independent constants have been absorbed into the normalization.",
      "notations": {
        "p_X(x)": "Prior probability density of the clean variable X evaluated at x",
        "x": "Value (vector) of the random variable X, lying in the subspace S",
        "\\tilde x_{\\paral}": "Component of the observed corrupted token parallel to S (projection of \\tilde x onto S)",
        "\\sigma_{Z}^2": "Noise variance (variance of the corruption Z)",
        "\\sigma_{0}^2": "Prior variance parameter of X (isotropic Gaussian on S)"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:51:52.533673"
    },
    {
      "label": "<<FORMULA_0187>>",
      "formula": "\\E\\Big[\\lVert X-\\frac{\\sigma_{0}^2}{\\sigma_{0}^2 + \\sigma_{Z}^2} P \\tilde X \\rVert^2\\Big] =  \\E\\Big[\\lVert\\frac{\\sigma_{Z}^2}{\\sigma_{0}^2 + \\sigma_{Z}^2}PX\\rVert^2\\Big]+\\E\\Big[\\lVert\\frac{\\sigma_{0}^2}{\\sigma_{0}^2 + \\sigma_{Z}^2} PZ \\rVert^2\\Big]=\\frac{\\sigma_Z^4d\\sigma_{0}^2+\\sigma_0^4d\\sigma_{Z}^2}{(\\sigma_{0}^2 + \\sigma_{Z}^2)^2}=\\frac{d\\sigma_0^2\\sigma_{Z}^2}{\\sigma_{0}^2 \\\\times \\sigma_{Z}^2}.",
      "raw_latex": "$$\\E\\Big[\\lVert X-\\frac{\\sigma_{0}^2}{\\sigma_{0}^2 + \\sigma_{Z}^2} P \\tilde X \\rVert^2\\Big] =  \\E\\Big[\\lVert\\frac{\\sigma_{Z}^2}{\\sigma_{0}^2 + \\sigma_{Z}^2}PX\\rVert^2\\Big]+\\E\\Big[\\lVert\\frac{\\sigma_{0}^2}{\\sigma_{0}^2 + \\sigma_{Z}^2} PZ \\rVert^2\\Big]=\\frac{\\sigma_Z^4d\\sigma_{0}^2+\\sigma_0^4d\\sigma_{Z}^2}{(\\sigma_{0}^2 + \\sigma_{Z}^2)^2}=\\frac{d\\sigma_0^2\\sigma_{Z}^2}{\\sigma_{0}^2 \\\\times \\sigma_{Z}^2}.$$",
      "formula_type": "display",
      "line_number": 1607,
      "is_formula": true,
      "high_level_explanation": "The expression computes the mean-squared error (MSE) of a linear Bayes/Wiener estimator that projects the noisy observation onto the signal subspace and shrinks it by the factor σ0^2/(σ0^2 + σZ^2). By orthogonality of the projection and independence of X and Z, the MSE decomposes into separate signal and noise contributions, yielding a closed-form expression. Under an isotropic Gaussian prior on a d-dimensional subspace and isotropic Gaussian noise, this simplifies to d·σ0^2·σZ^2/(σ0^2 + σZ^2).",
      "notations": {
        "\\E": "Expectation operator",
        "X": "Random variable input to the sequence model",
        "\\tilde X": "Corrupted/noisy version of X",
        "Z": "Additive noise random variable, independent of X",
        "P": "Orthogonal projection onto the signal subspace/manifold S",
        "\\sigma_{0}^2": "Prior variance of X within the subspace S",
        "\\sigma_{Z}^2": "Variance of the additive noise Z",
        "d": "Dimensionality of the subspace/manifold S"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:51:33.854916"
    },
    {
      "label": "<<FORMULA_0189>>",
      "formula": "\\begin{aligned}\n\\E [ X \\mid \\tilde X = \\tilde x ] \n  & = \n  \\frac{\\int e^{ + \\frac{\\lVert x - \\tilde x_{\\paral} \\rVert ^2 }{ 2 \\sigma_{Z}^2}} \\: x\\: p_X(x) dx}\n       {\\int e^{- \\frac{\\lVert x - \\tilde x_{\\paral} \\rVert ^2 }{ 2 \\sigma_{Z}^2}} \\: p_X(x) dx} \\\\\n  & = \n  \\frac{\\int e^{\\langle x, \\tilde x_{\\paral} \\rangle / \\sigma_{Z}^2} \\:x\\, d S_x}\n       {\\int e^{\\langle x, \\tilde x_{\\paral} \\rangle / \\sigma_{Z}^2} \\: d S_x}.\n\\end{aligned}",
      "raw_latex": "\\begin{equation*}\n\\begin{aligned}\n\\E [ X \\mid \\tilde X = \\tilde x ] \n  & = \n  \\frac{\\int e^{ + \\frac{\\lVert x - \\tilde x_{\\paral} \\rVert ^2 }{ 2 \\sigma_{Z}^2}} \\: x\\: p_X(x) dx}\n       {\\int e^{- \\frac{\\lVert x - \\tilde x_{\\paral} \\rVert ^2 }{ 2 \\sigma_{Z}^2}} \\: p_X(x) dx} \\\\\n  & = \n  \\frac{\\int e^{\\langle x, \\tilde x_{\\paral} \\rangle / \\sigma_{Z}^2} \\:x\\, d S_x}\n       {\\int e^{\\langle x, \\tilde x_{\\paral} \\rangle / \\sigma_{Z}^2} \\: d S_x}.\n\\end{aligned}\n\\end{equation*}",
      "formula_type": "equation*",
      "line_number": 1618,
      "is_formula": true,
      "high_level_explanation": "This formula gives the Bayes posterior mean estimator of X given a noisy observation \\tilde X = \\tilde x under additive isotropic Gaussian noise with variance \\sigma_Z^2. The first ratio is the standard conditional-expectation integral with the Gaussian likelihood and prior p_X. The second equality rewrites it when the prior is supported on a manifold S, using the component of \\tilde x parallel to S; constants independent of x cancel, yielding a softmax-like weighting proportional to exp(⟨x, \\tilde x_{\\paral}⟩/\\sigma_Z^2) over S.",
      "notations": {
        "X": "Random variable representing the clean data point",
        "\\tilde X": "Observed noisy version of X",
        "\\tilde x": "A specific observed value of \\tilde X",
        "\\tilde x_{\\paral}": "Projection of \\tilde x onto the subspace/manifold S (component parallel to S)",
        "x": "Integration variable representing a possible value of X",
        "p_X(x)": "Prior density of X evaluated at x",
        "\\sigma_{Z}^2": "Variance of the additive Gaussian noise Z",
        "S": "Manifold supporting the distribution of X (e.g., a sphere)",
        "d S_x": "Surface measure element on S at point x"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:51:34.927463"
    },
    {
      "label": "<<FORMULA_0193>>",
      "formula": "\\begin{aligned}\n  \\frac{\\int_0^{\\pi} e^{R \\lVert \\tilde x_{\\paral} \\rVert \\cos \\theta / \\sigma_{Z}^2} \\: \\cos \\theta \\sin^{(d - 1)} \\theta \\: d\\theta}\n       {\\int_0^{\\pi} e^{R \\lVert \\tilde x_{\\paral} \\rVert \\cos \\theta / \\sigma_{Z}^2} \\: \\sin^{(d - 1)} \\theta \\: d\\theta}\n  R \\frac{\\tilde x_{\\paral}}{\\lVert \\tilde x_{\\paral} \\rVert} \\\\\n  = \n  \\frac{I_\\frac{d+1}{2} \\left(R \\frac{\\lVert \\tilde x_{\\paral} \\rVert}{\\sigma_{Z}^2} \\right)}\n       {I_\\frac{d-1}{2} \\left(R \\frac{\\lVert \\tilde x_{\\paral} \\rVert}{\\sigma_{Z}^2} \\right)} \n  R \\frac{\\tilde x_{\\paral}}{\\lVert \\tilde x_{\\paral} \\rVert},\n\\end{aligned}",
      "raw_latex": "\\begin{equation*}\n\\begin{aligned}\n  \\frac{\\int_0^{\\pi} e^{R \\lVert \\tilde x_{\\paral} \\rVert \\cos \\theta / \\sigma_{Z}^2} \\: \\cos \\theta \\sin^{(d - 1)} \\theta \\: d\\theta}\n       {\\int_0^{\\pi} e^{R \\lVert \\tilde x_{\\paral} \\rVert \\cos \\theta / \\sigma_{Z}^2} \\: \\sin^{(d - 1)} \\theta \\: d\\theta}\n  R \\frac{\\tilde x_{\\paral}}{\\lVert \\tilde x_{\\paral} \\rVert} \\\\\n  = \n  \\frac{I_\\frac{d+1}{2} \\left(R \\frac{\\lVert \\tilde x_{\\paral} \\rVert}{\\sigma_{Z}^2} \\right)}\n       {I_\\frac{d-1}{2} \\left(R \\frac{\\lVert \\tilde x_{\\paral} \\rVert}{\\sigma_{Z}^2} \\right)} \n  R \\frac{\\tilde x_{\\paral}}{\\lVert \\tilde x_{\\paral} \\rVert},\n\\end{aligned}\n\\end{equation*}",
      "formula_type": "equation*",
      "line_number": 1633,
      "is_formula": true,
      "high_level_explanation": "This identity evaluates a ratio of angular integrals (with sin^(d−1) weighting) that arise when computing the posterior mean on a d-dimensional sphere of radius R under an exponential tilt in the direction of the observed signal. The ratio simplifies to a ratio of modified Bessel functions of the first kind, evaluated at R times the norm of the parallel component of the observation divided by the noise variance. Multiplying by R times the unit vector in the direction of the parallel component yields the predictor: a point on the sphere aligned with the observation, scaled by a Bessel-function shrinkage factor.",
      "notations": {
        "R": "Radius of the d-sphere S",
        "\\tilde x_{\\paral}": "Component of \\tilde x parallel to the manifold S (projection onto S)",
        "\\sigma_{Z}^2": "Noise variance (variance of Z)",
        "d": "Dimension of the sphere S (S is a d-sphere)",
        "I_\\frac{d+1}{2}": "Modified Bessel function of the first kind of order (d+1)/2",
        "I_\\frac{d-1}{2}": "Modified Bessel function of the first kind of order (d-1)/2",
        "\\frac{\\tilde x_{\\paral}}{\\lVert \\tilde x_{\\paral} \\rVert}": "Unit vector in the direction of \\tilde x_{\\paral}",
        "R \\frac{\\tilde x_{\\paral}}{\\lVert \\tilde x_{\\paral} \\rVert}": "Point on S in the direction of \\tilde x_{\\paral}"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:51:52.259966"
    },
    {
      "label": "<<FORMULA_0197>>",
      "formula": "\\E [X \\mid \\tilde X = \\tilde x] = \n  \\frac\n   {\\int e^{-\\frac{\\|x - \\tilde{x}\\|^2}{2 \\sigma_Z^2}} \\sum_a \\left( w_{a} C_a e^{-\\frac{\\|x - \\mu_\\alpha\\|^2}{2 \\sigma_a^2}}\\right) \n     x \\, dx\n   }\n   {\\int e^{-\\frac{\\|x - \\tilde{x}\\|^2}{2 \\sigma_Z^2}} \\sum_a \\left( w_{a} C_a e^{-\\frac{\\|x - \\mu_a\\|^2}{2 \\sigma_a^2}} \\right)\n     \\, dx\n   },",
      "raw_latex": "\\begin{equation*}\n\\E [X \\mid \\tilde X = \\tilde x] = \n  \\frac\n   {\\int e^{-\\frac{\\|x - \\tilde{x}\\|^2}{2 \\sigma_Z^2}} \\sum_a \\left( w_{a} C_a e^{-\\frac{\\|x - \\mu_\\alpha\\|^2}{2 \\sigma_a^2}}\\right) \n     x \\, dx\n   }\n   {\\int e^{-\\frac{\\|x - \\tilde{x}\\|^2}{2 \\sigma_Z^2}} \\sum_a \\left( w_{a} C_a e^{-\\frac{\\|x - \\mu_a\\|^2}{2 \\sigma_a^2}} \\right)\n     \\, dx\n   },\n\\end{equation*}",
      "formula_type": "equation*",
      "line_number": 1657,
      "is_formula": true,
      "high_level_explanation": "This expression gives the Bayes-optimal estimator (posterior mean) of X given an observation tilde X = tilde x when the likelihood is Gaussian and the prior p_X is a mixture of Gaussians. The numerator integrates x weighted by the product of a Gaussian likelihood centered at the observation and the Gaussian-mixture prior, and the denominator is the corresponding normalizing integral (the marginal likelihood). The ratio equals the conditional expectation E[X | tilde X = tilde x].",
      "notations": {
        "X": "Random variable whose posterior mean is being computed",
        "\\tilde X": "Conditioning random variable (the observed variable)",
        "\\tilde x": "Observed value of \\tilde X",
        "x": "Integration variable over the support of X",
        "\\sigma_Z^2": "Variance parameter in the Gaussian likelihood p_{\\tilde X\\mid X} (observation noise variance)",
        "w_{a}": "NOT MENTIONED",
        "C_a": "NOT MENTIONED",
        "\\mu_\\alpha": "NOT MENTIONED",
        "\\sigma_a^2": "NOT MENTIONED",
        "\\mu_a": "NOT MENTIONED"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:52:28.099956"
    },
    {
      "label": "<<FORMULA_0201>>",
      "formula": "m_a \n% =\n% \\frac{\\frac{1}{\\sigma_\\eta^2} \\,\\tilde{x} + \\frac{1}{\\sigma_\\alpha^2}\\,\\mu_\\alpha}\n%      {\\frac{\\sigma_\\eta^2}{1} + \\frac{1}{\\sigma_\\alpha^2}}\n = \\frac{\\sigma_a^2 \\, \\tilde{x} + \\sigma_Z^2 \\,\\mu_a}\n     {\\sigma_a^2 + \\sigma_Z^2}.",
      "raw_latex": "\\begin{equation*}\nm_a \n% =\n% \\frac{\\frac{1}{\\sigma_\\eta^2} \\,\\tilde{x} + \\frac{1}{\\sigma_\\alpha^2}\\,\\mu_\\alpha}\n%      {\\frac{\\sigma_\\eta^2}{1} + \\frac{1}{\\sigma_\\alpha^2}}\n = \\frac{\\sigma_a^2 \\, \\tilde{x} + \\sigma_Z^2 \\,\\mu_a}\n     {\\sigma_a^2 + \\sigma_Z^2}.\n\\end{equation*}",
      "formula_type": "equation*",
      "line_number": 1690,
      "is_formula": true,
      "high_level_explanation": "This equation defines m_a as the mean obtained when combining an isotropic Gaussian prior with mean μ_a and variance σ_a^2 with a Gaussian-noisy observation ~tilde{x} of variance σ_Z^2. It arises by completing the square when multiplying the prior and likelihood in the Gaussian mixture denoising derivation. The result is a variance-weighted average of the observation and the prior mean: larger variance contributes less weight.",
      "notations": {
        "m_a": "Posterior mean of X for mixture component a given the noisy observation",
        "\\sigma_a^2": "Variance of mixture component a (isotropic prior on X)",
        "\\tilde{x}": "Observed noisy input vector",
        "\\sigma_Z^2": "Variance of the additive Gaussian noise Z",
        "\\mu_a": "Mean (center) of mixture component a"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:53:01.355217"
    },
    {
      "label": "<<FORMULA_0203>>",
      "formula": "\\E [X \\mid \\tilde X = \\tilde x] \n   = \n  \\frac\n   {\\sum_{a} w_{a}\\big((\\frac{\\sigma_Z^2+\\sigma_a^2} {\\sigma_a^2}\\big)^{n/2}) \\exp\\big(-\\frac{\\lVert \\tilde x - \\mu_{a}\\rVert^2}{2 (\\sigma_Z^2 + \\sigma_a^2)}\\big) \\big(\\frac{\\sigma_a^2 \\, \\tilde{x} + \\sigma_Z^2 \\,\\mu_a}\n     {\\sigma_a^2 + \\sigma_Z^2} \\big)}\n   {\\sum_{a} w_{a} \\big(\\frac{\\sigma_Z^2+\\sigma_a^2} {\\sigma_a^2}\\big)^{n/2} \\exp\\big(-\\frac{\\lVert \\tilde x - \\mu_{a}\\rVert^2}{2 (\\sigma_Z^2 + \\sigma_a^2)}\\big)}",
      "raw_latex": "\\begin{equation*}\n  \\E [X \\mid \\tilde X = \\tilde x] \n   = \n  \\frac\n   {\\sum_{a} w_{a}\\big((\\frac{\\sigma_Z^2+\\sigma_a^2} {\\sigma_a^2}\\big)^{n/2}) \\exp\\big(-\\frac{\\lVert \\tilde x - \\mu_{a}\\rVert^2}{2 (\\sigma_Z^2 + \\sigma_a^2)}\\big) \\big(\\frac{\\sigma_a^2 \\, \\tilde{x} + \\sigma_Z^2 \\,\\mu_a}\n     {\\sigma_a^2 + \\sigma_Z^2} \\big)}\n   {\\sum_{a} w_{a} \\big(\\frac{\\sigma_Z^2+\\sigma_a^2} {\\sigma_a^2}\\big)^{n/2} \\exp\\big(-\\frac{\\lVert \\tilde x - \\mu_{a}\\rVert^2}{2 (\\sigma_Z^2 + \\sigma_a^2)}\\big)}\n\\end{equation*}",
      "formula_type": "equation*",
      "line_number": 1700,
      "is_formula": true,
      "high_level_explanation": "This equation gives the posterior mean (Bayes-optimal estimator under squared error) of the clean vector X given the noisy observation \\tilde X = \\tilde x when X follows an isotropic Gaussian mixture prior and the noise is additive isotropic Gaussian with variance \\sigma_Z^2. The numerator sums component-wise posterior means, each equal to (\\sigma_a^2 \\tilde x + \\sigma_Z^2 \\mu_a)/(\\sigma_a^2 + \\sigma_Z^2), weighted by the posterior responsibility of component a for \\tilde x. The denominator normalizes these weights to sum to one. The result is a convex combination of the component conditional means, emphasizing components whose means are closer to \\tilde x and with appropriate variance trade-offs.",
      "notations": {
        "X": "Latent clean random vector drawn from a Gaussian mixture prior",
        "\\tilde X": "Noisy observation of X (additive Gaussian noise)",
        "\\tilde x": "Observed value of \\tilde X used for conditioning",
        "w_{a}": "Mixture weight of component a",
        "\\mu_{a}": "Mean (center) of mixture component a",
        "\\sigma_a^2": "Isotropic variance of mixture component a",
        "\\sigma_Z^2": "Variance of the additive Gaussian noise Z",
        "n": "NOT MENTIONED"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:51:30.245840"
    },
    {
      "label": "<<FORMULA_0206>>",
      "formula": "\\E [X \\mid \\tilde X = \\tilde x] \n%  & = \n%  \\frac{\\sigma_\\alpha^2}\n%     {\\sigma_\\alpha^2 + \\sigma_\\eta^2} \\, \\tilde{x}\n%    + \n%\\frac{\\sigma_\\eta^2}\n%     {\\sigma_\\alpha^2 + \\sigma_\\eta^2}\n%  \\frac\n%   {\\sum_{\\alpha} w_{\\alpha} \\mu_{\\alpha} \\int \\exp(g_{\\alpha}) \\,dx}\n%   {\\sum_{\\alpha} w_{\\alpha} \\int \\exp(g_{\\alpha}) \\,dx} \\\\\n = \n  \\frac{\\sigma_0^2}\n     {\\sigma_0^2 + \\sigma_Z^2} \\, \\tilde{x}\n    + \n\\frac{\\sigma_Z^2}\n     {\\sigma_0^2 + \\sigma_Z^2}\n  \\frac\n   {\\sum_{a} w_{a} \\mu_{a} \n            \\exp\\left( \\frac{\\langle \\tilde{x}, \\mu_a \\rangle}{\\sigma_Z^2 + \\sigma_0^2} \\right)\n     }\n   {\\sum_{a} w_{a} \n            \\exp\\left( \\frac{\\langle \\tilde{x}, \\mu_a \\rangle}{\\sigma_Z^2 + \\sigma_0^2} \\right)\n            %e^{\\frac{\\langle \\tilde{x}, \\mu_\\alpha \\rangle}{(\\sigma_\\eta^2 + \\sigma_\\alpha^2)}}\n     }.",
      "raw_latex": "\\begin{equation*}\n  \\E [X \\mid \\tilde X = \\tilde x] \n%  & = \n%  \\frac{\\sigma_\\alpha^2}\n%     {\\sigma_\\alpha^2 + \\sigma_\\eta^2} \\, \\tilde{x}\n%    + \n%\\frac{\\sigma_\\eta^2}\n%     {\\sigma_\\alpha^2 + \\sigma_\\eta^2}\n%  \\frac\n%   {\\sum_{\\alpha} w_{\\alpha} \\mu_{\\alpha} \\int \\exp(g_{\\alpha}) \\,dx}\n%   {\\sum_{\\alpha} w_{\\alpha} \\int \\exp(g_{\\alpha}) \\,dx} \\\\\n = \n  \\frac{\\sigma_0^2}\n     {\\sigma_0^2 + \\sigma_Z^2} \\, \\tilde{x}\n    + \n\\frac{\\sigma_Z^2}\n     {\\sigma_0^2 + \\sigma_Z^2}\n  \\frac\n   {\\sum_{a} w_{a} \\mu_{a} \n            \\exp\\left( \\frac{\\langle \\tilde{x}, \\mu_a \\rangle}{\\sigma_Z^2 + \\sigma_0^2} \\right)\n     }\n   {\\sum_{a} w_{a} \n            \\exp\\left( \\frac{\\langle \\tilde{x}, \\mu_a \\rangle}{\\sigma_Z^2 + \\sigma_0^2} \\right)\n            %e^{\\frac{\\langle \\tilde{x}, \\mu_\\alpha \\rangle}{(\\sigma_\\eta^2 + \\sigma_\\alpha^2)}}\n     }.\n\\end{equation*}",
      "formula_type": "equation*",
      "line_number": 1711,
      "is_formula": true,
      "high_level_explanation": "This equation gives the Bayes-optimal posterior mean E[X | ~tilde X = ~tilde x] for an additive Gaussian noise model with a Gaussian-mixture prior under the simplifications described in the text. The estimator splits into a linear shrinkage of the observation ~tilde x and a softmax-weighted average of the component means μ_a, where the weights are proportional to w_a exp(⟨~tilde x, μ_a⟩ / (σ_Z^2 + σ_0^2)). The factors σ_0^2/(σ_0^2 + σ_Z^2) and σ_Z^2/(σ_0^2 + σ_Z^2) balance the influence of the observation versus the prior according to the signal and noise variances.",
      "notations": {
        "X": "Clean (latent) signal vector (random variable)",
        "\\tilde X": "Noisy observation of X",
        "\\tilde x": "Observed value (realization) of \\tilde X",
        "\\tilde{x": "Observed value (realization) of \\tilde X",
        "\\sigma_0^2": "Isotropic prior (component) variance of X",
        "\\sigma_Z^2": "Variance of the additive Gaussian noise Z",
        "w_{a}": "Mixture weight of component a",
        "\\mu_{a}": "Mean (cluster center) of mixture component a",
        "a": "Index over mixture components"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:51:53.180679"
    },
    {
      "label": "<<FORMULA_0245>>",
      "formula": "\\label{eq:softmax-ratio}\ng(\\{X_t\\}_{t=1}^L,\\tilde x)=\\frac{\\sum_{t=1}^LX_te^{\\langle X_t,\\tilde x\\rangle/\\sigma_Z^2}}{\\sum_{t=1}^Le^{\\langle X_t,\\tilde x\\rangle/\\sigma_Z^2}}=\\frac{\\frac{1}{L}\\sum_{t=1}^LX_te^{\\langle X_t,\\tilde x\\rangle/\\sigma_Z^2}}{\\frac{1}{L}\\sum_{t=1}^Le^{\\langle X_t,\\tilde x\\rangle/\\sigma_Z^2}}.",
      "raw_latex": "\\begin{equation}\n\\label{eq:softmax-ratio}\ng(\\{X_t\\}_{t=1}^L,\\tilde x)=\\frac{\\sum_{t=1}^LX_te^{\\langle X_t,\\tilde x\\rangle/\\sigma_Z^2}}{\\sum_{t=1}^Le^{\\langle X_t,\\tilde x\\rangle/\\sigma_Z^2}}=\\frac{\\frac{1}{L}\\sum_{t=1}^LX_te^{\\langle X_t,\\tilde x\\rangle/\\sigma_Z^2}}{\\frac{1}{L}\\sum_{t=1}^Le^{\\langle X_t,\\tilde x\\rangle/\\sigma_Z^2}}.\n\\end{equation}",
      "formula_type": "equation",
      "line_number": 1850,
      "is_formula": true,
      "high_level_explanation": "The expression defines g as a softmax-weighted average of the vectors X_t, where each weight is proportional to the exponential of the similarity (inner product) between X_t and the query vector x-tilde, scaled by sigma_Z squared. The second equality shows that multiplying both numerator and denominator by 1/L leaves the ratio unchanged, expressing g as a ratio of empirical averages. This form is useful for applying laws of large numbers to study convergence as the number of tokens L grows.",
      "notations": {
        "g": "Aggregation function (softmax-weighted average) defined by this equation",
        "X_t": "t-th token/input vector",
        "\\{X_t\\}_{t=1}^L": "Sequence of L input tokens/vectors",
        "\\tilde x": "Query vector",
        "L": "Number of tokens/samples in the sequence",
        "\\sigma_Z^2": "NOT MENTIONED"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:52:31.654323"
    },
    {
      "label": "<<FORMULA_0293>>",
      "formula": "\\textrm{Pr}\\Bigg[\\Big|\\frac{1}{L}\\sum_{t=1}^Le^{\\langle X_t,\\tilde x\\rangle/\\sigma_Z^2}-\\E[ e^{\\langle X, \\tilde x \\rangle / \\sigma_{Z}^2}]\\Big|<\\sinh{\\bigg(\\frac{R||\\tilde x||_2}{\\sigma_Z^2}\\bigg)}\\sqrt{\\frac{2}{L}\\ln\\bigg(\\frac{2}{\\delta}\\bigg)}\\Bigg]\\ge1-\\delta",
      "raw_latex": "$$\\textrm{Pr}\\Bigg[\\Big|\\frac{1}{L}\\sum_{t=1}^Le^{\\langle X_t,\\tilde x\\rangle/\\sigma_Z^2}-\\E[ e^{\\langle X, \\tilde x \\rangle / \\sigma_{Z}^2}]\\Big|<\\sinh{\\bigg(\\frac{R||\\tilde x||_2}{\\sigma_Z^2}\\bigg)}\\sqrt{\\frac{2}{L}\\ln\\bigg(\\frac{2}{\\delta}\\bigg)}\\Bigg]\\ge1-\\delta$$",
      "formula_type": "display",
      "line_number": 1923,
      "is_formula": true,
      "high_level_explanation": "This is a concentration inequality showing that the empirical average of exp(<X_t, tilde x> / sigma_Z^2) is close to its population expectation with high probability. Specifically, with probability at least 1 - delta, the absolute deviation is bounded by a term that scales like sinh(R ||tilde x||_2 / sigma_Z^2) times sqrt((2/L) log(2/delta)). The bound follows from Hoeffding’s inequality using that each summand is bounded, and it tightens as the sample size L increases.",
      "notations": {
        "L": "Number of samples in the empirical average",
        "X_t": "t-th sample of the random vector X",
        "X": "Random vector with distribution p_X supported in V",
        "\\tilde x": "Projection of the query onto V",
        "\\sigma_Z^2": "NOT MENTIONED",
        "R": "NOT MENTIONED",
        "\\delta": "Failure probability (confidence) parameter"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:52:56.463937"
    },
    {
      "label": "<<FORMULA_0294>>",
      "formula": "\\textrm{Pr}\\Bigg[\\Big|\\Big|\\frac{1}{L}\\sum_{t=1}^LX_te^{\\langle X_t,\\tilde x\\rangle/\\sigma_Z^2}-\\E[ Xe^{\\langle X, \\tilde x \\rangle / \\sigma_{Z}^2}]\\Big|\\Big|_\\infty<Re^{\\frac{R||\\tilde x||_2}{\\sigma_Z^2}}\\sqrt{\\frac{2}{L}\\ln\\bigg(\\frac{2d}{\\delta}\\bigg)}\\Bigg]\\ge 1-\\delta.",
      "raw_latex": "$$\\textrm{Pr}\\Bigg[\\Big|\\Big|\\frac{1}{L}\\sum_{t=1}^LX_te^{\\langle X_t,\\tilde x\\rangle/\\sigma_Z^2}-\\E[ Xe^{\\langle X, \\tilde x \\rangle / \\sigma_{Z}^2}]\\Big|\\Big|_\\infty<Re^{\\frac{R||\\tilde x||_2}{\\sigma_Z^2}}\\sqrt{\\frac{2}{L}\\ln\\bigg(\\frac{2d}{\\delta}\\bigg)}\\Bigg]\\ge 1-\\delta.$$",
      "formula_type": "display",
      "line_number": 1925,
      "is_formula": true,
      "high_level_explanation": "This is a high-probability concentration bound for the sup-norm error between the empirical average of exponentially weighted vectors (1/L)∑ X_t e^{⟨X_t, x̃⟩/σ_Z^2} and its population mean E[X e^{⟨X, x̃⟩/σ_Z^2}]. With probability at least 1 − δ, the coordinatewise maximum deviation is bounded by R e^{(R||x̃||_2)/σ_Z^2} times a √(log(2d/δ)/L) factor. The bound is obtained via Hoeffding’s inequality and a union bound over the d coordinates, using that X is bounded in norm by R.",
      "notations": {
        "L": "Number of samples (terms in the empirical average)",
        "X_t": "t-th sample vector of the random vector X",
        "X": "Random vector drawn from p_X supported in V",
        "\\tilde x": "Projection of the query onto V",
        "\\sigma_Z^2": "NOT MENTIONED",
        "R": "Upper bound on the Euclidean norm of X (i.e., ||X||_2 \\le R almost surely)",
        "d": "Dimension of the subspace V",
        "\\delta": "Failure probability (confidence) parameter in (0,1)",
        "||\\cdot||_\\infty": "Maximum norm on V with respect to a chosen orthonormal basis",
        "||\\tilde x||_2": "Euclidean norm of \\tilde x"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:53:38.768163"
    },
    {
      "label": "<<FORMULA_0296>>",
      "formula": "\\textrm{Pr}\\Bigg[\\Big|\\frac{1}{L}\\sum_{t=1}^Le^{\\langle X_t,\\tilde x\\rangle/\\sigma_Z^2}-\\E[ e^{\\langle X, \\tilde x \\rangle / \\sigma_{Z}^2}]\\Big|\\ge \\epsilon]\\le 2\\exp\\Bigg[-\\frac{2L\\epsilon^2}{\\Big(\\exp\\big(\\frac{\\sigma_Z^2}{R||\\tilde x||_2}\\big)-\\exp\\big(-\\frac{R||\\tilde x||_2}{\\sigma_Z^2}\\big)\\Big)^2}\\Bigg]=2\\exp\\Bigg[-\\frac{L\\epsilon^2}{2\\sinh^2\\big(\\frac{R||\\tilde x||_2}{\\sigma_Z^2}\\big)}\\Bigg].",
      "raw_latex": "$$\\textrm{Pr}\\Bigg[\\Big|\\frac{1}{L}\\sum_{t=1}^Le^{\\langle X_t,\\tilde x\\rangle/\\sigma_Z^2}-\\E[ e^{\\langle X, \\tilde x \\rangle / \\sigma_{Z}^2}]\\Big|\\ge \\epsilon]\\le 2\\exp\\Bigg[-\\frac{2L\\epsilon^2}{\\Big(\\exp\\big(\\frac{\\sigma_Z^2}{R||\\tilde x||_2}\\big)-\\exp\\big(-\\frac{R||\\tilde x||_2}{\\sigma_Z^2}\\big)\\Big)^2}\\Bigg]=2\\exp\\Bigg[-\\frac{L\\epsilon^2}{2\\sinh^2\\big(\\frac{R||\\tilde x||_2}{\\sigma_Z^2}\\big)}\\Bigg].$$",
      "formula_type": "display",
      "line_number": 1932,
      "is_formula": true,
      "high_level_explanation": "This is a Hoeffding-type concentration inequality bounding the probability that the sample average of e^{<X_t, \\tilde x>/\\sigma_Z^2} deviates from its expectation by at least ε. It uses that each summand is bounded between e^{\\pm R||\\tilde x||_2/\\sigma_Z^2}, yielding a sub-Gaussian tail 2 exp(-2Lε^2/(range)^2), which is then rewritten using exp(a) - exp(-a) = 2 sinh(a) to obtain the final form with sinh. The bound shows how the deviation probability decreases with the number of samples L and depends on R, ||\\tilde x||_2, and \\sigma_Z^2.",
      "notations": {
        "\\textrm{Pr}": "Probability of the event",
        "\\E": "Expectation operator",
        "X_t": "t-th random vector sample",
        "X": "A random vector distributed like the X_t samples",
        "\\tilde x": "Projection of the query onto V",
        "L": "NOT MENTIONED",
        "\\sigma_Z^2": "NOT MENTIONED",
        "\\sigma_{Z}^2": "NOT MENTIONED",
        "R": "NOT MENTIONED",
        "||\\tilde x||_2": "Euclidean norm of \\tilde x",
        "\\langle X_t,\\tilde x\\rangle": "Inner product between X_t and \\tilde x",
        "\\epsilon": "NOT MENTIONED"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:51:44.171328"
    },
    {
      "label": "<<FORMULA_0301>>",
      "formula": "\\textrm{Pr}\\Bigg[\\Big|\\Big|\\frac{1}{L}\\sum_{t=1}^LX_te^{\\langle X_t,\\tilde x\\rangle/\\sigma_Z^2}-\\E[ Xe^{\\langle X, \\tilde x \\rangle / \\sigma_{Z}^2}]\\Big|\\Big|_\\infty\\ge \\epsilon]\\le 2d\\exp\\Bigg[-\\frac{L\\epsilon^2}{2R^2\\exp\\big(\\frac{2R||\\tilde x||_2}{\\sigma_Z^2}\\big)}\\Bigg].",
      "raw_latex": "$$\\textrm{Pr}\\Bigg[\\Big|\\Big|\\frac{1}{L}\\sum_{t=1}^LX_te^{\\langle X_t,\\tilde x\\rangle/\\sigma_Z^2}-\\E[ Xe^{\\langle X, \\tilde x \\rangle / \\sigma_{Z}^2}]\\Big|\\Big|_\\infty\\ge \\epsilon]\\le 2d\\exp\\Bigg[-\\frac{L\\epsilon^2}{2R^2\\exp\\big(\\frac{2R||\\tilde x||_2}{\\sigma_Z^2}\\big)}\\Bigg].$$",
      "formula_type": "display",
      "line_number": 1936,
      "is_formula": true,
      "high_level_explanation": "This is a high-probability concentration bound (of Hoeffding/union-bound type) for the sup-norm error between a vector-valued empirical average and its population expectation. It controls how far the empirical average (1/L)∑ X_t e^{⟨X_t, \\tilde x⟩/σ_Z^2} deviates from E[X e^{⟨X, \\tilde x⟩/σ_Z^2}] in infinity norm by a threshold ε. The tail probability decays exponentially in Lε^2 with constants depending on R, ||\\tilde x||_2, σ_Z^2, and a factor 2d reflecting a union bound over d coordinates. Thus, with more samples L the estimator concentrates, while larger ||\\tilde x||_2 or smaller σ_Z^2 worsen the constant.",
      "notations": {
        "\\textrm{Pr}": "Probability of the event",
        "\\E": "Expectation with respect to the distribution of X (and X_t)",
        "L": "Number of samples in the empirical average",
        "X_t": "t-th sample of the random vector X",
        "X": "Random vector drawn from the data distribution",
        "\\tilde x": "NOT MENTIONED",
        "||\\tilde x||_2": "Euclidean (ℓ2) norm of \\tilde x",
        "\\langle X_t,\\tilde x\\rangle": "Inner product between X_t and \\tilde x",
        "\\sigma_Z^2": "NOT MENTIONED",
        "R": "NOT MENTIONED",
        "d": "Dimension of the vector space V supporting X",
        "\\epsilon": "Deviation threshold in the tail event"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:53:32.776759"
    },
    {
      "label": "<<FORMULA_0309>>",
      "formula": "&F\\Big(E,\\big(\\frac{1}{\\epsilon}W_{PV},\\epsilon W_{KQ}\\big)\\Big)=\\frac{1}{\\epsilon}W_{PV} X_{1:L}\\Bigg[\\frac{1}{L}\\\\geft( \\mathbbm{1}_L +\\epsilon \\big( X_{1:L}^T W_{KQ}\\tilde X-(\\frac{1}{L}\\sum_t X_t^TW_{KQ}\\tilde X)\\mathbbm{1}_L\\big) + O(\\epsilon^2)\\right)\\Bigg]\\nonumber\\\\\n&=\\frac{1}{\\epsilon} W_{PV}\\bar X\n+ \\frac{1}{L}W_{PV}\\sum_{t=1}^LX_t(X_t-\\bar X)^TW_{KQ}\\tilde X+O(\\epsilon),",
      "raw_latex": "\\begin{align}\n&F\\Big(E,\\big(\\frac{1}{\\epsilon}W_{PV},\\epsilon W_{KQ}\\big)\\Big)=\\frac{1}{\\epsilon}W_{PV} X_{1:L}\\Bigg[\\frac{1}{L}\\\\geft( \\mathbbm{1}_L +\\epsilon \\big( X_{1:L}^T W_{KQ}\\tilde X-(\\frac{1}{L}\\sum_t X_t^TW_{KQ}\\tilde X)\\mathbbm{1}_L\\big) + O(\\epsilon^2)\\right)\\Bigg]\\nonumber\\\\\n&=\\frac{1}{\\epsilon} W_{PV}\\bar X\n+ \\frac{1}{L}W_{PV}\\sum_{t=1}^LX_t(X_t-\\bar X)^TW_{KQ}\\tilde X+O(\\epsilon),\n\\end{align}",
      "formula_type": "align",
      "line_number": 1977,
      "is_formula": true,
      "high_level_explanation": "The expression gives a small-argument (epsilon → 0) expansion of an attention-like mapping F when the weights are scaled as (1/epsilon) W_{PV} and epsilon W_{KQ}. Using the first-order Taylor expansion of softmax around zero, the attention weights are approximately uniform (1/L) times a vector of ones plus a linear correction given by centered scores X_t^T W_{KQ} \\tilde X. Multiplying by the inputs X_{1:L} and W_{PV} yields a leading term (1/epsilon) W_{PV} \\bar X and a first-order correction involving the centered inputs (X_t - \\bar X), with higher-order terms collected in O(epsilon).",
      "notations": {
        "F": "NOT MENTIONED",
        "E": "NOT MENTIONED",
        "\\epsilon": "Small positive expansion/scaling parameter tending to 0",
        "W_{PV}": "NOT MENTIONED",
        "W_{KQ}": "NOT MENTIONED",
        "X_{1:L}": "Collection of vectors X_1, \\ldots, X_L",
        "X_t": "t-th sample/input vector (a draw from p_X)",
        "\\mathbbm{1}_L": "L-dimensional column vector of ones",
        "\\tilde X": "NOT MENTIONED",
        "\\bar X": "Empirical mean (1/L) \\sum_{t=1}^L X_t",
        "L": "NOT MENTIONED",
        "O(\\epsilon^2)": "Terms of order epsilon squared and higher, omitted in the expansion",
        "O(\\epsilon)": "Terms of order epsilon, omitted beyond the displayed correction"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:52:37.559643"
    }
  ],
  "metadata": {
    "model": "gpt-5",
    "context_words": 300,
    "max_formulas": 20,
    "timestamp": "2025-10-31T16:53:38.770405",
    "total_formulas_in_paper": 20,
    "formulas_selected_for_analysis": 20,
    "skipped_by_length_limit": 0,
    "formulas_explained": 20,
    "notations_skipped": 0,
    "failed": 0
  },
  "skipped_notations": [],
  "failed": []
}