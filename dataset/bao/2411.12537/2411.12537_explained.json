{
  "formulas": [
    {
      "label": "<<FORMULA_0004>>",
      "formula": "\\label{eq:linrnn}\n\\begin{gathered}\n    \\mH_i =  \\mA(\\vx_i) \\mH_{i-1} \\\\times \\mB(\\vx_i), \\quad \\hat{\\vy}_i = \\mathrm{dec}(\\mH_i, \\vx_i), \\quad \\text{ for all } i \\in \\{1, \\dots, t\\}, \\\\\n\\mH_0 \\in \\C^{n \\times d}, \\quad \\mA: \\R^l \\to \\C^{n \\times n}, \\quad  \\mB: \\R^l \\to \\C^{n \\times d}, \\quad \\mathrm{dec}: \\C^{n \\times d} \\times \\R^l \\to \\R^p\n\\end{gathered}",
      "raw_latex": "\\begin{equation}\\label{eq:linrnn}\n\\begin{gathered}\n    \\mH_i =  \\mA(\\vx_i) \\mH_{i-1} \\\\times \\mB(\\vx_i), \\quad \\hat{\\vy}_i = \\mathrm{dec}(\\mH_i, \\vx_i), \\quad \\text{ for all } i \\in \\{1, \\dots, t\\}, \\\\\n\\mH_0 \\in \\C^{n \\times d}, \\quad \\mA: \\R^l \\to \\C^{n \\times n}, \\quad  \\mB: \\R^l \\to \\C^{n \\times d}, \\quad \\mathrm{dec}: \\C^{n \\times d} \\times \\R^l \\to \\R^p\n\\end{gathered}\n\\end{equation}",
      "formula_type": "equation",
      "line_number": 657,
      "is_formula": true,
      "high_level_explanation": "These equations define a Linear Recurrent Neural Network (LRNN) layer. At position i, the hidden state H_i is computed by applying an input-dependent linear map A(x_i) to the previous state H_{i-1} and combining it with an input-conditioned matrix B(x_i) via the operator × (as defined by the specific LRNN instance). A decoder dec then maps the current state and input to an output vector. The type signatures indicate inputs live in an l-dimensional real space, states in an n-by-d complex space, and outputs in a p-dimensional real space.",
      "notations": {
        "\\mH_i": "Hidden state matrix at step i",
        "\\mA": "Learnable function mapping \\R^l to \\C^{n \\times n} that produces the state-transition matrix conditioned on the current input",
        "\\vx_i": "NOT MENTIONED",
        "\\mH_{i-1}": "Previous hidden state matrix",
        "\\mB": "Learnable function mapping \\R^l to \\C^{n \\times d} that produces an input-dependent matrix",
        "\\hat{\\vy}_i": "Output vector at step i",
        "\\mathrm{dec}": "Learnable (generally non-linear) decoder mapping \\C^{n \\times d} \\times \\R^l to \\R^p",
        "i": "Index over sequence positions",
        "t": "NOT MENTIONED",
        "\\mH_0": "Initial hidden state matrix",
        "\\C": "Set of complex numbers",
        "n": "NOT MENTIONED",
        "d": "NOT MENTIONED",
        "\\R": "Set of real numbers",
        "l": "NOT MENTIONED",
        "p": "NOT MENTIONED"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:01:53.578116"
    },
    {
      "label": "<<FORMULA_0125>>",
      "formula": "\\label{eq:GHset}\n    \\mathcal{M}_{k}^n(\\Omega) : =\\left\\{\\mC_1\\mC_2 \\cdots \\mC_k \\ : \\ \\mC_i = \\mI - \\beta_i \\vv_i \\vv_i^\\top, \\quad (1- \\beta_i) \\in \\Omega, \\quad \\vv_i \\in \\R^{n},\\, \\lVert \\vv_i \\rVert = 1 \\right\\}.",
      "raw_latex": "\\begin{equation}\\label{eq:GHset}\n    \\mathcal{M}_{k}^n(\\Omega) : =\\left\\{\\mC_1\\mC_2 \\cdots \\mC_k \\ : \\ \\mC_i = \\mI - \\beta_i \\vv_i \\vv_i^\\top, \\quad (1- \\beta_i) \\in \\Omega, \\quad \\vv_i \\in \\R^{n},\\, \\lVert \\vv_i \\rVert = 1 \\right\\}. \n\\end{equation}",
      "formula_type": "equation",
      "line_number": 890,
      "is_formula": true,
      "high_level_explanation": "This defines the set of all n-by-n matrices that can be written as a product of k generalized Householder factors. Each factor has the form I − β_i v_i v_i^T, where v_i is a unit vector and the only nontrivial eigenvalue of the factor, 1 − β_i, is constrained to lie in Ω. Thus, the set collects all matrices obtainable by composing k such reflections/scalings along directions v_i with the specified eigenvalue range.",
      "notations": {
        "\\mathcal{M}_{k}^n(\\Omega)": "Set of n×n matrices expressible as a product of k generalized Householder matrices whose nontrivial eigenvalues lie in \\Omega",
        "k": "Number of generalized Householder factors in the product",
        "n": "Ambient dimension of the vectors and matrices",
        "\\Omega": "Allowed set/range for the nontrivial eigenvalue 1 − \\beta_i",
        "\\mC_i": "i-th generalized Householder factor, defined as \\mI - \\beta_i \\vv_i \\vv_i^\\top",
        "\\mI": "Identity matrix in \\R^{n \\times n}",
        "\\beta_i": "Scalar parameter for the i-th factor; sets the nontrivial eigenvalue as 1 − \\beta_i",
        "(1- \\beta_i)": "The nontrivial eigenvalue of \\mC_i (along direction \\vv_i), constrained to lie in \\Omega",
        "\\vv_i": "Unit vector in \\R^{n} that defines the affected direction of the i-th factor",
        "\\R^{n}": "n-dimensional real vector space"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:02:36.735814"
    },
    {
      "label": "<<FORMULA_0215>>",
      "formula": "\\indic{s} := \\begin{cases}\n        1 \\text{ if  $s$ is true} \\\\\n        0 \\text{ if $s$ is false }\n    \\end{cases}, \\qquad \\mathrm{sign}(x) := \\begin{cases}\n        1 &\\text{if } x \\geq 0 \\\\\n        -1 &\\text{if } x > 0\n    \\end{cases}.",
      "raw_latex": "\\begin{equation*}\n    \\indic{s} := \\begin{cases}\n        1 \\text{ if  $s$ is true} \\\\\n        0 \\text{ if $s$ is false }\n    \\end{cases}, \\qquad \\mathrm{sign}(x) := \\begin{cases}\n        1 &\\text{if } x \\geq 0 \\\\\n        -1 &\\text{if } x > 0\n    \\end{cases}.\n\\end{equation*}",
      "formula_type": "equation*",
      "line_number": 1244,
      "is_formula": true,
      "high_level_explanation": "The expression defines two helper functions. The indicator function \\indic{s} returns 1 when the Boolean statement s is true and 0 otherwise. The sign function returns 1 for nonnegative inputs and -1 for negative inputs; note that the given second case \"-1 if x > 0\" appears to be a typographical error and should read \"x < 0\". These definitions compactly encode logical conditions and the polarity of a scalar.",
      "notations": {
        "\\indic{s}": "Indicator of the Boolean condition s (1 if true, 0 if false)",
        "s": "Boolean condition",
        "\\mathrm{sign}(x)": "Sign function of scalar x (1 for x ≥ 0, -1 for x < 0); the snippet's 'x > 0' appears to be a typo",
        "x": "Scalar quantity"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:02:23.630594"
    },
    {
      "label": "<<FORMULA_0356>>",
      "formula": "a_{2k} &= \\sum_{i=1}^n \\hat{c}_i\\binom{2k}{m_i}  |\\lambda_i|^{2k- m_i}, \\quad \\hat{c}_i = c_i\\mathrm{sign}(\\lambda_i)^{2k- m_i} \\\\\na_{2k+1} &= \\sum_{i=1}^n \\tilde{c}_i\\binom{2k+1}{m_i}  |\\lambda_i|^{2k+1- m_i}, \\quad \\tilde{c}_i = c_i \\mathrm{sign}(\\lambda_i)^{2k+1- m_i},",
      "raw_latex": "\\begin{align*}\n    a_{2k} &= \\sum_{i=1}^n \\hat{c}_i\\binom{2k}{m_i}  |\\lambda_i|^{2k- m_i}, \\quad \\hat{c}_i = c_i\\mathrm{sign}(\\lambda_i)^{2k- m_i} \\\\\na_{2k+1} &= \\sum_{i=1}^n \\tilde{c}_i\\binom{2k+1}{m_i}  |\\lambda_i|^{2k+1- m_i}, \\quad \\tilde{c}_i = c_i \\mathrm{sign}(\\lambda_i)^{2k+1- m_i},  \n \\end{align*}",
      "formula_type": "align*",
      "line_number": 1368,
      "is_formula": true,
      "high_level_explanation": "The equations split the sequence a_k into its even and odd subsequences and express each term as a finite sum weighted by binomial coefficients. The magnitude |λ_i| controls growth while the sign of λ_i is absorbed into modified coefficients (\\hat{c}_i for even indices and \\tilde{c}_i for odd indices) via powers of the sign function, making the parity dependence explicit. This reparameterization isolates parity effects and separates sign from magnitude to facilitate analysis when some powers can be negative.",
      "notations": {
        "a_{2k}": "Even-indexed term of the sequence a_k, expressed as a weighted sum over i",
        "a_{2k+1}": "Odd-indexed term of the sequence a_k, expressed as a weighted sum over i",
        "\\hat{c}_i": "Coefficient for the even subsequence absorbing the sign of \\lambda_i via c_i\\,\\mathrm{sign}(\\lambda_i)^{2k- m_i}",
        "\\tilde{c}_i": "Coefficient for the odd subsequence absorbing the sign of \\lambda_i via c_i\\,\\mathrm{sign}(\\lambda_i)^{2k+1- m_i}",
        "c_i": "NOT MENTIONED",
        "m_i": "NOT MENTIONED",
        "\\lambda_i": "NOT MENTIONED",
        "n": "NOT MENTIONED",
        "\\mathrm{sign}(\\lambda_i)": "Sign function of \\lambda_i (returns +1 for nonnegative values and −1 for negative values)"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:02:08.952332"
    },
    {
      "label": "<<FORMULA_0378>>",
      "formula": "\\mA(1)^k = \\mP \\mJ^k \\mP^{-1}, \\quad \\mJ_{i}^k=\\left[\\begin{array}{cccccc}\n\\lambda_i^k & \\binom{k}{1} \\lambda_l^{k-1} & \\binom{k}{2} \\lambda_i^{k-2} & \\cdots & \\cdots & \\binom{k}{k_i-1} \\lambda_i^{k-k_i+1} \\\\\n& \\lambda_i^k & \\binom{k}{1} \\lambda_i^{k-1} & \\cdots & \\cdots & \\binom{k}{k_i-2} \\lambda_i^{k-k_i+2} \\\\\n& & \\ddots & \\ddots & \\vdots & \\vdots \\\\\n& & & \\ddots & \\ddots & \\vdots \\\\\n& & & & \\lambda_i^k & \\binom{k}{1} \\lambda_i^{k-1} \\\\\n& & & & & \\lambda_i^k\n\\end{array}\\right].",
      "raw_latex": "\\begin{equation*}\n     \\mA(1)^k = \\mP \\mJ^k \\mP^{-1}, \\quad \\mJ_{i}^k=\\left[\\begin{array}{cccccc}\n\\lambda_i^k & \\binom{k}{1} \\lambda_l^{k-1} & \\binom{k}{2} \\lambda_i^{k-2} & \\cdots & \\cdots & \\binom{k}{k_i-1} \\lambda_i^{k-k_i+1} \\\\\n& \\lambda_i^k & \\binom{k}{1} \\lambda_i^{k-1} & \\cdots & \\cdots & \\binom{k}{k_i-2} \\lambda_i^{k-k_i+2} \\\\\n& & \\ddots & \\ddots & \\vdots & \\vdots \\\\\n& & & \\ddots & \\ddots & \\vdots \\\\\n& & & & \\lambda_i^k & \\binom{k}{1} \\lambda_i^{k-1} \\\\\n& & & & & \\lambda_i^k\n\\end{array}\\right].\n \\end{equation*}",
      "formula_type": "equation*",
      "line_number": 1394,
      "is_formula": true,
      "high_level_explanation": "The first equality expresses the k-th power of the matrix A(1) via its Jordan decomposition: A(1)^k = P J^k P^{-1}. The second display gives the explicit formula for the k-th power of a Jordan block: the diagonal entries are lambda_i^k and the superdiagonal bands are binomial coefficients times appropriate powers of lambda_i. This representation separates the effect of eigenvalues and the nilpotent structure, making it clear how entries grow or decay with k and facilitating finite-precision analysis.",
      "notations": {
        "\\mA(1)": "Square matrix whose k-th power is analyzed (the matrix being decomposed via Jordan form)",
        "\\mP": "Similarity matrix from the Jordan decomposition (matrix of generalized eigenvectors)",
        "\\mJ": "Block-diagonal Jordan matrix similar to \\mA(1)",
        "\\mJ_{i}^k": "k-th power of the i-th Jordan block",
        "k": "Time index / exponent applied to the matrix power",
        "k_i": "Size (dimension) of the i-th Jordan block",
        "\\lambda_i": "i-th eigenvalue associated with the Jordan block (with multiplicity)",
        "\\lambda_l": "NOT MENTIONED"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:01:19.353346"
    },
    {
      "label": "<<FORMULA_0406>>",
      "formula": "\\label{eq:hoscillating}\n    \\widehat\\mH_{k} := \\begin{cases}\n        \\overline{\\mH}_1 \\quad \\text{if } k \\mod 2 = 0 \\\\\n        \\overline{\\mH}_2 \\quad \\text{otherwise} \n    \\end{cases}, \\quad \\hat y_k  = \\begin{cases}\n        \\bar y_1 \\quad \\text{if } k \\mod 2 = 0 \\\\\n        \\bar y_2 \\quad \\text{otherwise}\n    \\end{cases}",
      "raw_latex": "\\begin{equation}\\label{eq:hoscillating}\n    \\widehat\\mH_{k} := \\begin{cases}\n        \\overline{\\mH}_1 \\quad \\text{if } k \\mod 2 = 0 \\\\\n        \\overline{\\mH}_2 \\quad \\text{otherwise} \n    \\end{cases}, \\quad \\hat y_k  = \\begin{cases}\n        \\bar y_1 \\quad \\text{if } k \\mod 2 = 0 \\\\\n        \\bar y_2 \\quad \\text{otherwise}\n    \\end{cases}\n\\end{equation}",
      "formula_type": "equation",
      "line_number": 1417,
      "is_formula": true,
      "high_level_explanation": "The expression defines the finite-precision hidden state and scalar output at time step k to alternate between two fixed values based on the parity of k. Specifically, for even k the values are \\overline{\\mH}_1 and \\bar y_1, and for odd k they are \\overline{\\mH}_2 and \\bar y_2. This encodes a period-2 oscillation of the system's state and output under finite-precision effects.",
      "notations": {
        "\\widehat\\mH_{k}": "Finite-precision hidden state at time step k",
        "\\overline{\\mH}_1": "NOT MENTIONED",
        "\\overline{\\mH}_2": "NOT MENTIONED",
        "k": "Time step index",
        "\\hat y_k": "Finite-precision scalar output at time step k",
        "\\bar y_1": "NOT MENTIONED",
        "\\bar y_2": "NOT MENTIONED"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:01:11.356155"
    },
    {
      "label": "<<FORMULA_0428>>",
      "formula": "\\widehat\\mC_l := \\mathrm{cast}(\\mA(1)^k \\mB) = \\begin{cases}\n        \\overline\\mC_1 \\text{ if } k \\bmod 2 = 1 \\\\\n        \\overline\\mC_2 \\text{ if } k \\bmod 2 = 0\n    \\end{cases}\n    \\widehat\\mD_k := \\mathrm{cast}(\\mA(1)^k \\mH_0) =  \\begin{cases}\n        \\overline\\mD_1 \\text{ if } k \\bmod 2 = 1 \\\\\n        \\overline\\mD_2 \\text{ if } k \\bmod 2 = 0\n    \\end{cases}",
      "raw_latex": "\\begin{equation*}\n    \\widehat\\mC_l := \\mathrm{cast}(\\mA(1)^k \\mB) = \\begin{cases}\n        \\overline\\mC_1 \\text{ if } k \\bmod 2 = 1 \\\\\n        \\overline\\mC_2 \\text{ if } k \\bmod 2 = 0\n    \\end{cases}\n    \\widehat\\mD_k := \\mathrm{cast}(\\mA(1)^k \\mH_0) =  \\begin{cases}\n        \\overline\\mD_1 \\text{ if } k \\bmod 2 = 1 \\\\\n        \\overline\\mD_2 \\text{ if } k \\bmod 2 = 0\n    \\end{cases}\n\\end{equation*}",
      "formula_type": "equation*",
      "line_number": 1435,
      "is_formula": true,
      "high_level_explanation": "The expression defines two finite-precision (cast) quantities obtained from repeated multiplication by the matrix A(1). After applying the casting operator, the products A(1)^k B and A(1)^k H_0 take one of two fixed matrix values depending on whether k is odd or even. This captures a parity-induced two-cycle in finite precision, which is used to show alternating behavior in subsequent states.",
      "notations": {
        "\\widehat\\mC_l": "Finite-precision matrix obtained by casting \\mA(1)^k \\mB; equals \\overline\\mC_1 for odd k and \\overline\\mC_2 for even k",
        "\\mathrm{cast}": "Finite-precision casting/quantization operator applied to its matrix argument",
        "\\mA(1)": "NOT MENTIONED",
        "\\mB": "NOT MENTIONED",
        "k": "Index of the iteration (used in powers and parity conditions)",
        "\\overline\\mC_1": "Fixed matrix value attained by \\widehat\\mC_l when k is odd",
        "\\overline\\mC_2": "Fixed matrix value attained by \\widehat\\mC_l when k is even",
        "\\widehat\\mD_k": "Finite-precision matrix obtained by casting \\mA(1)^k \\mH_0; equals \\overline\\mD_1 for odd k and \\overline\\mD_2 for even k",
        "\\mH_0": "Initial state of the recurrence",
        "\\overline\\mD_1": "Fixed matrix value attained by \\widehat\\mD_k when k is odd",
        "\\overline\\mD_2": "Fixed matrix value attained by \\widehat\\mD_k when k is even"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:01:34.079579"
    },
    {
      "label": "<<FORMULA_0431>>",
      "formula": "\\widehat\\mH_{2k} &= \\mathrm{cast}\\left(\\sum_{i=1}^{\\tau-1} \\widehat \\mC_i  + \\left(k - \\frac{\\tau}{2} +1\\right) \\overline \\mC_2 + \\left(k - \\frac{\\tau}{2}\\right) \\overline \\mC_1 + k\\overline\\mD_2 \\right) \\\\\n    \\widehat\\mH_{2k+1} &= \\mathrm{cast}\\left(\\sum_{i=1}^{\\tau-1} \\widehat \\mC_i+ \\left(k - \\frac{\\tau}{2} +1\\right)  (\\overline\\mC_2 + \\overline \\mC_1) + k\\overline\\mD_1 \\right)  \\\\",
      "raw_latex": "\\begin{align*}\n    \\widehat\\mH_{2k} &= \\mathrm{cast}\\left(\\sum_{i=1}^{\\tau-1} \\widehat \\mC_i  + \\left(k - \\frac{\\tau}{2} +1\\right) \\overline \\mC_2 + \\left(k - \\frac{\\tau}{2}\\right) \\overline \\mC_1 + k\\overline\\mD_2 \\right) \\\\\n    \\widehat\\mH_{2k+1} &= \\mathrm{cast}\\left(\\sum_{i=1}^{\\tau-1} \\widehat \\mC_i+ \\left(k - \\frac{\\tau}{2} +1\\right)  (\\overline\\mC_2 + \\overline \\mC_1) + k\\overline\\mD_1 \\right)  \\\\\n\\end{align*}",
      "formula_type": "align*",
      "line_number": 1446,
      "is_formula": true,
      "high_level_explanation": "These equations express the finite-precision state matrices at even and odd time indices as the quantized result of a sum of a finite transient term and terms that grow linearly with the index k. The constants multiplying k are fixed matrices that come from the two possible cast outcomes and alternate with parity. The form is used to argue that, under finite-precision casting, entries become constant or saturate for large k, yielding an oscillatory (period-2) behavior.",
      "notations": {
        "\\widehat\\mH_{2k}": "Finite-precision state matrix at even time index 2k",
        "\\widehat\\mH_{2k+1}": "Finite-precision state matrix at odd time index 2k+1",
        "\\mathrm{cast}": "Finite-precision casting/quantization operator applied elementwise to its matrix argument",
        "\\widehat\\mC_i": "NOT MENTIONED",
        "\\overline\\mC_2": "One of two constant finite-precision matrices obtained by casting the product A(1)^k B when k is even",
        "\\overline\\mC_1": "One of two constant finite-precision matrices obtained by casting the product A(1)^k B when k is odd",
        "\\overline\\mD_2": "Constant finite-precision matrix obtained by casting the product A(1)^k H_0 when k is even",
        "\\overline\\mD_1": "Constant finite-precision matrix obtained by casting the product A(1)^k H_0 when k is odd",
        "k": "Time-step index (integer)",
        "\\tau": "NOT MENTIONED"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:01:46.169255"
    },
    {
      "label": "<<FORMULA_0457>>",
      "formula": "\\widehat\\mH_{2k}^{(2)} = \\begin{cases}\n        \\overline\\mH^{(2)}_1 \\text{ if } k \\bmod 2 = 0 \\\\\n        \\overline\\mH^{(2)}_2 \\text{ if } k \\bmod 2 = 1 \n    \\end{cases}, \\quad     \\widehat\\mH_{2k - 1}^{(2)} = \\begin{cases}\n        \\overline\\mH^{(2)}_3 \\text{ if } k \\bmod 2 = 0 \\\\\n        \\overline\\mH^{(2)}_4 \\text{ if } k \\bmod 2 = 1 \n    \\end{cases}.",
      "raw_latex": "\\begin{equation*}\n    \\widehat\\mH_{2k}^{(2)} = \\begin{cases}\n        \\overline\\mH^{(2)}_1 \\text{ if } k \\bmod 2 = 0 \\\\\n        \\overline\\mH^{(2)}_2 \\text{ if } k \\bmod 2 = 1 \n    \\end{cases}, \\quad     \\widehat\\mH_{2k - 1}^{(2)} = \\begin{cases}\n        \\overline\\mH^{(2)}_3 \\text{ if } k \\bmod 2 = 0 \\\\\n        \\overline\\mH^{(2)}_4 \\text{ if } k \\bmod 2 = 1 \n    \\end{cases}.\n\\end{equation*}",
      "formula_type": "equation*",
      "line_number": 1480,
      "is_formula": true,
      "high_level_explanation": "The expression defines the finite-precision hidden states of the second layer at even and odd time indices as piecewise-constant values that alternate with the parity of k. Specifically, even-indexed states switch between two fixed matrices depending on whether k is even or odd, and the odd-indexed states likewise switch between another two fixed matrices. This structure implies a periodic pattern with overall period dividing four.",
      "notations": {
        "\\widehat\\mH_{2k}^{(2)}": "Finite-precision hidden state (matrix) of the second layer at time index 2k",
        "\\widehat\\mH_{2k - 1}^{(2)}": "Finite-precision hidden state (matrix) of the second layer at time index 2k-1",
        "\\overline\\mH^{(2)}_1": "NOT MENTIONED",
        "\\overline\\mH^{(2)}_2": "NOT MENTIONED",
        "\\overline\\mH^{(2)}_3": "NOT MENTIONED",
        "\\overline\\mH^{(2)}_4": "NOT MENTIONED",
        "k": "Time-step index (integer)"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:01:08.904286"
    },
    {
      "label": "<<FORMULA_0479>>",
      "formula": "\\mR(\\theta) := \\Big[\\begin{array}{cc}\n\\cos \\theta & -\\sin \\theta \\\\\n\\sin \\theta & \\cos \\theta\n\\end{array}\\Big] = \\left(\\mI + 2\\vv_1\\vv_1^\\top\\right)\\left(\\mI - 2\\vv_2\\vv_2^\\top\\right), \\quad \\theta = \\frac{2\\pi}{m}.",
      "raw_latex": "\\begin{equation*}\n    \\mR(\\theta) := \\Big[\\begin{array}{cc}\n\\cos \\theta & -\\sin \\theta \\\\\n\\sin \\theta & \\cos \\theta\n\\end{array}\\Big] = \\left(\\mI + 2\\vv_1\\vv_1^\\top\\right)\\left(\\mI - 2\\vv_2\\vv_2^\\top\\right), \\quad \\theta = \\frac{2\\pi}{m}.\n\\end{equation*}",
      "formula_type": "equation*",
      "line_number": 1500,
      "is_formula": true,
      "high_level_explanation": "The expression defines the 2D rotation matrix by an angle theta and shows it can be factored as the product of two generalized Householder (reflection) matrices. Setting theta to 2π divided by m makes the rotation have order m, so repeated application cycles through m distinct orientations. This factorization underlies constructing a state-transition that counts modulo m via planar rotations.",
      "notations": {
        "\\mR(\\theta)": "2D rotation matrix by angle theta",
        "\\theta": "Rotation angle set to 2π/m",
        "m": "Modulus; positive integer determining the rotation step (2π/m radians)",
        "\\mI": "2×2 identity matrix",
        "\\vv_1": "Unit-norm vector (in \\mathbb{R}^2) used to define a GH factor",
        "\\vv_2": "Unit-norm vector (in \\mathbb{R}^2) used to define a GH factor",
        "(\\mI + 2\\vv_1\\vv_1^\\top)": "First generalized Householder matrix (reflection-type factor)",
        "(\\mI - 2\\vv_2\\vv_2^\\top)": "Second generalized Householder matrix (reflection-type factor)"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:02:50.982025"
    },
    {
      "label": "<<FORMULA_0507>>",
      "formula": "\\mP = \\prod_{l=1}^{k-1} \\mP_{i_l j_l}, \\quad, \\mP_{ij} = (I - 2\\vv_{ij} \\vv_{ij}^\\top) \\qquad v_{ijl} = \\begin{cases}\n       1/\\sqrt{2} \\quad \\text{ if } l = i \\\\\n       -1/\\sqrt{2} \\quad \\text{ if } l = j \\\\\n       0 \\quad \\text{ otherwise} \n   \\end{cases},",
      "raw_latex": "\\begin{equation*}\n   \\mP = \\prod_{l=1}^{k-1} \\mP_{i_l j_l}, \\quad, \\mP_{ij} = (I - 2\\vv_{ij} \\vv_{ij}^\\top) \\qquad v_{ijl} = \\begin{cases}\n       1/\\sqrt{2} \\quad \\text{ if } l = i \\\\\n       -1/\\sqrt{2} \\quad \\text{ if } l = j \\\\\n       0 \\quad \\text{ otherwise} \n   \\end{cases},  \n\\end{equation*}",
      "formula_type": "equation*",
      "line_number": 1532,
      "is_formula": true,
      "high_level_explanation": "The expression writes a permutation matrix \\mP as a product of pairwise swap matrices. Each swap matrix \\mP_{ij} is realized as a Householder (GH) reflection of the form I - 2\\vv_{ij}\\vv_{ij}^\\top, where the vector \\vv_{ij} has nonzero entries only in the two coordinates being swapped. This construction shows how transpositions (and hence any permutation) can be implemented using reflections.",
      "notations": {
        "\\mP": "Permutation matrix expressed as a product of pairwise swaps",
        "k": "NOT MENTIONED",
        "\\mP_{i_l j_l": "The l-th transposition (swap) matrix acting on coordinates i_l and j_l",
        "\\mP_{ij}": "Householder/GH reflection matrix defined by I - 2\\vv_{ij}\\vv_{ij}^\\top; it swaps coordinates i and j",
        "\\vv_{ij}": "Unit vector with entries 1/\\sqrt{2} at position i and -1/\\sqrt{2} at position j, zeros elsewhere",
        "v_{ijl}": "l-th component of \\vv_{ij} (component index l)",
        "i_l": "First index involved in the l-th swap",
        "j_l": "Second index involved in the l-th swap",
        "I": "Identity matrix"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:02:31.217061"
    },
    {
      "label": "<<FORMULA_0643>>",
      "formula": "\\begin{aligned}\n    \\mA^{(i)}(w)_{q',q}  &= \\indic{\\delta(q, w)= q'}, && \\mB^{(i)}(w)_{q'} = 0,  \\quad  &&\\text{ if } \\delta^{(i)}(\\cdot, w) \\text{ is bijective, or} \\\\\n    \\mA^{(i)}(w)_{q',q}  &= 0, && \\mB^{(i)}(w)_{q'} = \\indic{q' = q(w)},  &&\\text{ if } \\delta^{(i)}(\\cdot, w) \\equiv q(w).\n\\end{aligned}",
      "raw_latex": "\\begin{equation*}   \n\\begin{aligned}\n    \\mA^{(i)}(w)_{q',q}  &= \\indic{\\delta(q, w)= q'}, && \\mB^{(i)}(w)_{q'} = 0,  \\quad  &&\\text{ if } \\delta^{(i)}(\\cdot, w) \\text{ is bijective, or} \\\\\n    \\mA^{(i)}(w)_{q',q}  &= 0, && \\mB^{(i)}(w)_{q'} = \\indic{q' = q(w)},  &&\\text{ if } \\delta^{(i)}(\\cdot, w) \\equiv q(w).\n\\end{aligned}\n\\end{equation*}",
      "formula_type": "equation*",
      "line_number": 1621,
      "is_formula": true,
      "high_level_explanation": "This piecewise definition constructs the matrices/vectors used to implement a permutation-reset automaton within a linear RNN layer. If the per-symbol transition δ^{(i)}(·, w) is a bijection (a permutation over states), the matrix A^{(i)}(w) becomes the corresponding permutation matrix and B^{(i)}(w) is zero. If the transition is a constant map (reset), A^{(i)}(w) is the zero matrix while B^{(i)}(w) is the one-hot vector selecting the reset state q(w). The indicator functions set the appropriate entries to 1 or 0 in each case.",
      "notations": {
        "\\mA^{(i)}(w)_{q',q}": "(q', q)-entry of the state-transition matrix for automaton/layer i when reading symbol w",
        "\\mB^{(i)}(w)_{q'}": "q'-th entry of the reset/bias vector for automaton/layer i when reading symbol w",
        "\\delta^{(i)}(\\cdot, w)": "Per-symbol state mapping of automaton i induced by input w (a function from states to states)",
        "\\delta(q, w)": "State reached when applying the transition function to state q with input w",
        "\\indic{\\delta(q, w)= q'}": "Indicator that equals 1 if the transition from q on input w is q', and 0 otherwise",
        "\\indic{q' = q(w)}": "Indicator that equals 1 if q' equals the designated reset state q(w) for input w, 0 otherwise",
        "q(w)": "Reset state chosen for input w when the transition is constant",
        "q": "Current state in the state set Q^{(i)}",
        "q'": "Next state in the state set Q^{(i)}",
        "w": "Input symbol from the automaton alphabet"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:01:56.195899"
    },
    {
      "label": "<<FORMULA_0647>>",
      "formula": "\\mH^{(i)}_t &= \\mA^{(i)}(w^{(i)}_t) \\mH^{(i)}_{t-1} - \\mB^{(i)}(w^{(i)}_t), \\qquad  \\mH^{(i)}_0 = (1,0\\dots,0)^\\top \\in \\R^n \\\\  \n     y^{(i)} &= \\mathrm{dec}^{(i)}(\\mH_t^{(i)}, w_t^{(i)})   = (g(\\mH_t^{(i)}), w_t^{(i)}) = (\\delta^{(i)}(q_0^{(i)}, \\vw^{(i)}), w^{(i)})",
      "raw_latex": "\\begin{align*}\n     \\mH^{(i)}_t &= \\mA^{(i)}(w^{(i)}_t) \\mH^{(i)}_{t-1} - \\mB^{(i)}(w^{(i)}_t), \\qquad  \\mH^{(i)}_0 = (1,0\\dots,0)^\\top \\in \\R^n \\\\  \n     y^{(i)} &= \\mathrm{dec}^{(i)}(\\mH_t^{(i)}, w_t^{(i)})   = (g(\\mH_t^{(i)}), w_t^{(i)}) = (\\delta^{(i)}(q_0^{(i)}, \\vw^{(i)}), w^{(i)})\n\\end{align*}",
      "formula_type": "align*",
      "line_number": 1629,
      "is_formula": true,
      "high_level_explanation": "The equations define the recurrence and decoding of the i-th layer of a linear RNN that simulates a finite-state automaton. The hidden state updates linearly using an input-dependent transition matrix and an input-dependent offset (implementing permutation or reset), starting from a one-hot initial state. The decoder outputs a pair consisting of the automaton state inferred from the hidden state and the current input symbol; this state equals the result of applying the automaton’s transition function to the initial state on the input word processed so far.",
      "notations": {
        "\\mH^{(i)}_t": "Hidden state vector of layer i at time t",
        "\\mA^{(i)}(w^{(i)}_t)": "Input-dependent state transition matrix for layer i evaluated at the symbol w^{(i)}_t; constructed to implement the automaton transition (permutation when the transition is bijective)",
        "w^{(i)}_t": "Input symbol to layer i at time t",
        "\\mH^{(i)}_{t-1}": "Hidden state vector of layer i at time t−1",
        "\\mB^{(i)}(w^{(i)}_t)": "Input-dependent offset vector for layer i evaluated at w^{(i)}_t; implements reset when the transition is constant",
        "\\mH^{(i)}_0": "Initial hidden state of layer i",
        "(1,0\\dots,0)^\\top": "First canonical basis vector (one-hot) used as the initial state",
        "\\R^n": "n-dimensional real vector space",
        "y^{(i)}": "Output of layer i at time t",
        "\\mathrm{dec}^{(i)}(\\mH_t^{(i)}, w_t^{(i)})": "Decoder of layer i that maps the current hidden state and input symbol to the layer’s output",
        "g(\\mH_t^{(i)})": "Mapping from the hidden state to the corresponding automaton state",
        "\\delta^{(i)}(q_0^{(i)}, \\vw^{(i)})": "Automaton transition function of \\mathcal{A}^{(i)} extended to the input word \\vw^{(i)}, applied from the initial state",
        "q_0^{(i)}": "Initial state of automaton \\mathcal{A}^{(i)}",
        "\\vw^{(i)}": "Input word for layer i (the sequence processed so far)",
        "w_t^{(i)}": "Same as w^{(i)}_t (current input symbol to layer i at time t)",
        "w^{(i)}": "Current input symbol carried in the output pair"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:02:40.228073"
    },
    {
      "label": "<<FORMULA_0652>>",
      "formula": "y^{(s)}_t &= \\mathrm{dec}^{(s)}(\\mH_t, w_t^{(s)}) \\\\\n&= (\\delta^{(s)}(q_0^{(s)}, \\vw^{(s)}),y_t^{(s-1)}) \\\\\n&= (\\delta^{(s)}(q_0^{(s)}, \\vw^{(s)}), \\delta^{(s-1)}(q_0^{(s-1)}, \\vw^{(s-1)}), y_t^{(s-2)}) \\\\   \n&= (\\delta^{(s)}(q_0^{(s)}, \\vw^{(s)}),\\dots, \\delta^{(1)}(q_0^{(1)}, \\vw), w_t) \\in \\N^{s+1},",
      "raw_latex": "\\begin{align*}\ny^{(s)}_t &= \\mathrm{dec}^{(s)}(\\mH_t, w_t^{(s)}) \\\\\n&= (\\delta^{(s)}(q_0^{(s)}, \\vw^{(s)}),y_t^{(s-1)}) \\\\\n&= (\\delta^{(s)}(q_0^{(s)}, \\vw^{(s)}), \\delta^{(s-1)}(q_0^{(s-1)}, \\vw^{(s-1)}), y_t^{(s-2)}) \\\\   \n&= (\\delta^{(s)}(q_0^{(s)}, \\vw^{(s)}),\\dots, \\delta^{(1)}(q_0^{(1)}, \\vw), w_t) \\in \\N^{s+1},\n\\end{align*}",
      "formula_type": "align*",
      "line_number": 1634,
      "is_formula": true,
      "high_level_explanation": "The expression defines the s-th output vector y_t^{(s)} as the output of a decoder applied to the hidden state and its input, and then unrolls this definition recursively. Unrolling shows that y_t^{(s)} is the concatenation of the states reached by each automaton in the cascade, namely δ^{(i)}(q_0^{(i)}, v^{(i)}) for i = s, s−1, …, 1, followed by the current input symbol w_t. The result lies in a discrete space of dimension s+1.",
      "notations": {
        "y^{(s)}_t": "Output vector at time t from the s-th decoder/layer (concatenation of s automaton states and the current input symbol)",
        "\\mathrm{dec}^{(s)}": "s-th decoder function mapping its hidden state and input to the output vector",
        "\\mH_t": "NOT MENTIONED",
        "w_t^{(s)}": "Input provided to the s-th decoder at time t",
        "\\delta^{(s)}": "State-transition function of the s-th automaton in the cascade",
        "q_0^{(s)}": "Initial state of the s-th automaton",
        "\\vw^{(s)}": "Input word (sequence) for the s-th automaton",
        "y_t^{(s-1)}": "Output vector at time t from level s−1 of the cascade",
        "\\delta^{(s-1)}": "State-transition function of the (s−1)-th automaton",
        "q_0^{(s-1)}": "Initial state of the (s−1)-th automaton",
        "\\vw^{(s-1)}": "Input word for the (s−1)-th automaton",
        "y_t^{(s-2)}": "Output vector at time t from level s−2 of the cascade",
        "\\delta^{(1)}": "State-transition function of the first automaton",
        "q_0^{(1)}": "Initial state of the first automaton",
        "\\vw": "Input word to the first automaton",
        "w_t": "Current input symbol at time t",
        "\\N^{s+1}": "NOT MENTIONED"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:02:19.298708"
    },
    {
      "label": "<<FORMULA_0681>>",
      "formula": "\\vh^{(2)}_{0} = (1,0)^\\top, \\quad  \\vh^{(2)}_t = \\mA^{(2)}(\\vy^{(1)}_t) \\vh^{(2)}_{t + 1}, \\quad \\vy^{(2)}_t = \\mathrm{dec}^{(2)}(\\vh_t^{(2)}, \\vy^{(1)}_t)  \\\\\n    \\mA^{(2)}(\\vy) = \\mH(\\theta(y_1, y_2)) = \\left[\\begin{array}{cc}\n\\cos \\theta(y_1, y_2) &  \\sin \\theta(y_1, y_2) \\\\\n\\sin \\theta(y_1, y_2) & -\\cos \\theta(y_1, y_2)\n\\end{array}\\right] \\\\\n    \\mathrm{dec}^{(2)}(\\vh, \\vy) = \\argmax_{i \\in \\{0, \\dots, m-1\\}} \\max (\\vc_i^\\top \\vh, \\vd_i^\\top \\vh) \\quad",
      "raw_latex": "\\begin{gather*}\n    \\vh^{(2)}_{0} = (1,0)^\\top, \\quad  \\vh^{(2)}_t = \\mA^{(2)}(\\vy^{(1)}_t) \\vh^{(2)}_{t + 1}, \\quad \\vy^{(2)}_t = \\mathrm{dec}^{(2)}(\\vh_t^{(2)}, \\vy^{(1)}_t)  \\\\\n    \\mA^{(2)}(\\vy) = \\mH(\\theta(y_1, y_2)) = \\left[\\begin{array}{cc}\n\\cos \\theta(y_1, y_2) &  \\sin \\theta(y_1, y_2) \\\\\n\\sin \\theta(y_1, y_2) & -\\cos \\theta(y_1, y_2)\n\\end{array}\\right] \\\\\n    \\mathrm{dec}^{(2)}(\\vh, \\vy) = \\argmax_{i \\in \\{0, \\dots, m-1\\}} \\max (\\vc_i^\\top \\vh, \\vd_i^\\top \\vh) \\quad \n\\end{gather*}",
      "formula_type": "gather*",
      "line_number": 1667,
      "is_formula": true,
      "high_level_explanation": "These equations specify the second layer of a linear RNN whose 2D hidden state evolves via time-varying reflection matrices. At each time t, the state update uses a reflection matrix A^{(2)}(y_t^{(1)}) = H(θ(y_1, y_2)) parameterized by the first-layer signal, and the layer outputs an index selected by a decoder that maximizes alignment with template vectors {c_i} and {d_i}. This construction underlies an implementation of modular addition over the cyclic group Z_m by alternating reflections determined by the first-layer outputs.",
      "notations": {
        "\\vh^{(2)}_{0}": "Initial hidden state of the second layer",
        "(1,0)^\\top": "Initial 2D state vector",
        "\\vh^{(2)}_t": "2D hidden state of the second layer at time t",
        "\\vy^{(1)}_t": "Output of the first layer at time t (control input to the second layer)",
        "\\vy^{(2)}_t": "Decoded output of the second layer at time t (an index in {0, ..., m-1})",
        "\\mA^{(2)}(\\vy)": "Second-layer state-transition matrix as a function of input \\vy",
        "\\mH(\\theta(y_1, y_2))": "2×2 reflection matrix parameterized by the angle \\theta(y_1, y_2)",
        "\\theta(y_1, y_2)": "Angle that determines the reflection; exact definition NOT MENTIONED",
        "y_1": "First component of \\vy used by \\theta; NOT MENTIONED",
        "y_2": "Second component of \\vy used by \\theta; NOT MENTIONED",
        "\\mathrm{dec}^{(2)}(\\vh, \\vy)": "Decoder that maps (state, input) to an index by argmax over max(c_i^\\top h, d_i^\\top h)",
        "\\vc_i": "i-th state vector in set \\mathcal{C} (associated with reflections), used as a decoding template",
        "\\vd_i": "i-th state vector in set \\mathcal{D} (associated with rotations), used as a decoding template",
        "m": "Modulus/size of the cyclic group \\mathbb{Z}_m (number of classes/states)"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:01:41.034574"
    },
    {
      "label": "<<FORMULA_0694>>",
      "formula": "\\begin{aligned}\n\\mR(\\alpha) \\mR(\\gamma) & =\\mR(\\alpha + \\gamma), \\quad \n &&\\mH(\\alpha)\\mH(\\gamma)  =\\mR(\\alpha- \\gamma), \\\\\n\\mR(\\alpha) \\mH(\\gamma) & =\\mH\\left(\\alpha+ \\gamma\\right) \n&&\\mH(\\gamma) \\mR(\\alpha)  =\\mH\\\\geft(\\gamma-\\alpha\\right).\n\\end{aligned}",
      "raw_latex": "\\begin{align*}\n\\begin{aligned}\n\\mR(\\alpha) \\mR(\\gamma) & =\\mR(\\alpha + \\gamma), \\quad \n &&\\mH(\\alpha)\\mH(\\gamma)  =\\mR(\\alpha- \\gamma), \\\\\n\\mR(\\alpha) \\mH(\\gamma) & =\\mH\\left(\\alpha+ \\gamma\\right) \n&&\\mH(\\gamma) \\mR(\\alpha)  =\\mH\\\\geft(\\gamma-\\alpha\\right).\n\\end{aligned}\n\\end{align*}",
      "formula_type": "align*",
      "line_number": 1687,
      "is_formula": true,
      "high_level_explanation": "These are composition identities for 2D rotations and reflections. Multiplying two rotations adds their angles, multiplying two reflections yields a rotation by the difference of their angles, and composing a rotation with a reflection gives a reflection whose parameter is the sum (or difference, depending on order) of the angles. These equalities encode the group laws in the planar orthogonal group and let one reduce a product of rotations/reflections to a single rotation or reflection with an appropriate angle.",
      "notations": {
        "\\mR(\\alpha)": "2×2 rotation matrix by angle \\alpha",
        "\\mH(\\alpha)": "2×2 reflection matrix about a line whose orientation is parameterized by angle \\alpha",
        "\\alpha": "angle parameter (in radians)",
        "\\gamma": "angle parameter (in radians)"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:02:16.269401"
    },
    {
      "label": "<<FORMULA_0695>>",
      "formula": "\\label{eq:dtoc}\n\\begin{aligned}\n     \\mH(\\theta(j,1))\\vd_i &= \\mH(\\theta(j,1))\\mR(2i\\pi/m)\\vd_0 \\\\\n     &= \\mH(\\theta(j,1))\\mR(2i\\pi/m)\\mH(\\pi/m)\\vc_0 \\\\\n     &= \\mH(\\theta(j,1))\\mH(\\theta(i,0))\\vc_0 \\\\\n     &= \\mR( \\theta(j,1) - \\theta(i,0))\\vc_0 \\\\\n     &= \\mR(-2(i+j)\\pi/m)\\vc_0 = \\vc_{i+j \\bmod m},\n\\end{aligned}",
      "raw_latex": "\\begin{equation}\\label{eq:dtoc}\n\\begin{aligned}\n     \\mH(\\theta(j,1))\\vd_i &= \\mH(\\theta(j,1))\\mR(2i\\pi/m)\\vd_0 \\\\\n     &= \\mH(\\theta(j,1))\\mR(2i\\pi/m)\\mH(\\pi/m)\\vc_0 \\\\\n     &= \\mH(\\theta(j,1))\\mH(\\theta(i,0))\\vc_0 \\\\\n     &= \\mR( \\theta(j,1) - \\theta(i,0))\\vc_0 \\\\\n     &= \\mR(-2(i+j)\\pi/m)\\vc_0 = \\vc_{i+j \\bmod m},\n\\end{aligned}    \n\\end{equation}",
      "formula_type": "equation",
      "line_number": 1696,
      "is_formula": true,
      "high_level_explanation": "This chain of equalities shows that applying the reflection matrix with angle θ(j,1) to the state vector d_i is equivalent to rotating the base vector c_0 by −2(i+j)π/m, which equals the state c_{i+j mod m}. Thus, the action implements addition of indices modulo m when moving from the D-states to the C-states. The derivation uses standard 2D rotation/reflection composition identities together with the specific angle choice θ and the definitions of the state sets C and D.",
      "notations": {
        "\\mH(\\theta(j,1))": "2×2 reflection matrix whose reflection line is specified by the angle value \\theta(j,1)",
        "\\vd_i": "i-th state vector in the 'reflection' set D (in R^2)",
        "\\mR(2i\\pi/m)": "2D rotation matrix by angle 2 i π / m",
        "\\vd_0": "reference (index-0) state vector in the 'reflection' set D",
        "\\mH(\\pi/m)": "2×2 reflection matrix with angle π / m",
        "\\vc_0": "reference (index-0) state vector in the 'rotation' set C",
        "\\mH(\\theta(i,0))": "2×2 reflection matrix whose reflection line is specified by the angle value \\theta(i,0)",
        "\\mR(\\theta(j,1) - \\theta(i,0))": "2D rotation matrix by the angle difference θ(j,1) − θ(i,0)",
        "\\mR(-2(i+j)\\pi/m)": "2D rotation matrix by angle −2(i + j) π / m",
        "\\vc_{i+j \\\\bmod m}": "(i + j mod m)-th state vector in the 'rotation' set C (in R^2)",
        "i": "state/index variable for modular addition (integer modulo m)",
        "j": "index/input variable determining the reflection angle (integer modulo m)",
        "m": "modulus (size of the cyclic group for addition)",
        "\\theta(j,1)": "angle function used to parameterize reflections in the second layer (defined in the paper)",
        "\\theta(i,0)": "angle function used to parameterize reflections in the second layer (defined in the paper)",
        "\\pi": "the mathematical constant pi (≈ 3.14159)"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:01:42.634740"
    },
    {
      "label": "<<FORMULA_0696>>",
      "formula": "\\label{eq:ctod}\n\\begin{aligned}\n     \\mH(\\theta(j,0))\\vc_i &= \\mH(\\theta(j,1))\\mR(-2i\\pi/m)\\vc_0 \\\\\n     &= \\mH(\\theta(j,0))\\mR(-2i\\pi/m)\\mH(\\pi/m)\\vd_0 \\\\\n      &= \\mH(\\theta(j,0))\\mH(\\theta(i,1))\\vd_0 \\\\\n     &= \\mR( \\theta(j,0) - \\theta(i,1))\\vd_0 \\\\\n     &= \\mR(2(i+j)\\pi/m)\\vd_0 = \\vd_{i - j \\bmod m},\n\\end{aligned}",
      "raw_latex": "\\begin{equation}\\label{eq:ctod}\n\\begin{aligned}\n     \\mH(\\theta(j,0))\\vc_i &= \\mH(\\theta(j,1))\\mR(-2i\\pi/m)\\vc_0 \\\\\n     &= \\mH(\\theta(j,0))\\mR(-2i\\pi/m)\\mH(\\pi/m)\\vd_0 \\\\\n      &= \\mH(\\theta(j,0))\\mH(\\theta(i,1))\\vd_0 \\\\\n     &= \\mR( \\theta(j,0) - \\theta(i,1))\\vd_0 \\\\\n     &= \\mR(2(i+j)\\pi/m)\\vd_0 = \\vd_{i - j \\bmod m},\n\\end{aligned} \n\\end{equation}",
      "formula_type": "equation",
      "line_number": 1706,
      "is_formula": true,
      "high_level_explanation": "This chain of equalities shows how applying the reflection H(θ(j,0)) to the state vector c_i yields the state d_{i − j mod m}. It rewrites c_i via rotations of a base vector and a fixed bridging reflection H(π/m), then uses identities for products of 2D reflections/rotations to collapse the composition into a single rotation that corresponds to subtracting j modulo m. This is the c-to-d counterpart of the earlier d-to-c relation and implements modular index updates within the LRNN’s second layer.",
      "notations": {
        "\\mH(\\theta(j,0))": "2×2 reflection matrix parameterized by the angle \\theta(j,0) (reflects vectors about a line at that angle)",
        "\\mH(\\theta(j,1))": "2×2 reflection matrix parameterized by the angle \\theta(j,1)",
        "\\mH(\\pi/m)": "Reflection matrix with angle \\pi/m (used to map between the two state families)",
        "\\mH(\\theta(i,1))": "2×2 reflection matrix parameterized by the angle \\theta(i,1)",
        "\\mR(-2i\\pi/m)": "2D rotation matrix by angle −2i\\pi/m",
        "\\mR( \\theta(j,0) - \\theta(i,1))": "2D rotation matrix by angle \\theta(j,0) − \\theta(i,1)",
        "\\mR(2(i+j)\\pi/m)": "2D rotation matrix by angle 2(i+j)\\pi/m",
        "\\vc_i": "i-th state vector in the rotation-family (c) set used by the second layer; index taken modulo m",
        "\\vc_0": "Reference/base vector of the rotation-family (c) set",
        "\\vd_0": "Reference/base vector of the reflection-family (d) set",
        "\\vd_{i - j \\bmod m}": "State vector in the reflection-family (d) set with index i − j modulo m",
        "\\theta(j,0)": "Reflection-angle function evaluated at (j,0) that determines the reflection angle (defined earlier in the paper)",
        "\\theta(j,1)": "Reflection-angle function evaluated at (j,1) that determines the reflection angle (defined earlier in the paper)",
        "\\theta(i,1)": "Reflection-angle function evaluated at (i,1) that determines the reflection angle (defined earlier in the paper)",
        "i": "Index in {0, …, m−1} labeling states",
        "j": "Index/operand in {0, …, m−1} used to parameterize the reflection and update the state",
        "m": "Modulus (size of the cyclic group for addition mod m)"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:02:11.258392"
    },
    {
      "label": "<<FORMULA_0703>>",
      "formula": "\\vh_1^{(2)} &= \\mH(\\theta(x_1, 1)) \\vh_0^{(2)} = \\mH(\\theta(x_1, 1))  \\vd_0  = \\vc_{x_1 \\bmod m} =  \\vc_{y_1}  \\\\\n    \\vh_2^{(2)}  &= \\mH(\\theta(x_2, 0)) \\vh_1^{(2)} = \\mH(\\theta(x_2, 0)) \\vc_{x_1 \\bmod m} = \\vd_{x_1 - x_2 \\bmod m} = \\vd_{y_2},",
      "raw_latex": "\\begin{align*}\n    \\vh_1^{(2)} &= \\mH(\\theta(x_1, 1)) \\vh_0^{(2)} = \\mH(\\theta(x_1, 1))  \\vd_0  = \\vc_{x_1 \\bmod m} =  \\vc_{y_1}  \\\\\n    \\vh_2^{(2)}  &= \\mH(\\theta(x_2, 0)) \\vh_1^{(2)} = \\mH(\\theta(x_2, 0)) \\vc_{x_1 \\bmod m} = \\vd_{x_1 - x_2 \\bmod m} = \\vd_{y_2},\n\\end{align*}",
      "formula_type": "align*",
      "line_number": 1725,
      "is_formula": true,
      "high_level_explanation": "These equalities give the first two transitions of the second-layer hidden state in the LRNN that performs modular arithmetic using angle-controlled reflections. After the first input x1, the state becomes the C-state indexed by x1 mod m; after the second input x2, it becomes the D-state indexed by x1 − x2 mod m. This establishes the base cases for the induction showing the layer alternates between C and D state sets while accumulating modular sums/differences via reflection matrices parameterized by θ.",
      "notations": {
        "\\vh_1^{(2)}": "Second-layer hidden state after processing the first input (a 2D vector).",
        "\\mH(\\theta(x_1, 1))": "2×2 reflection matrix parameterized by the angle \\theta(x_1, 1) (the second argument selects one of two reflections based on the parity signal).",
        "\\vh_0^{(2)}": "Initial second-layer hidden state; equal to (1, 0)^\\top.",
        "\\vd_0": "Base vector in the D-state set (rotation-associated states).",
        "\\vc_{x_1 \\bmod m}": "C-state indexed by x_1 mod m (reflection-associated state).",
        "\\vc_{y_1}": "C-state indexed by y_1, where y_1 = x_1 \\bmod m.",
        "\\vh_2^{(2)}": "Second-layer hidden state after processing the second input.",
        "\\mH(\\theta(x_2, 0))": "2×2 reflection matrix parameterized by the angle \\theta(x_2, 0) (the second argument selects one of two reflections based on the parity signal).",
        "\\vd_{x_1 - x_2 \\bmod m}": "D-state indexed by x_1 − x_2 mod m (rotation-associated state).",
        "\\vd_{y_2}": "D-state indexed by y_2, where y_2 = x_1 − x_2 \\bmod m.",
        "x_1": "NOT MENTIONED",
        "x_2": "NOT MENTIONED",
        "m": "Modulus for addition; also determines the number of residue classes (the second layer has 2m states).",
        "\\theta(x_1, 1)": "Angle used to define the reflection; determined by inputs (exact formula not provided here).",
        "\\theta(x_2, 0)": "Angle used to define the reflection; determined by inputs (exact formula not provided here).",
        "y_1": "Equal to x_1 \\bmod m (from the displayed equalities).",
        "y_2": "Equal to x_1 − x_2 \\bmod m (from the displayed equalities)."
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:03:17.904099"
    },
    {
      "label": "<<FORMULA_0706>>",
      "formula": "\\vh_{i+1}^{(2)} = \\begin{cases}\n        \\mH(\\theta(x_{i+1}, 1))  \\vh_{i}^{(2)} = \\mH(\\theta(x_{i+1}, 1))  \\vc_{y_i} = \\vc_{x_{i+1} + y_i \\bmod m} = \\vc_{y_{i+1}}  \\text{ if } i \\bmod 2 = 1\\\\\n        \\mH(\\theta(x_{i+1}, 0))  \\vh_{i}^{(2)} = \\mH(\\theta(x_{i+1}, 0))  \\vd_{s_i} = \\vd_{x_{i+1} + y_i \\bmod m} = \\vd_{y_{i - 1}} \\text{ if } i \\bmod 2 = 0\n    \\end{cases}.",
      "raw_latex": "\\begin{equation*}\n    \\vh_{i+1}^{(2)} = \\begin{cases}\n        \\mH(\\theta(x_{i+1}, 1))  \\vh_{i}^{(2)} = \\mH(\\theta(x_{i+1}, 1))  \\vc_{y_i} = \\vc_{x_{i+1} + y_i \\bmod m} = \\vc_{y_{i+1}}  \\text{ if } i \\bmod 2 = 1\\\\\n        \\mH(\\theta(x_{i+1}, 0))  \\vh_{i}^{(2)} = \\mH(\\theta(x_{i+1}, 0))  \\vd_{s_i} = \\vd_{x_{i+1} + y_i \\bmod m} = \\vd_{y_{i - 1}} \\text{ if } i \\bmod 2 = 0\n    \\end{cases}.\n\\end{equation*}",
      "formula_type": "equation*",
      "line_number": 1738,
      "is_formula": true,
      "high_level_explanation": "This piecewise recurrence specifies how the second-layer hidden state updates when processing input x_{i+1}. Depending on whether i is odd or even, a reflection matrix H with angle determined by θ(x_{i+1}, b) (where b ∈ {1,0} encodes parity) is applied to the current hidden state, which coincides with a prototype state from one of two families (c· or d·). The chained equalities assert that this operation updates the state index by x_{i+1} + y_i modulo m, yielding c_{y_{i+1}} in the odd case and d_{y_{i-1}} in the even case. Thus, the dynamics implement modular addition while alternating between reflection- and rotation-associated state families.",
      "notations": {
        "\\vh_{i+1}^{(2)}": "Second-layer hidden state vector at step i+1",
        "\\vh_{i}^{(2)}": "Second-layer hidden state vector at step i",
        "\\mH(\\theta(x_{i+1}, 1))": "2×2 reflection matrix with reflection angle given by \\theta(x_{i+1}, 1)",
        "\\mH(\\theta(x_{i+1}, 0))": "2×2 reflection matrix with reflection angle given by \\theta(x_{i+1}, 0)",
        "\\theta(x_{i+1}, 1)": "Angle parameter determining the reflection when the parity flag is 1 (odd step); exact definition NOT MENTIONED",
        "\\theta(x_{i+1}, 0)": "Angle parameter determining the reflection when the parity flag is 0 (even step); exact definition NOT MENTIONED",
        "\\vc_{y_i}": "Prototype state from the c-family indexed by y_i (associated with reflections per context)",
        "\\vc_{x_{i+1} + y_i \\bmod m}": "c-family state indexed by x_{i+1} + y_i modulo m",
        "\\vc_{y_{i+1}}": "c-family state indexed by y_{i+1}",
        "\\vd_{s_i}": "Prototype state from the d-family indexed by s_i (associated with rotations per context)",
        "\\vd_{x_{i+1} + y_i \\bmod m}": "d-family state indexed by x_{i+1} + y_i modulo m",
        "\\vd_{y_{i - 1}}": "d-family state indexed by y_{i-1}",
        "x_{i+1}": "NOT MENTIONED",
        "y_i": "NOT MENTIONED",
        "y_{i+1}": "NOT MENTIONED",
        "y_{i - 1}": "NOT MENTIONED",
        "s_i": "NOT MENTIONED",
        "m": "Modulus used for arithmetic in the construction"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:02:07.778904"
    }
  ],
  "metadata": {
    "model": "gpt-5",
    "context_words": 300,
    "max_formulas": 20,
    "timestamp": "2025-10-31T17:03:17.906848",
    "total_formulas_in_paper": 20,
    "formulas_selected_for_analysis": 20,
    "skipped_by_length_limit": 0,
    "formulas_explained": 20,
    "notations_skipped": 0,
    "failed": 0
  },
  "skipped_notations": [],
  "failed": []
}