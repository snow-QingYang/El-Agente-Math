{
  "formulas": [
    {
      "label": "<<FORMULA_0010>>",
      "formula": "\\label{prediction_ODE}\n\t\t\\bm{x}(t + \\Delta t) = \\bm{x}(t) + \\int_t^{t + \\Delta t} {\\bm{f}\\left( {\\bm{x}\\left( \\tau \\right), \\bm{I}\\left( \\tau \\right),\\tau} \\right)d\\tau}.",
      "raw_latex": "\\begin{align} \\label{prediction_ODE}\n\t\t\\bm{x}(t + \\Delta t) = \\bm{x}(t) + \\int_t^{t + \\Delta t} {\\bm{f}\\left( {\\bm{x}\\left( \\tau \\right), \\bm{I}\\left( \\tau \\right),\\tau} \\right)d\\tau}. \n\t\\end{align}",
      "formula_type": "align",
      "line_number": 621,
      "is_formula": true,
      "high_level_explanation": "This equation is the integral (variation-of-constants) form of the state evolution in a Neural ODE: the future state at time t + Δt equals the current state plus the accumulated effect of the dynamics over the interval [t, t + Δt]. The vector field f depends on the state, an external input I, and time, and its integral captures how the system evolves continuously. It is used to predict the next state from an initial value in controlled continuous-time dynamics.",
      "notations": {
        "\\bm{x}(t + \\Delta t)": "State vector at time t + Δt",
        "\\bm{x}(t)": "State vector at time t",
        "\\bm{x}\\left( \\tau \\right)": "State vector evaluated at the integration time τ",
        "\\bm{f}": "Latent nonlinear mapping (vector field) in the ODE",
        "\\bm{I}\\left( \\tau \\right)": "External input evaluated at time τ",
        "t": "Time",
        "\\Delta t": "Time increment",
        "\\tau": "Integration time variable"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:00:04.384728"
    },
    {
      "label": "<<FORMULA_0011>>",
      "formula": "\\label{pure_NN}\n\t\t\\bm{f}\\left( {\\bm{x}(t),\\bm{I}(t), t} \\right) = \\bm{f}_{neural}\\left( {\\bm{x}(t),\\bm{I}(t), t,\\bm{\\theta}} \\right) + \\Delta \\bm{f}(t)",
      "raw_latex": "\\begin{align} \\label{pure_NN}\n\t\t\\bm{f}\\left( {\\bm{x}(t),\\bm{I}(t), t} \\right) = \\bm{f}_{neural}\\left( {\\bm{x}(t),\\bm{I}(t), t,\\bm{\\theta}} \\right) + \\Delta \\bm{f}(t) \n\t\\end{align}",
      "formula_type": "align",
      "line_number": 628,
      "is_formula": true,
      "high_level_explanation": "This equation decomposes the true continuous-time dynamics into a learned neural ODE component plus a residual error term. The system’s vector field f, evaluated at the current state, input, and time, is approximated by a neural network f_neural with parameters θ, while Δf(t) captures unknown model mismatch or learning residuals. This formalizes the gap between the learned dynamics and the true dynamics.",
      "notations": {
        "\\bm{f}": "True (latent) system dynamics/vector field",
        "\\bm{x}(t)": "State vector at time t",
        "\\bm{I}(t)": "External input at time t",
        "t": "Time",
        "\\bm{f}_{neural}": "Learned ODE model parameterized by \\bm{\\theta}",
        "\\bm{\\theta}": "Parameters of the neural ODE model",
        "\\Delta \\bm{f}(t)": "Unknown learning residual error (model mismatch) at time t"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:00:23.010267"
    },
    {
      "label": "<<FORMULA_0025>>",
      "formula": "\\label{sys1_1}\n\t\t\\bm{\\hat{f}}_{neural}(t) = \\bm{f}_{neural}(t) + \\sum\\limits_{i = 0}^k {\\bm{L}\\left( {\\bm{x}\\left( {{t_i}} \\right) - \\bm {\\bar{x}}\\left( {{t_k}} \\right)} \\right)}",
      "raw_latex": "\\begin{align} \\label{sys1_1}\n\t\t\\bm{\\hat{f}}_{neural}(t) = \\bm{f}_{neural}(t) + \\sum\\limits_{i = 0}^k {\\bm{L}\\left( {\\bm{x}\\left( {{t_i}} \\right) - \\bm {\\bar{x}}\\left( {{t_k}} \\right)} \\right)} \n\t\\end{align}",
      "formula_type": "align",
      "line_number": 647,
      "is_formula": true,
      "high_level_explanation": "The formula defines a feedback-corrected neural dynamics function by adding accumulated state-feedback terms to the originally learned neural ODE vector field. Each feedback term uses a positive definite matrix to scale the discrepancy between a measured state at past evaluation times and the predicted state from the most recent evaluation. By summing these historical errors, the corrected dynamics aim to compensate for residual modeling errors and improve robustness when transferring to new cases.",
      "notations": {
        "\\bm{\\hat{f}}_{neural}(t)": "Feedback-corrected neural ODE latent dynamics at time t",
        "\\bm{f}_{neural}(t)": "Originally learned neural ODE latent dynamics (without feedback) at time t",
        "\\bm{L}": "Positive definite matrix",
        "\\bm{x}\\left( {{t_i}} \\right)": "State measurement at the historical evaluation time t_i",
        "\\bm {\\bar{x}}\\left( {{t_k}} \\right)": "Predicted state from the last evaluation moment (e.g., via Euler integration)",
        "t": "Current time",
        "t_i": "Historical evaluation time instant",
        "t_k": "Most recent (last) evaluation time instant"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:00:11.215770"
    },
    {
      "label": "<<FORMULA_0030>>",
      "formula": "\\label{auxiliary_var}\n\t\t\\bm {\\hat{x}}\\left( {{t}} \\right) = \\bm {\\bar{x}}\\left( {{t}} \\right) - \\sum\\limits_{i = 0}^{k-1} {\\left( {\\bm{x}\\left( {{t_i}} \\right) - \\bm {\\bar{x}}\\left( {{t_n}} \\right)} \\right)}",
      "raw_latex": "\\begin{align}  \\label{auxiliary_var}\n\t\t\\bm {\\hat{x}}\\left( {{t}} \\right) = \\bm {\\bar{x}}\\left( {{t}} \\right) - \\sum\\limits_{i = 0}^{k-1} {\\left( {\\bm{x}\\left( {{t_i}} \\right) - \\bm {\\bar{x}}\\left( {{t_n}} \\right)} \\right)}\n\t\\end{align}",
      "formula_type": "align",
      "line_number": 657,
      "is_formula": true,
      "high_level_explanation": "This equation defines an auxiliary, feedback-corrected state variable \\hat{x}(t). It starts from the nominal predicted state \\bar{x}(t) and subtracts the accumulated discrepancies between the measured states x(t_i) and the corresponding predictions \\bar{x}(t_n) over the most recent k evaluation instants. The construction aggregates historical evaluation errors into a single variable so they can be used without storing all past measurements.",
      "notations": {
        "\\bm {\\hat{x}}\\left( {{t}} \\right)": "Auxiliary state variable that aggregates historical evaluation errors for feedback-based correction",
        "\\bm {\\bar{x}}\\left( {{t}} \\right)": "Predicted state obtained from the last evaluation moment (e.g., via Euler integration), evaluated at time t",
        "\\bm{x}\\left( {{t_i}} \\right)": "State measurement at the historical evaluation time t_i",
        "\\bm {\\bar{x}}\\left( {{t_n}} \\right)": "Predicted state evaluated at time t_n (the role/definition of t_n is NOT MENTIONED)",
        "t": "Current time",
        "t_i": "Historical evaluation moment",
        "t_n": "NOT MENTIONED",
        "k": "NOT MENTIONED"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:00:00.518616"
    },
    {
      "label": "<<FORMULA_0050>>",
      "formula": "\\mathcal{B}_2 = \\left\\{{\\bm{\\dot{\\tilde x}}}(t)\\in\\mathbb{R}^n:\\left\\|{\\bm{\\dot{\\tilde x}}}(t)\\right\\|\\le{\\gamma}\\lambda_M(\\bm{L})/{{\\lambda_m(\\bm{L})}} + \\gamma\\right\\}",
      "raw_latex": "$\\mathcal{B}_2 = \\left\\{{\\bm{\\dot{\\tilde x}}}(t)\\in\\mathbb{R}^n:\\left\\|{\\bm{\\dot{\\tilde x}}}(t)\\right\\|\\le{\\gamma}\\lambda_M(\\bm{L})/{{\\lambda_m(\\bm{L})}} + \\gamma\\right\\}$",
      "formula_type": "inline",
      "line_number": 689,
      "is_formula": true,
      "high_level_explanation": "This expression defines the bounded set B2 consisting of all possible values of the time derivative of the state observation error vector. It is a Euclidean ball in R^n centered at the origin with radius equal to gamma times the ratio of the largest to the smallest eigenvalue of L, plus an additional gamma. Hence, the bound depends on the conditioning of the positive-definite gain matrix L and can be tightened by increasing its minimum eigenvalue.",
      "notations": {
        "\\mathcal{B}_2": "Bounded set (Euclidean ball) for the error-derivative vector",
        "{\\bm{\\dot{\\tilde x}}}(t)": "Time derivative of the state observation error vector at time t",
        "\\mathbb{R}^n": "n-dimensional real vector space",
        "n": "NOT MENTIONED",
        "\\gamma": "NOT MENTIONED",
        "\\lambda_M(\\bm{L})": "Largest eigenvalue of L",
        "\\lambda_m(\\bm{L})": "Smallest eigenvalue of L",
        "\\bm{L}": "Positive definite feedback gain matrix"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:00:26.640685"
    },
    {
      "label": "<<FORMULA_0071>>",
      "formula": "\\label{learn_feedback_part}\n\t\t{\\bm{\\xi} ^ * } & = \\mathop {\\arg \\min }\\limits_{\\bm{\\xi}}  \\sum\\limits_{i = 1}^{{n_{case}}} {\\sum\\limits_{j \\in \\mathcal{D}_i^{tra}} {\\left\\| {\\bm{x}_{i,j}^ *  - \\bm{x}_{i,j} } \\right\\|}} \\notag \\\\\n\t\ts.t.\\quad {\\bm{x}_{i,j}} & = {\\bm{x}_{i,j - 1}} + {T_s}\\left( {{\\bm{f}_{neural}}({\\bm{x}_{i,j - 1}}) + \\bm{h}_{neural}\\left( {{\\bm{x}_{i,j - 1}} - {\\bm{\\hat x}_{i,j - 1}},\\bm{\\xi} } \\right)} \\right)",
      "raw_latex": "\\begin{align} \\label{learn_feedback_part}\n\t\t{\\bm{\\xi} ^ * } & = \\mathop {\\arg \\min }\\limits_{\\bm{\\xi}}  \\sum\\limits_{i = 1}^{{n_{case}}} {\\sum\\limits_{j \\in \\mathcal{D}_i^{tra}} {\\left\\| {\\bm{x}_{i,j}^ *  - \\bm{x}_{i,j} } \\right\\|}} \\notag \\\\\n\t\ts.t.\\quad {\\bm{x}_{i,j}} & = {\\bm{x}_{i,j - 1}} + {T_s}\\left( {{\\bm{f}_{neural}}({\\bm{x}_{i,j - 1}}) + \\bm{h}_{neural}\\left( {{\\bm{x}_{i,j - 1}} - {\\bm{\\hat x}_{i,j - 1}},\\bm{\\xi} } \\right)} \\right)\n\t\\end{align}",
      "formula_type": "align",
      "line_number": 752,
      "is_formula": true,
      "high_level_explanation": "This optimization learns the feedback parameters by minimizing the summed discrepancy between labeled states and one-step predictions across all domain-randomized cases and training indices. The one-step prediction is computed with a forward Euler update of the frozen nominal neural ODE and an added feedback correction, which depends on the state prediction error and the learnable parameters. Solving the problem yields the parameter vector that best reduces the prediction error while keeping the nominal dynamics fixed.",
      "notations": {
        "{\\bm{\\xi} ^ * }": "Optimal value of the parameter vector obtained by the minimization",
        "\\bm{\\xi}": "Trainable parameter vector (optimization variable)",
        "n_{case}": "Number of randomized cases",
        "\\mathcal{D}_i^{tra}": "Training set (index set) for the i-th case",
        "{\\bm{x}_{i,j}^ * }": "Labeled state at index (i, j)",
        "{\\bm{x}_{i,j}}": "One-step predicted state at index (i, j)",
        "T_s": "Sampling time (Euler integration step size)",
        "\\bm{f}_{neural}": "Neural ODE model trained on the nominal task (frozen during this stage)",
        "\\bm{h}_{neural}": "Feedback neural network to be learned",
        "{\\bm{x}_{i,j - 1}}": "Predicted state at the previous step (i, j−1)",
        "{\\bm{\\hat x}_{i,j - 1}}": "NOT MENTIONED"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:59:19.218920"
    },
    {
      "label": "<<FORMULA_0084>>",
      "formula": "\\label{eq:oc}\n\t\t&\\min_{\\bm{\\theta}} \\sum_{i=1}^{N-1} \\;l_i(\\bm{x}_i, {\\bm{x}_i}^r, \\bm{\\theta}) - l_N(\\bm{x}_N, {\\bm{x}_N}^r)\\\\\n\t\t& s.t.\\;\\;\\bm{x}_{i+1} = \n\t\t\\bm{f}_{neural}^d(\\bm{x}_i, \\bm{I}_i, t_i, \\bm{\\theta})",
      "raw_latex": "\\begin{align}\n\t\t\\label{eq:oc}\n\t\t&\\min_{\\bm{\\theta}} \\sum_{i=1}^{N-1} \\;l_i(\\bm{x}_i, {\\bm{x}_i}^r, \\bm{\\theta}) - l_N(\\bm{x}_N, {\\bm{x}_N}^r)\\\\\n\t\t& s.t.\\;\\;\\bm{x}_{i+1} = \n\t\t\\bm{f}_{neural}^d(\\bm{x}_i, \\bm{I}_i, t_i, \\bm{\\theta})\n\t\\end{align}",
      "formula_type": "align",
      "line_number": 908,
      "is_formula": true,
      "high_level_explanation": "This is an optimal control–style learning objective for training a neural ODE with feedback. It optimizes the network parameters to reduce the discrepancy between the model’s rollout states and real-world samples along a recorded trajectory, subject to the discretized neural dynamics. The dynamics map advances the state using external inputs and timestamps, and the objective aggregates per-step losses along the trajectory together with a terminal term on the final state.",
      "notations": {
        "\\bm{\\theta}": "Network parameters to be learned",
        "N": "Number of recorded time steps in the trajectory",
        "l_i": "Per-step loss at time t_i quantifying the mismatch between model rollout and real sample (chosen as a weighted quadratic form in this article)",
        "l_N": "Terminal loss on the final state at step N measuring the final-state mismatch (chosen as a weighted quadratic form)",
        "\\bm{x}_i": "Model rollout state at time t_i",
        "{\\bm{x}_i}^r": "Real-world state sample at time t_i",
        "\\bm{x}_N": "Model rollout state at the final time step N",
        "{\\bm{x}_N}^r": "Real-world state sample at the final time step N",
        "\\bm{x}_{i+1}": "Model rollout state at the next time step",
        "\\bm{f}_{neural}^d": "Discretized neural dynamics mapping (fixed-step integration of the continuous-time neural ODE)",
        "\\bm{I}_i": "External input at time t_i",
        "t_i": "Timestamp corresponding to step i"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:59:17.357646"
    },
    {
      "label": "<<FORMULA_0093>>",
      "formula": "& \\begin{aligned}\n\t\t\tH &= J + \n\t\t\t\\sum_{i=1}^{N-1} \\bm{\\lambda}_{i+1}^\\top\n\t\t\t\\bm{f}_{neural}^d(\\bm{x}_i, \\bm{I}_i, t_i, \\bm{\\theta})\n\t\t\\end{aligned}\\\\\n\t\t&\\bm{x}_{i+1} = \\nabla_{\\bm{\\lambda}} H \n\t\t= \\bm{f}_{neural}^d(\\bm{x}_i, \\bm{I}_i, t_i, \\bm{\\theta}),\n\t\t\\;\\bm{x}_1 = \\bm{x}(0) \\label{rollout}\\\\\n\t\t& \\bm{\\lambda}_i = \\nabla_{\\bm{x}}H \n\t\t= \\nabla_{\\bm{x}} l_i \\\\times (\\frac{\\partial \\bm{f}_{neural}^d}{\\partial{\\bm{x}}})^\\top\n\t\t\\bm{\\lambda}_{i+1} \\label{adjoint},\n\t\t\\;\\bm{\\lambda}_N = \\frac{\\partial{l_N}}{\\partial{\\bm{x}_N}}\\\\\n\t\t& \\begin{aligned}\n\t\t\t\\frac{\\partial{H}}{\\partial{\\bm{\\theta}}}\n\t\t\t= \\sum_{i=1}^{N-1} \\;\\nabla_{\\bm{\\theta}} l_i + \\bm{\\lambda}_{i+1}^\\top\n\t\t\t\\nabla_{\\bm{\\theta}} \\bm{f}_{neural}^d = 0\n\t\t\\end{aligned} \\label{gradient}",
      "raw_latex": "\\begin{align}\n\t\t& \\begin{aligned}\n\t\t\tH &= J + \n\t\t\t\\sum_{i=1}^{N-1} \\bm{\\lambda}_{i+1}^\\top\n\t\t\t\\bm{f}_{neural}^d(\\bm{x}_i, \\bm{I}_i, t_i, \\bm{\\theta})\n\t\t\\end{aligned}\\\\\n\t\t&\\bm{x}_{i+1} = \\nabla_{\\bm{\\lambda}} H \n\t\t= \\bm{f}_{neural}^d(\\bm{x}_i, \\bm{I}_i, t_i, \\bm{\\theta}),\n\t\t\\;\\bm{x}_1 = \\bm{x}(0) \\label{rollout}\\\\\n\t\t& \\bm{\\lambda}_i = \\nabla_{\\bm{x}}H \n\t\t= \\nabla_{\\bm{x}} l_i \\\\times (\\frac{\\partial \\bm{f}_{neural}^d}{\\partial{\\bm{x}}})^\\top\n\t\t\\bm{\\lambda}_{i+1} \\label{adjoint},\n\t\t\\;\\bm{\\lambda}_N = \\frac{\\partial{l_N}}{\\partial{\\bm{x}_N}}\\\\\n\t\t& \\begin{aligned}\n\t\t\t\\frac{\\partial{H}}{\\partial{\\bm{\\theta}}}\n\t\t\t= \\sum_{i=1}^{N-1} \\;\\nabla_{\\bm{\\theta}} l_i + \\bm{\\lambda}_{i+1}^\\top\n\t\t\t\\nabla_{\\bm{\\theta}} \\bm{f}_{neural}^d = 0\n\t\t\\end{aligned} \\label{gradient}\n\t\\end{align}",
      "formula_type": "align",
      "line_number": 921,
      "is_formula": true,
      "high_level_explanation": "These equations define the discrete-time Hamiltonian for learning a neural ODE with inputs and derive the forward rollout, backward adjoint recursion, and parameter-gradient condition. The state evolves via the discrete neural dynamics, the adjoint (costate) propagates backward by combining the state-loss gradient and the Jacobian-transpose of the dynamics, and the gradient with respect to parameters aggregates per-step contributions from the loss and the dynamics sensitivity. Together, they constitute the first-order optimality (adjoint/Pontryagin) conditions used to compute analytic gradients for optimizing the model parameters.",
      "notations": {
        "H": "Hamiltonian of the learning problem",
        "J": "Objective function defined in the optimization problem (sum of per-step losses minus the terminal loss)",
        "\\bm{\\lambda}_i": "Adjoint (costate) variable at time step i",
        "\\bm{\\lambda}_{i+1}": "Adjoint (costate) variable at time step i+1",
        "\\bm{\\lambda}_N": "Terminal adjoint (costate) at step N",
        "\\bm{f}_{neural}^d": "Discretized neural-ODE dynamics (state-update function) with fixed timestep",
        "\\bm{x}_i": "Model rollout state at time t_i",
        "\\bm{x}_N": "Model rollout state at the final step N",
        "\\bm{x}(0)": "Initial state at time 0",
        "\\bm{I}_i": "NOT MENTIONED",
        "t_i": "NOT MENTIONED",
        "\\bm{\\theta}": "Neural network parameters",
        "l_i": "Per-step loss measuring the discrepancy between \\bm{x}_i and the real state (chosen as a weighted quadratic in the paper)",
        "l_N": "Terminal loss at step N comparing \\bm{x}_N to the real state (chosen as a weighted quadratic in the paper)",
        "\\frac{\\partial \\bm{f}_{neural}^d}{\\partial{\\bm{x}}}": "Jacobian of the discrete dynamics with respect to the state",
        "\\nabla_{\\bm{\\theta}} \\bm{f}_{neural}^d": "Jacobian of the discrete dynamics with respect to the parameters \\bm{\\theta}",
        "N": "NOT MENTIONED"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:59:42.357769"
    },
    {
      "label": "<<FORMULA_0118>>",
      "formula": "\\label{lyapunov_devi}\n\t\t\\dot V\\left( t \\right) & = \\bm{{\\tilde {x}}}{\\left( t \\right)^T}\\bm{\\dot{\\tilde {x}}}\\left( t \\right) \\notag \\\\\n\t\t&\\mathop  = \\limits^{(a)} \\bm{{\\tilde {x}}}{\\left( t \\right)^T}\\left( {- \\bm{L}{\\bm{{\\tilde {x}}}}(t) + \\Delta \\bm{f}(t)} \\right) \\notag \\\\\n\t\t& = -\\bm{{\\tilde {x}}}{\\left( t \\right)^T}\\bm{L}{\\bm{{\\tilde {x}}}}(t) \\\\times \\bm{{\\tilde {x}}}{\\left( t \\right)^T}\\Delta \\bm{f}(t) \\notag \\\\\n\t\t& \\mathop  \\le \\limits^{(b)} \\textcolor{black}{-\\frac{\\lambda_m(\\bm{L})}{2} \\bm{{\\tilde {x}}}{\\left( t \\right)^T}\\bm{{\\tilde {x}}}{\\left( t \\right)} + \\frac{1}{2\\lambda_m(\\bm{L})}\\gamma^2}",
      "raw_latex": "\\begin{align} \\label{lyapunov_devi}\n\t\t\\dot V\\left( t \\right) & = \\bm{{\\tilde {x}}}{\\left( t \\right)^T}\\bm{\\dot{\\tilde {x}}}\\left( t \\right) \\notag \\\\\n\t\t&\\mathop  = \\limits^{(a)} \\bm{{\\tilde {x}}}{\\left( t \\right)^T}\\left( {- \\bm{L}{\\bm{{\\tilde {x}}}}(t) + \\Delta \\bm{f}(t)} \\right) \\notag \\\\\n\t\t& = -\\bm{{\\tilde {x}}}{\\left( t \\right)^T}\\bm{L}{\\bm{{\\tilde {x}}}}(t) \\\\times \\bm{{\\tilde {x}}}{\\left( t \\right)^T}\\Delta \\bm{f}(t) \\notag \\\\\n\t\t& \\mathop  \\le \\limits^{(b)} \\textcolor{black}{-\\frac{\\lambda_m(\\bm{L})}{2} \\bm{{\\tilde {x}}}{\\left( t \\right)^T}\\bm{{\\tilde {x}}}{\\left( t \\right)} + \\frac{1}{2\\lambda_m(\\bm{L})}\\gamma^2}\n\t\\end{align}",
      "formula_type": "align",
      "line_number": 989,
      "is_formula": true,
      "high_level_explanation": "The expression computes and then upper-bounds the time derivative of a Lyapunov function along the error dynamics. Substituting the error dynamics shows the derivative equals a negative quadratic form in the error plus an inner product with the learning residual. Using the minimum eigenvalue of L and Young’s inequality, the derivative is bounded above by a negative definite term plus a constant proportional to gamma squared, indicating ultimate boundedness of the error when the residual is bounded.",
      "notations": {
        "\\dot V\\left( t \\right)": "Time derivative of the Lyapunov function V(t)",
        "\\bm{{\\tilde {x}}}{\\left( t \\right)}": "State observation error vector at time t",
        "\\bm{\\dot{\\tilde {x}}}\\left( t \\right)": "Time derivative of the state observation error",
        "\\bm{L}": "Feedback gain matrix",
        "\\Delta \\bm{f}(t)": "Learning residual (modeling/approximation error) at time t",
        "\\lambda_m(\\bm{L})": "Minimum eigenvalue of matrix L",
        "\\gamma": "Upper bound on the norm of the learning residual \\(\\Delta \\bm{f}(t)\\)"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:59:18.923673"
    },
    {
      "label": "<<FORMULA_0119>>",
      "formula": "\\tilde{\\bm{x}}^{T} \\Delta \\bm{f}(t)\\le \\sqrt{\\lambda_{m}(\\bm{L})}\\|\\tilde{\\bm{x}}\\| \\frac{\\gamma}{\\sqrt{\\lambda_{m}(\\bm{L})}}\\\\ge \\frac{\\lambda_{m}(\\bm{L})\\|\\tilde{\\bm{x}}\\|^{2}}{2}+\\frac{\\gamma^{2}}{2 \\lambda_{m}(\\bm{L})}",
      "raw_latex": "$\\tilde{\\bm{x}}^{T} \\Delta \\bm{f}(t)\\le \\sqrt{\\lambda_{m}(\\bm{L})}\\|\\tilde{\\bm{x}}\\| \\frac{\\gamma}{\\sqrt{\\lambda_{m}(\\bm{L})}}\\\\ge \\frac{\\lambda_{m}(\\bm{L})\\|\\tilde{\\bm{x}}\\|^{2}}{2}+\\frac{\\gamma^{2}}{2 \\lambda_{m}(\\bm{L})}$",
      "formula_type": "inline",
      "line_number": 995,
      "is_formula": true,
      "high_level_explanation": "This inequality upper-bounds the cross term between the error vector and the modeling residual that arises in the Lyapunov derivative. First, the inner product is bounded by the product of norms using Cauchy–Schwarz together with the assumption that the residual’s norm is bounded by gamma. Then Young’s inequality is applied with a balancing factor tied to the minimum eigenvalue of L to split the product into a sum of quadratic terms. This bound is used to combine with the negative quadratic term from the feedback to show stability and derive an ultimate bound.",
      "notations": {
        "\\tilde{\\bm{x}}": "State observation error vector",
        "\\Delta \\bm{f}(t)": "Learning residual (modeling error) at time t",
        "\\gamma": "Upper bound on the norm of the learning residual (i.e., the uncertainty bound)",
        "\\lambda_{m}(\\bm{L})": "Minimum eigenvalue of \\bm{L}",
        "\\bm{L}": "Feedback gain matrix used in the error dynamics"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:59:16.732485"
    },
    {
      "label": "<<FORMULA_0123>>",
      "formula": "\\label{bounded_converg}\n\t\t\\textcolor{black}{\\mathop {\\lim }\\limits_{t \\to \\infty }\\|\\bm{\\tilde{x}}(t)\\|\\le \\frac{\\gamma}{{\\lambda_m(\\bm{L})}}}",
      "raw_latex": "\\begin{align} \\label{bounded_converg}\n\t\t\\textcolor{black}{\\mathop {\\lim }\\limits_{t \\to \\infty }\\|\\bm{\\tilde{x}}(t)\\|\\le \\frac{\\gamma}{{\\lambda_m(\\bm{L})}}}\n\t\\end{align}",
      "formula_type": "align",
      "line_number": 1006,
      "is_formula": true,
      "high_level_explanation": "This inequality states that as time goes to infinity, the norm of the state observation error converges to an ultimate bound no larger than gamma divided by the minimal eigenvalue of the gain matrix L. It is a Lyapunov-based result under bounded learning/modeling residuals. Increasing the smallest eigenvalue of L (i.e., strengthening the observer/feedback gain) shrinks the asymptotic error bound.",
      "notations": {
        "\\bm{\\tilde{x}}(t)": "State observation error vector at time t",
        "\\gamma": "Upper bound on the norm of the learning/modeling residual (uncertainty)",
        "\\lambda_m(\\bm{L})": "Minimum eigenvalue of the gain matrix \\bm{L}",
        "\\bm{L}": "Observer/feedback gain matrix",
        "t": "Continuous time"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:00:15.025267"
    },
    {
      "label": "<<FORMULA_0126>>",
      "formula": "\\mathcal{B}_2 = \\left\\{{\\bm{\\dot{\\tilde x}}}(t)\\in\\mathbb{R}^n:\\left\\|{\\bm{\\dot{\\tilde x}}}(t)\\right\\|\\le{\\gamma}\\lambda_M(\\bm{L})/{{\\lambda_m(\\bm{L})}} + \\gamma\\right\\}",
      "raw_latex": "$\\mathcal{B}_2 = \\left\\{{\\bm{\\dot{\\tilde x}}}(t)\\in\\mathbb{R}^n:\\left\\|{\\bm{\\dot{\\tilde x}}}(t)\\right\\|\\le{\\gamma}\\lambda_M(\\bm{L})/{{\\lambda_m(\\bm{L})}} + \\gamma\\right\\}$",
      "formula_type": "inline",
      "line_number": 1011,
      "is_formula": true,
      "high_level_explanation": "This expression defines the bounded set B2 consisting of all derivatives of the state observation error whose Euclidean norm does not exceed γ·(λ_M(L)/λ_m(L)) + γ. It is used in the Lyapunov-based analysis to characterize the asymptotic bound that the error derivative converges to under bounded modeling residuals and a chosen observer/feedback gain L.",
      "notations": {
        "\\mathcal{B}_2": "Bounded set for the derivative of the state observation error as defined by this inequality",
        "\\bm{\\dot{\\tilde x}}(t)": "Time derivative of the state observation error vector at time t",
        "\\gamma": "Upper bound on the uncertainty/learning residual norm (per the stated assumption)",
        "\\lambda_M(\\bm{L})": "Maximum eigenvalue of the feedback/observer gain matrix",
        "\\lambda_m(\\bm{L})": "Minimum eigenvalue of the feedback/observer gain matrix",
        "\\bm{L}": "Observer (feedback) gain matrix",
        "n": "NOT MENTIONED"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:59:54.236921"
    },
    {
      "label": "<<FORMULA_0130>>",
      "formula": "\\label{discrete_error}\n\t\t\\textcolor{black}{{\\bm{\\tilde x}}(t_j)} & = \\textcolor{black}{{\\bm{\\tilde x}}(t_{k-1}) + T_s(\\bm{f}(t_{k-1}) - \\bm{\\hat{f}}_{neural}(t_{k-1}))} \\notag \\\\\n\t\t&\\mathop  = \\limits^{(c)} \\textcolor{black}{{\\bm{\\tilde x}}(t_{k-1}) +  T_s(\\bm{{f}}_{neural}(t_{k-1}) - \\bm{\\hat{f}}_{neural}(t_{k-1}) + \\Delta \\bm{f}(t) )}\\notag \\\\\n\t\t&\\mathop  = \\limits^{(d)} \\textcolor{black}{(\\bm{I}- T_s\\bm{L}){\\bm{\\tilde x}}(t_{k-1}) + T_s\\Delta \\bm{f}(t)},",
      "raw_latex": "\\begin{align} \\label{discrete_error}\n\t\t\\textcolor{black}{{\\bm{\\tilde x}}(t_j)} & = \\textcolor{black}{{\\bm{\\tilde x}}(t_{k-1}) + T_s(\\bm{f}(t_{k-1}) - \\bm{\\hat{f}}_{neural}(t_{k-1}))} \\notag \\\\\n\t\t&\\mathop  = \\limits^{(c)} \\textcolor{black}{{\\bm{\\tilde x}}(t_{k-1}) +  T_s(\\bm{{f}}_{neural}(t_{k-1}) - \\bm{\\hat{f}}_{neural}(t_{k-1}) + \\Delta \\bm{f}(t) )}\\notag \\\\\n\t\t&\\mathop  = \\limits^{(d)} \\textcolor{black}{(\\bm{I}- T_s\\bm{L}){\\bm{\\tilde x}}(t_{k-1}) + T_s\\Delta \\bm{f}(t)},\n\t\\end{align}",
      "formula_type": "align",
      "line_number": 1037,
      "is_formula": true,
      "high_level_explanation": "This set of equalities gives the discrete-time evolution of the state observation error over one sampling step. First, the error updates by the sampling period times the mismatch between the true system dynamics and the (feedback-modified) neural estimate. Then, by decomposing the true dynamics into a nominal neural model plus a residual, and substituting the feedback law, the error update becomes a linear contraction term (I − T_s L) acting on the previous error, driven by the residual modeling error Δf(t). This form is used to assess input-to-state stability under bounded residuals and appropriate choice of the observer gain L.",
      "notations": {
        "\\bm{\\tilde x}": "State observation error vector (e.g., x − \\hat{x}); explicitly recalled later as \\bm{\\tilde{x}}(t+T_s) = \\bm{x}(t+T_s) - \\bm{\\hat{x}}(t+T_s)",
        "t_j": "NOT MENTIONED",
        "t_{k-1}": "NOT MENTIONED",
        "T_s": "Sampling period (discretization step), as recalled in the one-step prediction \\bm{\\tilde{x}}(t+T_s)",
        "\\bm{f}": "True system dynamics (decomposed as \\bm{f}_{neural} + \\Delta \\bm{f} in step (c))",
        "\\bm{f}_{neural}": "Nominal (pure) neural network model of the dynamics, referenced via (\\ref{pure_NN})",
        "\\bm{\\hat{f}}_{neural}": "Feedback-modified neural network model of the dynamics, referenced via (\\ref{feedback_NN})",
        "\\Delta \\bm{f}": "Learning residual/modeling error between the true dynamics and the neural model (assumed bounded under Assumption \\ref{assum})",
        "\\bm{I}": "Identity matrix",
        "\\bm{L}": "Observer gain matrix"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:59:37.445368"
    },
    {
      "label": "<<FORMULA_0134>>",
      "formula": "\\bm{\\dot{x}}\\left( t \\right) = \\left[ {\\begin{array}{*{20}{c}}\n\t\t\t\t{ - \\eta }&\\omega \\\\\n\t\t\t\t{ - \\omega }&{ - \\eta }\n\t\t\\end{array}} \\right]\\bm{x}\\left( t \\right) + \\\\geft[ {\\begin{array}{*{20}{c}}\n\t\t\t\t\\varepsilon \\\\\n\t\t\t\t\\varepsilon \n\t\t\\end{array}} \\right]",
      "raw_latex": "\\begin{align}\n\t\t\\bm{\\dot{x}}\\left( t \\right) = \\left[ {\\begin{array}{*{20}{c}}\n\t\t\t\t{ - \\eta }&\\omega \\\\\n\t\t\t\t{ - \\omega }&{ - \\eta }\n\t\t\\end{array}} \\right]\\bm{x}\\left( t \\right) + \\\\geft[ {\\begin{array}{*{20}{c}}\n\t\t\t\t\\varepsilon \\\\\n\t\t\t\t\\varepsilon \n\t\t\\end{array}} \\right]\n\t\\end{align}",
      "formula_type": "align",
      "line_number": 1055,
      "is_formula": true,
      "high_level_explanation": "This is a 2D linear time-invariant ordinary differential equation that generates spiral dynamics. The matrix with −η on the diagonal and ±ω off-diagonal produces a damped rotation of the state, while the constant input [ε; ε] adds a uniform bias to both state components. The parameter η controls decay (damping), ω controls the rotation speed, and ε sets the constant bias forcing.",
      "notations": {
        "\\bm{\\dot{x}}\\left( t \\right)": "Time derivative of the 2D state vector",
        "\\bm{x}\\left( t \\right)": "2D state vector",
        "\\eta": "Decay (damping) rate",
        "\\omega": "Rotation rate (angular frequency)",
        "\\varepsilon": "Constant bias input applied equally to both state components",
        "t": "Time"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:58:52.804873"
    },
    {
      "label": "<<FORMULA_0167>>",
      "formula": "\\label{quadrotor_model}\n\t\t\\begin{aligned}\n\t\t\t&\\dot {\\bm p} = \\bm v, \\quad \\dot {\\bm v} = \\bm a = \n\t\t\t-\\frac{1}{m}\\bm Z_B T + g\\bm Z_E\\\\\n\t\t\t& \\dot {\\bm{\\Theta}} = \\bm{W}(\\bm{\\Theta})\\bm{\\omega},\\quad\n\t\t\t\\bm J \\dot {\\bm \\omega}= \n\t\t\t-\\bm \\omega \\times (\\bm J \\bm \\omega) \n\t\t\t+ \\bm \\tau\\\\\n\t\t\t& [T, \\bm \\tau]^\\top = \\bm{C} \n\t\t\t[T_1, T_2, T_3, T_4]^\\top\n\t\t\\end{aligned}",
      "raw_latex": "\\begin{equation} \\label{quadrotor_model}\n\t\t\\begin{aligned}\n\t\t\t&\\dot {\\bm p} = \\bm v, \\quad \\dot {\\bm v} = \\bm a = \n\t\t\t-\\frac{1}{m}\\bm Z_B T + g\\bm Z_E\\\\\n\t\t\t& \\dot {\\bm{\\Theta}} = \\bm{W}(\\bm{\\Theta})\\bm{\\omega},\\quad\n\t\t\t\\bm J \\dot {\\bm \\omega}= \n\t\t\t-\\bm \\omega \\times (\\bm J \\bm \\omega) \n\t\t\t+ \\bm \\tau\\\\\n\t\t\t& [T, \\bm \\tau]^\\top = \\bm{C} \n\t\t\t[T_1, T_2, T_3, T_4]^\\top\n\t\t\\end{aligned}\n\t\\end{equation}",
      "formula_type": "equation",
      "line_number": 1092,
      "is_formula": true,
      "high_level_explanation": "This set of equations specifies a standard quadrotor rigid-body model. The first line gives the translational kinematics and dynamics: position differentiates to velocity, and acceleration is the sum of gravity and thrust (scaled by mass and directed along the body z-axis). The second line gives the attitude kinematics via an Euler-angle mapping and the rotational dynamics via the rigid-body equation with gyroscopic term and applied moments. The last line maps individual motor thrusts to the collective thrust and body torques through a control allocation matrix.",
      "notations": {
        "\\bm p": "Position vector",
        "\\bm v": "Velocity vector",
        "\\bm a": "Linear acceleration vector",
        "m": "NOT MENTIONED",
        "\\bm Z_B": "NOT MENTIONED",
        "T": "Aggregated thrust term produced by control allocation",
        "g": "Magnitude of gravitational acceleration",
        "\\bm Z_E": "NOT MENTIONED",
        "\\bm{\\Theta}": "Euler angle vector",
        "\\bm{W}(\\bm{\\Theta})": "Rotational mapping matrix of Euler angle dynamics",
        "\\bm{\\omega}": "Body rate (angular velocity) vector",
        "\\bm \\omega": "Body rate (angular velocity) vector",
        "\\bm J": "NOT MENTIONED",
        "\\bm \\tau": "NOT MENTIONED",
        "\\bm C": "Control allocation matrix",
        "T_1": "Motor 1 thrust input",
        "T_2": "Motor 2 thrust input",
        "T_3": "Motor 3 thrust input",
        "T_4": "Motor 4 thrust input"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:59:30.321593"
    },
    {
      "label": "<<FORMULA_0184>>",
      "formula": "\\begin{aligned}\n\t\t\t& \\min_{\\bm{x}_{1:N}, \\bm{u}_{0:N-1}}\\;\n\t\t\tl_N(\\bm{x}_N, \\bm{x}_N^r) +\n\t\t\t\\sum_{i=1}^{N}\\; l_x(\\bm{x}_i, \\bm{x}_i^r) + \n\t\t\tl_u(\\bm{u}_i, \\bm{u}_i^r)\\\\\n\t\t\t&\\begin{aligned}\n\t\t\t\ts.t.\\;\\;\n\t\t\t\t&\\bm{{x}}_{i+1}\n\t\t\t\t= \\bm{f}_d(\\bm{x}_i, \\bm{u}_i),\\; \\bm{x}_0 = \\bm{x}(0)\\\\\n\t\t\t\t&\\bm{u}_i \\in \\mathbb{U},\\;\\bm{x}_i \\in \\mathbb{X}\n\t\t\t\\end{aligned}\n\t\t\\end{aligned}\n\t\t\\label{Standard_MPC}",
      "raw_latex": "\\begin{equation}\n\t\t\\begin{aligned}\n\t\t\t& \\min_{\\bm{x}_{1:N}, \\bm{u}_{0:N-1}}\\;\n\t\t\tl_N(\\bm{x}_N, \\bm{x}_N^r) +\n\t\t\t\\sum_{i=1}^{N}\\; l_x(\\bm{x}_i, \\bm{x}_i^r) + \n\t\t\tl_u(\\bm{u}_i, \\bm{u}_i^r)\\\\\n\t\t\t&\\begin{aligned}\n\t\t\t\ts.t.\\;\\;\n\t\t\t\t&\\bm{{x}}_{i+1}\n\t\t\t\t= \\bm{f}_d(\\bm{x}_i, \\bm{u}_i),\\; \\bm{x}_0 = \\bm{x}(0)\\\\\n\t\t\t\t&\\bm{u}_i \\in \\mathbb{U},\\;\\bm{x}_i \\in \\mathbb{X}\n\t\t\t\\end{aligned}\n\t\t\\end{aligned}\n\t\t\\label{Standard_MPC}\n\t\\end{equation}",
      "formula_type": "equation",
      "line_number": 1120,
      "is_formula": true,
      "high_level_explanation": "This is a finite-horizon constrained optimal control problem used in model predictive control (MPC). It minimizes a terminal cost plus cumulative stage costs that penalize deviations of the predicted state and input from their references over a horizon, subject to the discrete-time system dynamics, an initial condition, and state/input constraints. The optimizer returns the control sequence (with only the first input typically applied) while ensuring feasibility within admissible sets.",
      "notations": {
        "\\bm{x}_{1:N": "Sequence of predicted states from step 1 to N over the horizon",
        "\\bm{u}_{0:N-1}": "Sequence of control inputs from step 0 to N−1 over the horizon",
        "l_N": "Terminal cost function (often quadratic) penalizing terminal tracking error",
        "\\bm{x}_N": "Predicted state at the terminal step N",
        "\\bm{x}_N^r": "Reference state at the terminal step N",
        "l_x": "Stage cost on state tracking error (often quadratic)",
        "\\bm{x}_i": "Predicted state at step i in the horizon",
        "\\bm{x}_i^r": "Reference state at step i",
        "l_u": "Stage cost on input tracking/effort (often quadratic)",
        "\\bm{u}_i": "Control input at step i in the horizon",
        "\\bm{u}_i^r": "Reference input at step i",
        "\\bm{f}_d": "Discrete-time dynamics model mapping (x_i, u_i) to x_{i+1}",
        "\\bm{x}_0": "Initial predicted state in the optimization (set to the current measured state)",
        "\\bm{x}(0)": "Measured current state at the beginning of the horizon",
        "\\mathbb{U}": "Admissible input set (feasibility constraints, typically box constraints)",
        "\\mathbb{X}": "Admissible state set (feasibility constraints, typically box constraints)",
        "N": "Receding-horizon length"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:59:07.954420"
    },
    {
      "label": "<<FORMULA_0208>>",
      "formula": "\\label{feedback_N^TN2}\n\t\t\\textcolor{black}{\\bm{\\hat{f}}_{neural}(t) = \\bm{\\Xi}(t)\\bm{\\hat{\\chi}} + \\bm{L}(\\bm{x}(t)-\\bm{\\hat{x}}(t)),}",
      "raw_latex": "\\begin{align} \\label{feedback_N^TN2}\n\t\t\\textcolor{black}{\\bm{\\hat{f}}_{neural}(t) = \\bm{\\Xi}(t)\\bm{\\hat{\\chi}} + \\bm{L}(\\bm{x}(t)-\\bm{\\hat{x}}(t)),}\n\t\\end{align}",
      "formula_type": "align",
      "line_number": 1183,
      "is_formula": true,
      "high_level_explanation": "This equation defines a neural feedback law that combines a parametric neural-network output with linear state-error feedback. The feature vector produced by the front layers of the neural network, Xi(t), is multiplied by the adjustable last-layer weight vector, hat{chi}, while the term L(x(t) − hat{x}(t)) injects observer feedback based on the state estimation error. The weight vector hat{chi} is updated online by an adaptive law, and the gain L is chosen positive definite to regulate convergence and robustness.",
      "notations": {
        "\\bm{\\hat{f}}_{neural}(t)": "Neural-network feedback output at time t",
        "\\bm{\\Xi}(t)": "Feature vector (output of the front layers of the neural network) at time t",
        "\\bm{\\hat{\\chi}}": "Estimated last-layer weight vector of the neural network; updated by an adaptive law",
        "\\bm{L}": "Positive-definite observer gain matrix",
        "\\bm{x}(t)": "System state at time t",
        "\\bm{\\hat{x}}(t)": "Estimated system state at time t"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:00:22.583091"
    },
    {
      "label": "<<FORMULA_0216>>",
      "formula": "\\mathcal{B}_2 = \\left\\{{\\bm{\\dot{\\tilde x}}}(t)\\in\\mathbb{R}^n:\\left\\|{\\bm{\\dot{\\tilde x}}}(t)\\right\\|\\le{\\gamma}\\lambda_M(\\bm{L})/{{\\lambda_j(\\bm{L})}} + \\gamma\\right\\}",
      "raw_latex": "$\\mathcal{B}_2 = \\left\\{{\\bm{\\dot{\\tilde x}}}(t)\\in\\mathbb{R}^n:\\left\\|{\\bm{\\dot{\\tilde x}}}(t)\\right\\|\\le{\\gamma}\\lambda_M(\\bm{L})/{{\\lambda_j(\\bm{L})}} + \\gamma\\right\\}$",
      "formula_type": "inline",
      "line_number": 1193,
      "is_formula": true,
      "high_level_explanation": "The expression defines the bounded set B_2 of all possible values of the time derivative of the estimation error vector whose norm does not exceed a threshold. This threshold depends on a parameter gamma and a ratio of eigenvalue-related quantities of the gain matrix L, plus an additive gamma term. In the surrounding theorem, this characterizes the set to which the error derivative converges, and the bound can be influenced by choosing L.",
      "notations": {
        "\\mathcal{B}_2": "Bounded set in which the error-derivative vector is guaranteed to lie",
        "\\bm{\\dot{\\tilde x}}(t)": "Time derivative of the state observation (estimation) error at time t",
        "\\mathbb{R}^n": "n-dimensional real vector space",
        "n": "NOT MENTIONED",
        "\\gamma": "NOT MENTIONED",
        "\\lambda_M(\\bm{L})": "NOT MENTIONED",
        "\\lambda_j(\\bm{L})": "NOT MENTIONED",
        "\\bm{L}": "Positive definite observer/feedback gain matrix (regulates the bound)"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:00:14.426222"
    },
    {
      "label": "<<FORMULA_0218>>",
      "formula": "\\label{lyapunov2}\n\t\t\t\\textcolor{black}{V(t) = \\frac{1}{2}{\\bm{{\\tilde {x}}}}(t)^T{\\bm{{\\tilde {x}}}}(t) + \\frac{1}{2}{\\bm{\\tilde \\chi}}^T \\bm{\\Gamma}^{-1}{\\bm{\\tilde \\chi}}.}",
      "raw_latex": "\\begin{align} \\label{lyapunov2}\n\t\t\t\\textcolor{black}{V(t) = \\frac{1}{2}{\\bm{{\\tilde {x}}}}(t)^T{\\bm{{\\tilde {x}}}}(t) + \\frac{1}{2}{\\bm{\\tilde \\chi}}^T \\bm{\\Gamma}^{-1}{\\bm{\\tilde \\chi}}.}\n\t\t\\end{align}",
      "formula_type": "align",
      "line_number": 1197,
      "is_formula": true,
      "high_level_explanation": "This defines a Lyapunov candidate V(t) as a quadratic function of an error vector and a parameter-related error. The first term is the squared norm of the error vector, and the second term weights the parameter error by the inverse of a matrix Γ. Such a positive-definite quadratic form is used to analyze stability and convergence in adaptive/observer-based control.",
      "notations": {
        "V(t)": "Lyapunov function candidate",
        "{\\bm{{\\tilde {x}}}}(t)": "NOT MENTIONED",
        "{\\bm{\\tilde \\chi}}": "NOT MENTIONED",
        "\\bm{\\Gamma}^{-1}": "NOT MENTIONED"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T17:00:03.703253"
    },
    {
      "label": "<<FORMULA_0220>>",
      "formula": "\\label{lyapunov_devi2}\n\t\t\t\\dot V\\left( t \\right) & = \\textcolor{black}{\\bm{{\\tilde {x}}}{\\left( t \\right)^T}(\\bm{\\dot{{x}}}\\left( t \\right)-\\bm{\\dot{\\hat {x}}}\\left( t \\right)) - {\\bm{\\tilde \\chi}}^T \\bm{\\Gamma}^{-1} {\\bm{\\dot{\\hat \\chi}}}}  \\notag \\\\\n\t\t\t&\\mathop  = \\limits^{(e)} \\textcolor{black}{-\\bm{{\\tilde {x}}}{\\left( t \\right)^T}\\bm{L}{\\bm{{\\tilde {x}}}}(t) - \\bm{{\\tilde {x}}}{\\left( t \\right)^T}(\\bm{\\Xi}(t)\\bm{\\tilde{\\chi}} + \\Delta \\bm{f}(t)) - {\\bm{\\tilde \\chi}}^T \\bm{\\Xi}^T(t)\\bm{\\tilde{x}}(t)} \\notag \\\\\n\t\t\t& = \\textcolor{black}{-\\bm{{\\tilde {x}}}{\\left( t \\right)^T}\\bm{L}{\\bm{{\\tilde {x}}}}(t) + \\bm{{\\tilde {x}}}{\\left( t \\right)^T}\\Delta \\bm{f}(t) \\notag} \\\\",
      "raw_latex": "\\begin{align} \\label{lyapunov_devi2}\n\t\t\t\\dot V\\left( t \\right) & = \\textcolor{black}{\\bm{{\\tilde {x}}}{\\left( t \\right)^T}(\\bm{\\dot{{x}}}\\left( t \\right)-\\bm{\\dot{\\hat {x}}}\\left( t \\right)) - {\\bm{\\tilde \\chi}}^T \\bm{\\Gamma}^{-1} {\\bm{\\dot{\\hat \\chi}}}}  \\notag \\\\\n\t\t\t&\\mathop  = \\limits^{(e)} \\textcolor{black}{-\\bm{{\\tilde {x}}}{\\left( t \\right)^T}\\bm{L}{\\bm{{\\tilde {x}}}}(t) - \\bm{{\\tilde {x}}}{\\left( t \\right)^T}(\\bm{\\Xi}(t)\\bm{\\tilde{\\chi}} + \\Delta \\bm{f}(t)) - {\\bm{\\tilde \\chi}}^T \\bm{\\Xi}^T(t)\\bm{\\tilde{x}}(t)} \\notag \\\\\n\t\t\t& = \\textcolor{black}{-\\bm{{\\tilde {x}}}{\\left( t \\right)^T}\\bm{L}{\\bm{{\\tilde {x}}}}(t) + \\bm{{\\tilde {x}}}{\\left( t \\right)^T}\\Delta \\bm{f}(t) \\notag} \\\\\n\t\t\\end{align}",
      "formula_type": "align",
      "line_number": 1202,
      "is_formula": true,
      "high_level_explanation": "This expression computes the time derivative of the Lyapunov function V(t) used to analyze the stability of an adaptive observer/neural-ODE-based estimator. By substituting the error dynamics, the feedback law, and the adaptive law (step (e)), the derivative decomposes into a negative quadratic term in the state observation error and a term driven by residual dynamics. Cross terms involving the parameter error cancel under the chosen adaptive law, yielding a form suitable for proving convergence to bounded sets regulated by the gain L.",
      "notations": {
        "\\dot V\\left( t \\right)": "Time derivative of the Lyapunov function",
        "V\\left( t \\right)": "Lyapunov function defined as 1/2 \\tilde{x}^T \\tilde{x} + 1/2 \\tilde{\\chi}^T \\Gamma^{-1} \\tilde{\\chi}",
        "\\bm{{\\tilde {x}}}(t)": "State observation error vector",
        "\\bm{\\dot{{x}}}\\left( t \\right)": "Time derivative of the true state",
        "\\bm{\\dot{\\hat {x}}}\\left( t \\right)": "Time derivative of the estimated state",
        "\\bm{\\tilde{\\chi}}": "Parameter estimation error vector (last-layer weights of the neural network)",
        "\\bm{\\Gamma}^{-1}": "NOT MENTIONED",
        "\\bm{\\dot{\\hat \\chi}}": "Time derivative of the parameter estimate (as given by the adaptive law)",
        "\\bm{L}": "Positive-definite observer gain matrix",
        "\\bm{\\Xi}(t)": "Regressor/features produced by the front layers of the neural network at time t",
        "\\Delta \\bm{f}(t)": "Residual dynamics (model mismatch) term"
      },
      "model_used": "gpt-5",
      "timestamp": "2025-10-31T16:59:57.105858"
    }
  ],
  "metadata": {
    "model": "gpt-5",
    "context_words": 300,
    "max_formulas": 20,
    "timestamp": "2025-10-31T17:00:26.643184",
    "total_formulas_in_paper": 20,
    "formulas_selected_for_analysis": 20,
    "skipped_by_length_limit": 0,
    "formulas_explained": 20,
    "notations_skipped": 0,
    "failed": 0
  },
  "skipped_notations": [],
  "failed": []
}